{"docstore/metadata": {"549e1b3b-a091-4f8d-abe1-2244b6980d26": {"doc_hash": "c64f7906f2321f5cce7d3e1675e521cb3145708cab73f76b2f14ba62a73ff1bf"}, "937eebbd-b7e2-42b6-b629-f4516bba1bce": {"doc_hash": "82617250d56ae1c4411eae072d7825ba53c7eb80053707ca20601b848f2e9316"}, "13e5a54c-cb0c-48f3-a507-72968247b25f": {"doc_hash": "bff7f33e1d35e971e4d6c0a062f62355f3f98f048d28ee9d2f3a9e3e93d60015"}, "4b99402d-2a0c-4bcb-9f25-4bafe50e25e0": {"doc_hash": "66f4121ce4187611d160fbdf1f6af14c597bc53e904fcf36834d24c750f1e083"}, "ff21b96b-3d44-46c1-8d56-afffd419ca7b": {"doc_hash": "8966e361b7fba709a94a09aecf31e28aac7f7cb2074b4aacd1f26fcb0fd68b16"}, "b73c1dad-7c1d-4a8c-a02b-f98bfa7ed10d": {"doc_hash": "3cfb041d8034b74d215e8aab534b25e369d51b0a81e498638c76b54e21b1fb16"}, "9a96f8ac-22d2-48de-980c-6a61390c23fa": {"doc_hash": "bdd7d1c7a4be680f7e2722405f0c5f79095edb43ae76de47ec243a5dae2b43b1"}, "033e0bb8-3dcf-433a-b4f2-2e67b4a0c6d8": {"doc_hash": "750ec4c92b264baecb98e72e60550f7510c7b19c020b5d374242bc5dc2dea9c0"}, "e4a1af29-7df3-4558-a3ed-125023d42c51": {"doc_hash": "c8fdb088458216878ccb2f2e644699c5a584c3336ecfe125341289a358ef30ee"}, "6a6e3dcf-5eef-4889-adfb-d7e044f459f6": {"doc_hash": "82df6dd957b8b66d38017d7c8a09aac95bd6b66f85347d6706dfbb56ca4fa0e1"}, "7fb5d34d-4a07-4911-abec-dbbf425108f4": {"doc_hash": "ec137f6aa04784f842ce0242de2b6536370d0120fbc35ba86de9792bad4f3b5d"}, "8ba6dbe8-8bcd-4626-9e9d-4b9c38bcdcae": {"doc_hash": "fc44645d77c195b71b634713fef11974f63b6c508f00e4358696a169283c62ce"}, "2737fd4b-f5af-4230-9635-e34ebb678626": {"doc_hash": "e3e0c3def90b5221f30190721d5916988b34a103e3253846b74d6ee05d8547f1"}, "da2a7276-8d87-4b9a-b968-68c38bde67b3": {"doc_hash": "b991e12c81ab55521e8da977f6c98e01760b1284505af16da8c5b3d0b04abd2f"}, "6ade6dee-14b1-4b27-8ba3-66a6b1217f5a": {"doc_hash": "b09d0b65bb29036f9fa069b24ab36f3f6d0ee6058ab4251e2e45903915a28bd3"}, "58c7a09b-8a15-4818-849d-387dc4e836d6": {"doc_hash": "13b571ba71a9dc8de41ca0e16f22cae0b296f08b84cd648f5c82973ff577d55d"}, "9cecb09d-cb03-458f-b2ce-7f27ab2bc876": {"doc_hash": "d7e45cdeb496554a315534e3b856c7c08beadd1292b2bd1c4499b75f51e1c804"}, "d7ae6d01-304b-4c09-a244-572a4058e14c": {"doc_hash": "e976802d9fd9209520df440480187c680450176165f8a190f87161e3f2b0ca28"}, "e574a7b8-56aa-40b1-b72b-7f1553fc4059": {"doc_hash": "74ebbc270f8dff20914cd321c0641733240a245626c27257c50f9cebf22a4aca"}, "78798ce2-5305-4892-9024-7cf57157fe08": {"doc_hash": "253a8f61203481365c6e6b4cd415a60ef0b94f03e2799e1006cbb0a37ef73c31"}, "0913e9f7-fdd1-4eed-b481-6b47a8c8e23c": {"doc_hash": "d4297990e0070dfd74aea2e8cdd0249446fa5b23a62a5aa44dea34fdef8f5ae6"}, "15e29758-4a9f-46a5-8d36-c73b1463c98e": {"doc_hash": "bbd58e6bfc73a5a5db5370b2c26b00d361294cc3089b89fb62bb8fc278d8af61"}, "a38c2721-cca2-494b-887d-ac5bb388323d": {"doc_hash": "1c499b50b45a77017fe8993da7000bd098bd64aac7dfb6d8c746ab3b3ed821f5"}, "c3ba744a-617b-4b10-acba-e1a62e7bbed0": {"doc_hash": "63e2616dbf384f801744deaff90c3ee591158920515a0e114877b93b808fa274"}, "fdb17ba8-4844-4df3-bcbb-392f471fb9bf": {"doc_hash": "0234380c9ebfef79fc6d9276542d9557d93a2b6aede73da36843aa3a698a0dab"}, "1a4fbe49-e77e-444c-aa62-6c31f1e2fc46": {"doc_hash": "96fc7aea8e7ff1c9e06035e43a167d014ec723b1599b8b8c66d09354fdd108f3"}, "ec05b3b1-aad5-44d2-8c44-040ee82fb753": {"doc_hash": "d178c3aff147a2668a2fdee3d3eea4a376fa38f6f4d0b2b481b255060b4c509f"}, "1367d900-2b26-4949-ad8b-e5b7c6d79721": {"doc_hash": "7ad2c28a32f2a457b9e46fc612a1eb662b2688e5d0672dd9737d88241a1157c9"}, "1c528bc4-d3b0-4ea5-9e00-ea1c812a5e63": {"doc_hash": "f05d258416acbda00439a7f2427410efb3ade5f8937a2ae0e9b5f90bde4b3fb9"}, "7217d96d-aad0-4cc9-aa1b-0818e7b73bdb": {"doc_hash": "2bc50ab04bb2f9f301bcb2653201770d6d50af803fe8089e581670f91bf576e5"}, "fe3c2a45-9111-42e1-8575-54ace684c88f": {"doc_hash": "dd240be7d952bfb4311fb1661e6429c58d84c544269db6cd7252666cafe506d8"}, "997f9442-a39f-4369-95e1-f13ab24249c0": {"doc_hash": "4dffffbe541b4c4adeae26dee9d690bbd9f2460235a79bc07758522cd0873069"}, "1a4be734-6624-420b-8a46-bfaa870706ae": {"doc_hash": "a2aee72e77e915e5454c71d5ceeeb852a099f0d3da853d588507ddbe9a5d5aeb"}, "c42cc958-8536-4c4f-a43b-4562d494ee68": {"doc_hash": "d427060196d75259a75819e752ab48c88c52d96c776f2e4e87c5b1ebdaab8b42"}, "562f55cb-358a-4448-8f15-73d39df4745d": {"doc_hash": "d34d5393906b4f5be0ded366f0e56c2e8c80c20e9493d5afa508c3fd8ae6006a"}, "bd786b2d-8393-4dd5-b5c7-7e07ab90747b": {"doc_hash": "797d4d32a4be41fc6ca58a931a1ed3ef0652a2f0e703a385def1abeea03996da"}, "ba68a9fb-d9f3-4cab-9b39-2107974a3240": {"doc_hash": "e38970e9f69c224bd8a9215c4ffc6bcb530b0b05730db4505b2fa951e3e0b112"}, "ae59d4a4-b6ff-4652-9261-0aba6de6a794": {"doc_hash": "1534c8364ee714b0aaa75c977fd6aa2ba93aa375da4c4440850d1e2166159207"}, "dd852b53-a1b3-49b8-8721-705d90fc3e96": {"doc_hash": "048a83963cbc9b57d0716c8761758d98c6371dfd0444eb7e7e8d355ed6c1edd6"}, "749ba495-d5ca-4d10-a92f-283833d68dd2": {"doc_hash": "da8760eaf692a2dd3ad4b7a82c4d7163f96f733be02c94b1d2c8a54e16931885"}, "8b124b9d-5509-4177-9e6c-d4e7c2cea021": {"doc_hash": "ab3d48ee4555b2b424472e53745a2f392a4b2fc422fb5e0f565d3f836fdfe705"}, "30d5b6e7-7500-4405-b0bf-10a1615bff47": {"doc_hash": "2a732a8c178282accfaaf9f75f2e1ad0cbe2f07b6ddb55b02fdaf8f549cabb6b"}, "128f4dbe-1291-4aa9-84f8-2b07369f8f9d": {"doc_hash": "479a0a34c6ca9643c34791a8974142640f1eba1d51637410c353ab5eb514665d"}, "a3c1b640-f9e6-4019-b9e3-bc78d5bdb498": {"doc_hash": "5a59ebf2fb73646f8acdeb5a3d79425d03463ecca7def42f73312e969cb1c5c1"}, "5e020124-a827-4495-82dc-522c8f050497": {"doc_hash": "2d3a61727376197d69f4ae3eaa12e31219f5d6c2b677ef013314bd751dcf5b81"}, "2fcfd476-bafc-40c5-8ba0-af0499e67607": {"doc_hash": "7e4d2e575c86c69cca9d27261a5e7866c5e30b4e18843b2261211094a9c91a61"}, "da36e7b2-1ce3-4399-90a8-4f3aa340ed78": {"doc_hash": "3728fa3a34517e51d11b7986878f87f180340f587dd3d6ec775af99798b38d85"}, "721837dc-eeac-49df-ad56-d17a2ad79445": {"doc_hash": "751d972ec164bdb49c1688a831c5a4ad1ac08d867c75f441f8d4582bb5c75873"}, "805869df-8afb-4fe8-aa9c-f78b9495293d": {"doc_hash": "9f372273b76f6503de13b10aa3c6e94277055cd3d4590bfa8b50a5d70d49c325"}, "861a8f99-6af7-480e-92bc-93dc949fe274": {"doc_hash": "c58a595a33685a0874ade751338168ef8e500ddca9f9d15ba5135b13938567f2"}, "9cf3eec0-f1d0-4c09-8697-d5e9209aa401": {"doc_hash": "9d6ca17d1712cb06898b2163d64074a7fb30d587ff0ea544cf14c6d6633513af"}, "f881f6b9-c2ef-4b8b-8e9c-f288d43d1bea": {"doc_hash": "bd619f83a0fdc61610dd778f349329dd9efcc39aaf95330a2a859b6ed6cc74df"}, "a2a03fce-b57d-422c-8109-c9debb279d07": {"doc_hash": "7fba2417fe74db88fc2e2499807aad1c3fc6598d83932110577e5cb2bb6b07d3"}, "68d67c24-2ade-4700-9f1b-0ae06f0c7b48": {"doc_hash": "78fd41ffe148644bf31cb0783d22f4d805566d84289aa3dd4ab08d230162aa36"}, "b429f3d3-930f-4efe-8130-a0b91c905db2": {"doc_hash": "68d972f40963acaca1f0dacdd980aeed5a4965be79297f7bfa2c31e420369d65"}, "5bf04aab-e0bb-4ea3-a49b-bfab1f38af92": {"doc_hash": "a967165eac7f2af22312099695e63feb475711b574ce19291abf45371db69ed0"}, "c252b906-0c44-4de1-a2e7-4fb5fa849b07": {"doc_hash": "be50953d8bbc649d56bfcd189645a187c778bbeeb2bf9b28b42f9d238dc781eb"}, "1352167c-6c9a-4aa0-a0b5-9e86ded06817": {"doc_hash": "af0fa96651385389625bfbfc9680beb83db1f07ab066c64ba4a117686d0abe21"}, "14dd126d-88c1-4a36-ae5c-481eb211bde1": {"doc_hash": "91842c00afcc23052ab54a4cc446848f1b81d2805380675214c6f1fe757b1cef"}, "8132d26d-422e-4746-921e-709ef0ebef6b": {"doc_hash": "d7eb559bb006b2cfed0aa1876470fc452809557541b93b15e6ebcb4fba76fe8b"}, "c376a09f-b21c-48dc-bbd6-16ce3eda517d": {"doc_hash": "6745d6dd75e369d4377105caeba932ed49f9374e0406ed52836402ed96c591e8"}, "d3233c7e-bafc-46f7-a025-194635e51725": {"doc_hash": "436832760f39bd44fe456c7aad4a08668b00743dc6cd39e4b8b79894882d0a30"}, "3493162d-89d8-494d-819b-28148f912471": {"doc_hash": "d8a721c07b52e9ab05a4d79ad253b10507161fa0d23fdd6a564b1043957bd023"}, "7894177e-62ee-4b0e-a2dd-612a30e939f8": {"doc_hash": "0daeb67434eae29001ca98d153e195cb9d093481483f4658980ec05cdae73050"}, "8cadc1a7-4345-4ce4-8dae-4fe134bef877": {"doc_hash": "5e3a98febec24fe1453dac1f9ca6373812a190521ed4c92ea749a66442da642f"}, "6f51ddd1-f17e-44c6-b54b-de27c1552691": {"doc_hash": "ec2020a1e00f2f4d6a1a9cb35f3931983e5e46bed8538047a78c22b8a9103165"}, "b58f2bcc-546a-4b45-b4de-7e264d2d5ce8": {"doc_hash": "e67e4b6cff6ceaadce6ff573c96d3d2a9c195e35f212bf31debdd3f6dac18f23"}, "01bd8987-9835-4bd2-840c-9ec42b2baa8a": {"doc_hash": "f2a6599fa8a6046591887ea8b451f25c242df76a7e44d724673f26e8ba237846"}, "3c4505c2-d38c-4c13-8c88-937414d7421f": {"doc_hash": "5f25fa9fe11ee360ae6a06645cab54c110320118e871b28141a2a26319918d02"}, "368d7731-a37c-4558-8253-ca751f363a3f": {"doc_hash": "fe99f70ae0bdeba6a4d443cb4690152b9cddbaac63bad2f6dca0c789a54b7a32"}, "daa9da1c-ea39-4154-bff7-ffc47d92b5c5": {"doc_hash": "6dc153a90f1964fadfc02ed923b5f563b731369b13658f43b7d3436d92078a77"}, "28fff96f-c15a-422f-b2b7-50bc4329b2b9": {"doc_hash": "c9b55d1725c81cfb9dc4e574ddc9bb17231d3bed0e81d584d11179a726a0b0e9"}, "7bd03e2c-d7cc-409a-a469-89af880ab637": {"doc_hash": "1648658bd2c9f1000b41febd17d4fde0a5cfe840a91c2f86b4f54ce72f403cff"}, "aac4e564-9b8e-41b8-a1b2-067cfc30e76e": {"doc_hash": "5dcaf0f5ed7eb8e1af07289e851b9af8a49493880103c46ba7feb31b68fe6166"}, "83be183d-be91-4545-9073-281e10ca37fe": {"doc_hash": "16a9739fde37b45cea2a2b1af725f748c08877116a7afbb38f372b59922ae5e4"}, "3c921241-32ed-466a-bc70-20c987edd916": {"doc_hash": "cda58b6b296ed54f7f9c83b702927ae18e29781c21517cd5354413d3304cd37c"}, "bf11b47f-2cee-4f29-ac7e-ddba1fb44e9f": {"doc_hash": "021a4a51deafd08ca9e47a18864cbec2592e195368514685f4064f9cb3062606"}, "548c70c4-a5c2-4af8-919f-e598b3a1894a": {"doc_hash": "a0b1954bbf18fcfaf76a375636bda8ec3d72ee2d7923b047fdf74efb7c46b2bd"}, "51f2fc71-6868-4970-b9e8-abe69179bd49": {"doc_hash": "d6e5cec2ebcba45398882d150e3f0a4ddb31c213d7c2cd3894ae4b8ba68be47f"}, "1a25a127-5a7b-46f9-9b2b-6eb046d2a48d": {"doc_hash": "e37dbf874c5f3ed07b00977b788c4e369f9dbda989afa19cab5e7814c96cc172"}, "c10ac9c0-2e0f-4be3-9b88-f8f332227633": {"doc_hash": "07ad429e8bdb87bda07bacb2bc86259985b05c2fe5bb7912fa5dd618349fd2ee"}, "ce7abbe7-6965-4e78-8da4-ec33851d2224": {"doc_hash": "5edb049a7f849bc33ab2bca4cf0ff25998c82a0ac52487b61e542746116b4b37"}, "b56aac4d-70fb-4af1-9dc2-1159e6f70a90": {"doc_hash": "a4f133c92f9e599be72f6a87c8f4100442e12f37f9a66b745efcc8c2f953773e"}, "f0cce281-4ff8-4f1f-ae48-fb2b43dcb158": {"doc_hash": "f612fd0aef14d38ab4ef708f70264f7e71555ea5e34de5722615da52ded9aadf"}, "b7a40f37-e82b-4365-b89b-4cc973ccc7d0": {"doc_hash": "492baf2cdb7fa78588600907084d45eecb6715b25e3d867bc3eca459ddeb69b4"}, "a969031d-199f-4d4f-a121-9e31ec421446": {"doc_hash": "3726e6919f4bff88372811a5b56c69b233d709800f17f81ed1d3c9c6609faf8f"}, "2ddf8e06-3dfa-4453-8241-b3ccdb1e4686": {"doc_hash": "da154cf33cfe398b66c6f852793a02a3d2ab1def196a609b13f4fbb94df9011b"}, "0d0f1663-f311-4760-bf06-5446e5a11ac8": {"doc_hash": "b005ac7b590cf814c8efb7a8a8b4d93bceb7fdcbcfef1af3ff6e924fdd7df57a"}, "72441c3d-08f1-4370-b886-13ae6988801b": {"doc_hash": "41a6b10258299c0ebc7581839eabf187b822e3c6b78a726d40eb9c08e8b899c2"}, "c0bf30a6-9b8b-477b-8ce8-3e76e373ff84": {"doc_hash": "212292e737dae8429e5724a41ff9caadcf11a6701909591efe24fc54027d5316"}, "76ad06c5-5675-4484-a6a0-d18b08d4ec51": {"doc_hash": "34974354506f03d0e22c8df748bc1ca4b4df17850e2fe7cb71a42c2257ceb55f"}, "9421f734-ac27-415c-ba4b-3f33a1c94d7d": {"doc_hash": "98024c2c5a0237d13ee8069b1824e8f8f9682f25bf97ee3f6c3556a1dcb9ae06"}, "4aed0592-1b53-48c4-98d3-b5ee8f234150": {"doc_hash": "498be9419469954352410606d1da42f1ae162d938a0585db6523989a10d8c5b2"}, "1ec42e5c-347d-44e6-bdcf-ee4075bfcf16": {"doc_hash": "bb55c4c156e396a371723589268158fc32ddcdaa5e4a05a3d97390c5b1877f36"}, "d209b906-a365-42d4-adc8-0c3063379b38": {"doc_hash": "86cfa3a7b0052198ddd9043e6ea08e9bf21579183f35a7fe2321ecd82497600d"}, "ffd86702-ba69-4f62-8c92-80c61026da92": {"doc_hash": "c53ac6331d3a0c971de72dbac06dbdba891724262f1a6cc7f24e02a57eaa4c47"}, "a70665c0-b121-40ed-9595-c3076e4bb1e5": {"doc_hash": "0ba2d6000a88423108ebef9818095ecf3e84cb19dc522a3c3868626dda233907"}, "0e6d7ce8-174e-4c01-9b11-175b862efb5c": {"doc_hash": "0d747c86c8a7f74b790c637b5a58bd9f10635c2269151d65a85c6b6aff1bf9be"}, "bd745a49-a5f7-4006-9784-b1a86f585eee": {"doc_hash": "60ffc833d80fd4fa0b693a8c6beb7d533dd1b091b1e2784890e5eb4931f648ea"}, "a8de0aef-41dd-4b0b-b165-eb9ad9fdaac7": {"doc_hash": "782d035d61d5e97c93b4bfda855559c1c0157c261ca8e4cb53e0567b283fab68"}, "f4d5f94e-5740-424c-b113-569c7060a94c": {"doc_hash": "41b49fe5bb1eb3dd6074b7b5de8c35d0d8b3c32183f190fe5b7c551907729b42"}, "df4bc328-3a90-417d-9072-96ad5d79f56a": {"doc_hash": "773303e937c11aae6bdbd5fddde9cc48a53dc2538e17ba6450eed63a96a5ecc1"}, "c88fa152-4bcc-489f-9ac5-741b47ad117c": {"doc_hash": "9d0399806d5a3331ed7b05e0113e5527b606a30f141dff45aa7c7b493c56feed"}, "13b2d855-039c-4589-991f-bb81c9ff9347": {"doc_hash": "370b73321008228ee68232459aef648705994a692064371fd58bf79bacc3df73"}, "efd79ef3-5949-46b5-96b3-d0ad75fff690": {"doc_hash": "62a4222265eb2e4633989affb648d3d46b62fab5a16a4000e9e26525a14363c7"}, "81560a8f-4e99-4e34-a475-dfb6369c27d1": {"doc_hash": "ab8fdfeb24a61d44f0ce87de4991a797f18d090ee3798a777a239642dfd3d109"}, "252b8374-5bea-45e9-a0d6-e4946391d66a": {"doc_hash": "842fb0b3680c67f1a52440d27c8b1828267c9c9441a1ea313c8bae433e77b25a"}, "17fd7791-7164-43fa-b030-bf9103575a6b": {"doc_hash": "dc1ced4fc62d56492ff15eab53b0b60b3cdf7fff3705f0e5a2feab40500dca9b"}, "2117b9b4-41eb-4f4e-82f4-ce9c8d1dfaa3": {"doc_hash": "368ca53496aa26c651862bf8cf2d09c877831b550c123e035d80c86f04b5150b"}, "b4382b29-0500-44fd-8ed4-2deaabb2e05b": {"doc_hash": "12370c5f58e0389492cd327efeb43ac234aa5c05252b070944fd851abb0a8465"}, "f75c8b03-4488-4383-a657-d7f10c309d6e": {"doc_hash": "e8c963ea15bd663ae06904b5dd6da166470abc46fe312143396f319a42750cc2"}, "ab4702da-d279-4896-8925-a65b7e313065": {"doc_hash": "748daa9cd5882293238877f990ffe62a6865fa4e807105dc4c1d0977c41b794b"}, "01af99df-40e7-40b0-b4a0-794650092dc1": {"doc_hash": "03206a160e6a429c1d04442978869eedea7514ebc6ecae8366c5ec6dbe62c3b3"}, "4ce5b866-dad9-4d22-bdf8-14e0101cc40f": {"doc_hash": "b5ac7c568bc9b42d870f671aef2fc7edd33edc295be509c1d81341dae7fa6a43"}, "110bc2e0-e46b-4815-916c-8a83238ce208": {"doc_hash": "9f2da30118b6b6cebef049d96c19c7d1a229414fb6a496b040ab4b8882687b5f"}, "d9be1181-982d-4805-806c-2bbac3fd3325": {"doc_hash": "4eeb7c30b21165938d8524bc5a6f0de7289dda9142531c823434fc6186a66c66"}, "5297f41d-0971-4b2b-81df-1e27a1f387d7": {"doc_hash": "afa27c220cbb74e0a2d727ef57054174fad4cb59bbc3a12510ae3b5f05a0a95e"}, "8ca061ec-19f8-4d1f-ae48-67aadc971609": {"doc_hash": "5715f400fce8607259844f9dbce84df57f4a4a5e95057ea226c50c00e426a8fc"}, "c80a239f-38a6-408d-ab3d-6f220a4389b1": {"doc_hash": "cab44818be2db456f791293ba059aff2f8212fff5dfe2c71f3652448d86c93aa"}, "51552b13-7c1a-42a7-813f-a88c16b17b46": {"doc_hash": "167ae83ca185f5cff75e0b28b7b7afa2b80ec5ba5197bbaa81ee1c47646b792e"}, "c2e545e5-9aae-4bd6-a37c-8a63a016b0a2": {"doc_hash": "676bcfb52f031ab568859cf485846c28b81ec55177e73b0e1a9739d16e7106e6"}, "a7c27c93-dde6-4a93-bc20-faba0e635acf": {"doc_hash": "a738b923ee462dfc2063b24dc4f8164a2730c0ac23b9c98b8d1054e43afd57d4"}, "d6dee29e-c418-4f94-a1d0-ecaa688bd3f7": {"doc_hash": "d250969e518c321319ba8d3663820c5e702d98a09d92579d14e2ffadf5795a8d"}, "2c5a052c-f004-47f5-bedd-510130dbbf90": {"doc_hash": "7408397257d981c89441bc6354ab713844ea0a45012b88b0771d62178f8199f5"}, "54a16377-b341-4c4a-ab6d-eeafe8cb7fd1": {"doc_hash": "d8fb3e3d4e1b02ce49b839038e168c0846b074eefae45c1f4a76a2803905bfa1"}, "c9c4d8c1-408f-4496-b4d3-abedd1762e8b": {"doc_hash": "7f3748beb35375bc4dc80d59cf7ddc9584c639ba167eec08d7cd81e3d5c50bc7"}, "4cd007e5-bd11-4e2f-aa9d-50c4b2feed70": {"doc_hash": "7ddda397aa9a44326a3517b502c8bf7e49417bbb817ce2f7b1cf2eccc3d1ce6e"}, "577c11e7-bb98-41df-8de9-cdaaa1d639c9": {"doc_hash": "6e47729b53342bfbe9ef649ee43f74f389345a28466d3e4ecc661c5a38ef343a"}, "7fd8de60-897f-4090-983e-1d52831ea649": {"doc_hash": "31d2aef22932596c8224723bd081d41b3d377f74dd295c9565cbbd39f9ea4ab1"}, "8ed38d5d-a5f1-4d79-8ac5-f0acbeb0b124": {"doc_hash": "6486ff2a487b79b62ebe91437ecb2d5ab3c66edafc14b4e0ebd8b2fd6745b18f"}, "f8605f3a-7656-43e8-a4e6-81677bcc909a": {"doc_hash": "8ddda4508c5c43df74802c1e0a71ab25556ed38238187e5d68573a0818e43168"}, "560552c8-8b86-4624-9648-827df3ef3543": {"doc_hash": "7d2547e4fd998655e70911f39d00f09e2fd6e272244f16151dc4df081911ad48"}, "dbf8dd20-f397-4917-a962-fc1dafdc121b": {"doc_hash": "05306b9067664b36a7a85bb41c3d9aa21d3170774e5df2f63e297e0ffc5e0f2f"}, "b9bd7ebe-c240-4518-8985-2d099c8fffdb": {"doc_hash": "994783ac9011059289e629d88f076d8822cd99c9b1a8d258b6a14f44cb2e91ae"}, "8ae3847f-a505-4e7f-9a0c-05cd6cdc9112": {"doc_hash": "0760fc18b702545969d684239b2b83ed81625cd7adbe362ccdc643a07a461bf5"}, "7054c97e-a62b-4ff0-9233-358d37455907": {"doc_hash": "c459119465e1db7754e3e99b570b84a65579edb8d39ddb8e44b62612925315f0"}, "a659c6a2-3b84-4066-acaa-8798044ee557": {"doc_hash": "3845eaf3b1be8c18164f43b9db61ac421530c101d2517a268fd1b2a01ff4e012"}, "5f68befb-8cb8-4dfe-b5cd-8ba2de4042e0": {"doc_hash": "7f5bbb9b0b18e3ff6941db4288da516bd62c151970a2c5300800f7a39b7c00f6"}, "a6f08484-e26b-4953-9a42-807bca5f22f1": {"doc_hash": "ec8ede817623bb03d759e2fa8c9331539c64e82966512775f1c9e868d34b6a58"}, "9943056c-56a8-41f4-b21c-d92534f5aece": {"doc_hash": "5c076b5ea31be443deec0a9c97e67b9e39459b66be6cdbe0fc0ec2c84963fda8"}, "cc6d33ba-4593-43fc-bd2f-2465e7bb1077": {"doc_hash": "fdaf0209a9a457fd3bfbaa12a4d1dfbb94572afc76c3d53b0e0db39f7c4b8958"}, "35c57f5e-f676-4e3b-b4e7-7cd353a50dc4": {"doc_hash": "5a7aabe6768b4785318d20a777f8766c62db5e707aa1009e45c568c7d1a9f256"}, "47b8b997-1352-42a6-9c13-40907baa2ffb": {"doc_hash": "1173e45579b36245e551ba3d3ebbe749e4b1f9e05b674aec2f8057f76066c53e"}, "5db188b7-2661-49a3-a1f2-c226e5132704": {"doc_hash": "646f30e8c829ea6fd4d8dd62453914e3f40f12a9f350fb629ac6c2b2538443ac"}, "15fbef4c-46b7-4d87-829d-200106fa753a": {"doc_hash": "ae2ff9a84d049000c5ce08e6f04c9b04a4c174d644439e496045a6e350d0a8b6"}, "96944c55-fd43-4667-9b61-492540397ba0": {"doc_hash": "b9e4876057ea05c7302ad567c6bb2faaa99d5ac9102ac18a120586434923aaf8"}, "6c3243a6-66b9-44b1-931e-b6c563088ffb": {"doc_hash": "7f201bf7cbe1ce0cad0f32e061b24b90581378213c70b40318bd3153b3b7a8a9"}, "53a91575-5885-4b08-ad3d-0cf3bfe21722": {"doc_hash": "d94c133019c7bf282a24f296be0bc1a428f0b32f04a3c61d8977ee3a17a2a630"}, "346b92a8-56bc-498e-94c1-e4a5fadb1e02": {"doc_hash": "824caa6c1d25d09ec22a96ddc186b79f7a028ad540f697f41f0b4287a739dac1"}, "e5449ccf-2bf2-4bc0-aaf9-081bd938dfc5": {"doc_hash": "34dd86354b3066faa8bd8d14c7c5bafeec1d9f7ca028d39635baabc9b6122a58"}, "de77cfa4-550c-4151-b98f-3ff1ddb3e672": {"doc_hash": "dac4cdf5b0481be578287bf86adcd866b9fc045a1fa75f4cb8a24801f02723ce"}, "111e0d8d-9ea7-46d1-97be-86c885590e7b": {"doc_hash": "8c6a75c06f18dca0c0595a175cdb548ab551cd14985a46d380a457f7c042bf81"}, "2c0724c7-6f79-4b64-8e0a-adeda852ae02": {"doc_hash": "2194c74ca6759b8e5761a9141e445d7c661cfdab52ff1a5819e7fe67af907015"}, "6c141fc2-7deb-4e13-a2e5-f9f7282e0823": {"doc_hash": "1c045a76bc46ae45479d0ec1958321375142faf6d69ed2641b8538407b7ba01c"}, "e1a594ef-52f6-43fe-a7d5-8cc90d2fdc5a": {"doc_hash": "8548e7d01ce43b44e1284b08985dc6bd4c0542dcfdc73f731e8bfad7d4f4a51c"}, "d2750f85-eeba-40dc-a6f9-f80310dc43c6": {"doc_hash": "e8d59a3adfe965c238220c9b3ee03479e2b4b73329991b668f75e19fb41459cb"}, "1427bb47-aac0-482f-b2d4-09041d58bd78": {"doc_hash": "6c72744f502d66ce72e09a5071fa39dfbf5a3d7b290b7a725e5b9526b86b7067"}, "353a2d97-af49-4f77-af61-d0b1b433f604": {"doc_hash": "294cc66517dec033828386cee867f6895a43ea0486c2acd70337418fa71764dd"}, "c09ae629-23c8-4815-b115-5070dbda2856": {"doc_hash": "765995901d3e887b40e4c600f06d0b373a3f4d96f63ada0f209a5acf0f80121a"}, "4318d69b-6d75-423c-97c4-b0988a2f4c89": {"doc_hash": "dddca4b725ef169814c19d5ac149642118d193bf11c782d3f9761f4f289e5099"}, "72ebd1e0-7104-49f3-ac43-b44ec7632f13": {"doc_hash": "fac5874b42abf03c62b40f44860d7df44a3cfbe626a64090e95a616663c83932"}, "5875b819-8e5a-423f-8899-130c92be13fe": {"doc_hash": "7fd5c98df6891481ba9f2999739113bda7f1eadae538c51ee16777be0e1681e1"}, "68e1646a-3244-46e4-8a48-3f40d635d268": {"doc_hash": "a0debe6ef0695919d6e4edf03ab2b79905b4362650a4a8d4ba0f04d896dd3c12"}, "1faeee5b-c3b6-4869-a10f-be0ef3f2eb9f": {"doc_hash": "abfbdd4fe66d6bc45586be7da391b28b27adf27148e429776b5b81bb7de34071"}, "e5635af9-1fdb-4d2f-b615-2e81e2d4e513": {"doc_hash": "d85c753ca1f22ee9e21f85905fdc0c6dcfa4b5eb2c533b71a5fdef3694d5635f", "ref_doc_id": "549e1b3b-a091-4f8d-abe1-2244b6980d26"}, "8143711d-516d-42f8-a52e-97dd35b07675": {"doc_hash": "57d6a5e5b80fc9cf491b7f6b272d67b7ebb4bb0b102c53b1101b771808e314fb", "ref_doc_id": "549e1b3b-a091-4f8d-abe1-2244b6980d26"}, "016a780b-82bf-48dc-b7a0-6d2f5005a13d": {"doc_hash": "396a5bd638d5f7c89e0ba10008b3e33fc1b6a7f7360706d168e886e09dfe0ef2", "ref_doc_id": "937eebbd-b7e2-42b6-b629-f4516bba1bce"}, "5f93ba42-f0fd-421f-92dc-ee6f7778d77e": {"doc_hash": "ceac7a6ad4fc22d8bdc0552cb51c0cc69846fee7ad1494f115cfdb9f6b901634", "ref_doc_id": "937eebbd-b7e2-42b6-b629-f4516bba1bce"}, "831d9a2b-5ccf-47d0-8351-161ef96e0023": {"doc_hash": "bff7f33e1d35e971e4d6c0a062f62355f3f98f048d28ee9d2f3a9e3e93d60015", "ref_doc_id": "13e5a54c-cb0c-48f3-a507-72968247b25f"}, "bf77674c-7216-4516-8e33-606cad88487a": {"doc_hash": "e6005810a0c0805df5a91111c5f93bca0c84c9341d3daff397d21a973be71d65", "ref_doc_id": "4b99402d-2a0c-4bcb-9f25-4bafe50e25e0"}, "23d7c70f-f784-43f9-a515-708542c786c2": {"doc_hash": "702cb318daa5b05e2ed60976145b6e4f715f6652040a5726f763e512d91dde5f", "ref_doc_id": "4b99402d-2a0c-4bcb-9f25-4bafe50e25e0"}, "51693bc7-c580-40e6-b3f6-cfb9b5552572": {"doc_hash": "53f6b429b7d29cf45383e36a027d765654c766f1ae1526b20399cae8640ceb27", "ref_doc_id": "ff21b96b-3d44-46c1-8d56-afffd419ca7b"}, "37d86c02-1a23-48d8-befe-9514f8aaacd2": {"doc_hash": "2fbe8a9ae295bc1cdd519298dd7204180e71eb15e0f773e0a4371ace68776eb6", "ref_doc_id": "ff21b96b-3d44-46c1-8d56-afffd419ca7b"}, "9e47eba8-3f53-4e69-a077-c58782bde03c": {"doc_hash": "ac9f5a847c65f00d668092e99b8731fe5be582ff6960890d4a3f67c9c3abba9f", "ref_doc_id": "b73c1dad-7c1d-4a8c-a02b-f98bfa7ed10d"}, "50e1968e-96a5-419c-be50-f4525e3c95fb": {"doc_hash": "7f0ec04d40628c8604f32b96cb03670bdd91a69b81d422771d79967d5490f91b", "ref_doc_id": "b73c1dad-7c1d-4a8c-a02b-f98bfa7ed10d"}, "1243974e-f8e4-4a2e-8add-f662318bbe43": {"doc_hash": "0ddd889cd5cfdc3ce830c3793e7807feb9d81f2798b288b2093a26ed3a14d765", "ref_doc_id": "9a96f8ac-22d2-48de-980c-6a61390c23fa"}, "8b714e85-9b55-436d-8850-c7248d366e94": {"doc_hash": "170a15c4c3946990f7deb9ff651de6c347eee28d2c44e7ad9884930e24d1f7f4", "ref_doc_id": "9a96f8ac-22d2-48de-980c-6a61390c23fa"}, "70b0c482-be1e-4f37-b892-419da9e187de": {"doc_hash": "750ec4c92b264baecb98e72e60550f7510c7b19c020b5d374242bc5dc2dea9c0", "ref_doc_id": "033e0bb8-3dcf-433a-b4f2-2e67b4a0c6d8"}, "3aaa02f6-d8dd-48b0-8032-96c96c68da8a": {"doc_hash": "d0f826d9ea56608e2e5ec15e104907e4863c4485dc45e0b115594a628b8116d1", "ref_doc_id": "e4a1af29-7df3-4558-a3ed-125023d42c51"}, "a9fe1235-c857-4172-95a7-3e7cdda6cbb6": {"doc_hash": "c52ffa6cf084c5c9a8d596f025ee279013aabf0f7c7567eb67e816d6b01fccda", "ref_doc_id": "e4a1af29-7df3-4558-a3ed-125023d42c51"}, "b25c9ca3-d116-4a86-8589-80242f3e2529": {"doc_hash": "ff2659abe5d4a614b56fd77db8ed7e553ff7193d8ca2009953765aea8e6f31d5", "ref_doc_id": "6a6e3dcf-5eef-4889-adfb-d7e044f459f6"}, "64ab38de-2ab2-4004-9c54-30307d2e8a01": {"doc_hash": "629b0a40a909935a5b639958a869fa4f8a47b2c767d519f5c221214ee211ae75", "ref_doc_id": "6a6e3dcf-5eef-4889-adfb-d7e044f459f6"}, "4ce9d3df-ba9e-4e68-81a5-2dca7d25e1a8": {"doc_hash": "331ce92af8c9b76a08f76bb0d80427456fe7c00957697a518908f2c6b61fd2ee", "ref_doc_id": "7fb5d34d-4a07-4911-abec-dbbf425108f4"}, "aa939a26-1d07-416f-aca8-65fbdaa48348": {"doc_hash": "1d43ed1ffb6fc3fb7ae232e74f2330f96789a88513e3dbe9145a5946ae5baaf8", "ref_doc_id": "7fb5d34d-4a07-4911-abec-dbbf425108f4"}, "e7207de6-6d84-4172-b7fa-6ce41e8cdf96": {"doc_hash": "bcc15814340b68e22128bc95bef0c71be0fff098284da21cfb359cbab3400766", "ref_doc_id": "8ba6dbe8-8bcd-4626-9e9d-4b9c38bcdcae"}, "49d68f11-3811-46e6-ac67-6931e8ec84a3": {"doc_hash": "501a8bf377521edf0cea3f17f0111a42d75a63c045a9d93849a87312bee27ec4", "ref_doc_id": "8ba6dbe8-8bcd-4626-9e9d-4b9c38bcdcae"}, "3ff1d2fc-0ee8-4776-9987-303059c8a6d6": {"doc_hash": "bbeb6456987a0dce8b258abc3a4ea0241253bd57deca9ccecccb3fa2ed9ac55f", "ref_doc_id": "2737fd4b-f5af-4230-9635-e34ebb678626"}, "bd2e9333-ec11-40a4-9386-bae415501b28": {"doc_hash": "d14daf202395bad7763f85ffff78c3a9a6842e582222a7626c911278aae64fef", "ref_doc_id": "2737fd4b-f5af-4230-9635-e34ebb678626"}, "186a110d-27b0-448e-a529-3b3a031ad2ff": {"doc_hash": "c88ea233d9689bcbf26af37e0ca91ad202b2efc62eb8235e50c073788eb7e500", "ref_doc_id": "da2a7276-8d87-4b9a-b968-68c38bde67b3"}, "7cd548c7-4dab-4201-aa76-f12bace264be": {"doc_hash": "b29fb39122c4a82e99fc716242a65543c2cf469b8c0faa26d80b7e178262df26", "ref_doc_id": "da2a7276-8d87-4b9a-b968-68c38bde67b3"}, "cb9d4f26-4723-4d4d-9371-44f35e54c106": {"doc_hash": "0763d77bcc968b56860602ae73433102e3aa0130aeec33e85c0258b1c2d264c3", "ref_doc_id": "6ade6dee-14b1-4b27-8ba3-66a6b1217f5a"}, "9e64d7ba-c5de-419c-85c7-f716a25e7fdb": {"doc_hash": "8c8df64d1c7cadb755c43271c4c167a900efd92a9367cfc1b8cf915917e037b9", "ref_doc_id": "6ade6dee-14b1-4b27-8ba3-66a6b1217f5a"}, "6844e2e9-74ad-47c8-825a-5c807fd50fe7": {"doc_hash": "13b571ba71a9dc8de41ca0e16f22cae0b296f08b84cd648f5c82973ff577d55d", "ref_doc_id": "58c7a09b-8a15-4818-849d-387dc4e836d6"}, "b8a82c3d-8149-4653-bb1c-7e9ed8624403": {"doc_hash": "d7e45cdeb496554a315534e3b856c7c08beadd1292b2bd1c4499b75f51e1c804", "ref_doc_id": "9cecb09d-cb03-458f-b2ce-7f27ab2bc876"}, "dc25b042-69a3-46b4-9ce8-9414d7b98230": {"doc_hash": "8b153633621bd1093c0b62fda779d6dfa03a6a1cad6f4543eb9a0d28331827e7", "ref_doc_id": "d7ae6d01-304b-4c09-a244-572a4058e14c"}, "b15cd3a5-eb40-4e36-980e-3a7088a9a1d4": {"doc_hash": "0a7e3c91ec6060e5455bb4115653e4044d6e59fcde1a8e82da5e1b4ac25cf0f5", "ref_doc_id": "d7ae6d01-304b-4c09-a244-572a4058e14c"}, "9671fd20-6caa-4599-bcfa-ef4feeb27726": {"doc_hash": "013e9d65433dcd0a22f049f038b5de3e21cb0799040cafa4ae9b7bb688c6c6dd", "ref_doc_id": "d7ae6d01-304b-4c09-a244-572a4058e14c"}, "8ea26e5c-6bb4-4c34-92bf-6c1630ae1741": {"doc_hash": "74ebbc270f8dff20914cd321c0641733240a245626c27257c50f9cebf22a4aca", "ref_doc_id": "e574a7b8-56aa-40b1-b72b-7f1553fc4059"}, "b9f262d6-21bd-43ce-9a57-5628e1fd884f": {"doc_hash": "253a8f61203481365c6e6b4cd415a60ef0b94f03e2799e1006cbb0a37ef73c31", "ref_doc_id": "78798ce2-5305-4892-9024-7cf57157fe08"}, "20f0cac9-6d2b-42cb-9c6f-68c1d587c0a4": {"doc_hash": "d4297990e0070dfd74aea2e8cdd0249446fa5b23a62a5aa44dea34fdef8f5ae6", "ref_doc_id": "0913e9f7-fdd1-4eed-b481-6b47a8c8e23c"}, "ee69358b-a8ed-41fc-becb-d3ceaa668f2a": {"doc_hash": "bbd58e6bfc73a5a5db5370b2c26b00d361294cc3089b89fb62bb8fc278d8af61", "ref_doc_id": "15e29758-4a9f-46a5-8d36-c73b1463c98e"}, "533c7a64-acba-4267-aaa3-66c093a0d474": {"doc_hash": "1c499b50b45a77017fe8993da7000bd098bd64aac7dfb6d8c746ab3b3ed821f5", "ref_doc_id": "a38c2721-cca2-494b-887d-ac5bb388323d"}, "6cd8f8ae-3ca5-47b3-a75e-f7819f9a3d66": {"doc_hash": "63e2616dbf384f801744deaff90c3ee591158920515a0e114877b93b808fa274", "ref_doc_id": "c3ba744a-617b-4b10-acba-e1a62e7bbed0"}, "d0892b19-09a9-4e8e-97fb-8bf1d22a182e": {"doc_hash": "0234380c9ebfef79fc6d9276542d9557d93a2b6aede73da36843aa3a698a0dab", "ref_doc_id": "fdb17ba8-4844-4df3-bcbb-392f471fb9bf"}, "fd53522e-075c-4667-bfab-278780d89409": {"doc_hash": "96fc7aea8e7ff1c9e06035e43a167d014ec723b1599b8b8c66d09354fdd108f3", "ref_doc_id": "1a4fbe49-e77e-444c-aa62-6c31f1e2fc46"}, "67b0e9e0-57c3-4673-8de9-112b6a8a2811": {"doc_hash": "d178c3aff147a2668a2fdee3d3eea4a376fa38f6f4d0b2b481b255060b4c509f", "ref_doc_id": "ec05b3b1-aad5-44d2-8c44-040ee82fb753"}, "2ef333ac-4042-461b-8927-32b9b9f45f85": {"doc_hash": "9c9c94950511871a54de8816efd37cf7a3bab4a49f9c666b9bdb16494cec8fff", "ref_doc_id": "1367d900-2b26-4949-ad8b-e5b7c6d79721"}, "e9021931-d6e6-45c9-a307-c58de1e20a21": {"doc_hash": "22d4e6b73ef26088b44fd3e98bbfb8d2f96acce11b86f5cf388b8f085bc0511b", "ref_doc_id": "1367d900-2b26-4949-ad8b-e5b7c6d79721"}, "b86c3c4f-f7e9-4929-926e-60dfb5f1c41d": {"doc_hash": "f05d258416acbda00439a7f2427410efb3ade5f8937a2ae0e9b5f90bde4b3fb9", "ref_doc_id": "1c528bc4-d3b0-4ea5-9e00-ea1c812a5e63"}, "a6225579-6446-4701-91fa-e3ffded97535": {"doc_hash": "2bc50ab04bb2f9f301bcb2653201770d6d50af803fe8089e581670f91bf576e5", "ref_doc_id": "7217d96d-aad0-4cc9-aa1b-0818e7b73bdb"}, "7c57aae8-eea2-4ec4-84b5-22a3ea586fea": {"doc_hash": "139f75f161f5af983091a698e5dc4a4b5844f780e421bce3212c081d0889814c", "ref_doc_id": "fe3c2a45-9111-42e1-8575-54ace684c88f"}, "e5aa6003-6819-4298-9edc-9203de83f727": {"doc_hash": "3fb2f8ae25bd126e4d429e03419e24911ea075cf2f475abf0313ec885d812540", "ref_doc_id": "fe3c2a45-9111-42e1-8575-54ace684c88f"}, "869f8667-4260-41d3-9154-a8428ee7b3b4": {"doc_hash": "b173c37b6ef407e030d77e204600864203539278318abf10fd784fd2166bb48c", "ref_doc_id": "997f9442-a39f-4369-95e1-f13ab24249c0"}, "ef6655fd-e555-4977-8f6d-5fff1d3eeed5": {"doc_hash": "cfdac1f6b198b2b3f4ae1ddd61041cf6b18dc0efb3aacf2eed72cbb50dfa6fa5", "ref_doc_id": "997f9442-a39f-4369-95e1-f13ab24249c0"}, "e2a1dcbe-c6a2-4a78-a389-5e9441efc45c": {"doc_hash": "011cca552e3aefc2a3d72e287b77864743bc3962c6713b7e75d0383ffb81ff38", "ref_doc_id": "1a4be734-6624-420b-8a46-bfaa870706ae"}, "06fa9fd3-6bb7-491b-b0e7-4d10cc4ece24": {"doc_hash": "d78d703a8d362ff377c947694303fcb79866f246730f3ce206a411b5861a96b3", "ref_doc_id": "1a4be734-6624-420b-8a46-bfaa870706ae"}, "1c21241b-f464-413d-9942-dfec22189f6c": {"doc_hash": "421f8df779b1ee7f824b13b713611e8ae6320972d5545395eb62d0dae92abb59", "ref_doc_id": "c42cc958-8536-4c4f-a43b-4562d494ee68"}, "d14c55cc-8bc3-4f55-8b33-71578bfcde0d": {"doc_hash": "3925e43fe9b8917be9572bc7fd32fe74156a8d201fcccd3a33f04a80e245bb99", "ref_doc_id": "c42cc958-8536-4c4f-a43b-4562d494ee68"}, "d4189545-9f9a-4ef1-98cc-0a81aec6ea93": {"doc_hash": "2ac79d75d1c8c76790bfc3877c449bcbaad7a1c001e095fe2af12c1388f2dab8", "ref_doc_id": "562f55cb-358a-4448-8f15-73d39df4745d"}, "b84764ee-9a70-4440-a2b0-d61a221a5e13": {"doc_hash": "3ab3cdeeebd9bbe8b0ac3ea97d94118be65f56bb5ddd93b554973babf683e162", "ref_doc_id": "562f55cb-358a-4448-8f15-73d39df4745d"}, "2a0f0850-85f3-439f-84ce-f190377921d1": {"doc_hash": "01c7c3b7031a73002b4dc728e84757db3998f9a7f81cad589c6349ce1145a858", "ref_doc_id": "bd786b2d-8393-4dd5-b5c7-7e07ab90747b"}, "3dbeae1b-299d-4f5f-a1b3-f0d4f879a423": {"doc_hash": "caeb8398650aa21414dbb6411a1bd6b3d31f02c40bf7dba5b7fa5504a9e41f83", "ref_doc_id": "bd786b2d-8393-4dd5-b5c7-7e07ab90747b"}, "1b251110-29d9-4bfb-9962-1398169cae0b": {"doc_hash": "274a18597750e796f4527929cc4770fe282fa445cd04907ac6f6be8f7591a99f", "ref_doc_id": "ba68a9fb-d9f3-4cab-9b39-2107974a3240"}, "e24fd0b9-8aec-4fa5-b5cb-2706b417a90f": {"doc_hash": "9377c1d76d9abf072c80e6577c818e08f8590233d8574cd4ed0d6a8d39904e3b", "ref_doc_id": "ba68a9fb-d9f3-4cab-9b39-2107974a3240"}, "357e8147-4dbe-4fd0-a197-20df732bec23": {"doc_hash": "a7c84bf0bf1fa325a4dc972a365731cb4ed1697d043574e7c735eecb724e516f", "ref_doc_id": "ae59d4a4-b6ff-4652-9261-0aba6de6a794"}, "fbd78736-83ee-4a0f-8123-4b3066399ee0": {"doc_hash": "53a1439417b8a6f7cc86293f9de13d7ef36e83db244f33ccf3846eb7325786a2", "ref_doc_id": "ae59d4a4-b6ff-4652-9261-0aba6de6a794"}, "9bec28a5-2384-4702-b391-0c209de2c43b": {"doc_hash": "7bca6ac65e4224502cc785b98628ed3e78aadfd54d24b7ba0fcc66a42fe77c57", "ref_doc_id": "dd852b53-a1b3-49b8-8721-705d90fc3e96"}, "71c2f4d4-9f8b-404a-9473-aae3dcf0894a": {"doc_hash": "152b31f037a36cb92fda355a98841c7dfaedeeff3ede9d37fb748a2092faba22", "ref_doc_id": "dd852b53-a1b3-49b8-8721-705d90fc3e96"}, "69a130fa-e299-4c24-8003-83a85da8ed33": {"doc_hash": "bdc3d3559b7546e634912cb286d52d4e7e5b6b9d2b746c131920f4d098f3b5bd", "ref_doc_id": "749ba495-d5ca-4d10-a92f-283833d68dd2"}, "68aa091d-c8f4-44fd-ad48-98eec269670e": {"doc_hash": "416f593e17bc49713a85a73253a178b8f63b9d60eca56f1764b2edfa04b85c39", "ref_doc_id": "749ba495-d5ca-4d10-a92f-283833d68dd2"}, "445afb20-7c2e-4c7d-93ee-72a2e75b38cb": {"doc_hash": "fe1e5c4713f52a484c7c012a443927d58229937d5c4d2fc7f7fcba599005ecb8", "ref_doc_id": "8b124b9d-5509-4177-9e6c-d4e7c2cea021"}, "14f4c3aa-a3d8-4f30-a22e-9d18f9bbc32b": {"doc_hash": "4d63d2c3d3e68cbd83c7c909195840aa9b50ca0d9485d6574a9fea1add732dc3", "ref_doc_id": "8b124b9d-5509-4177-9e6c-d4e7c2cea021"}, "a792c8ef-7a81-48a7-bc0d-6797331df8b8": {"doc_hash": "92504ba55a5c2281fbfa482c167862ce4f16fd01741dbfc01497453b8155b07a", "ref_doc_id": "30d5b6e7-7500-4405-b0bf-10a1615bff47"}, "19f1515b-1bd1-4641-99a5-93e14df1457d": {"doc_hash": "614fe9c1604f7668991f2a489a86073e1825ae0a6e32bfed2030424ca181ce40", "ref_doc_id": "30d5b6e7-7500-4405-b0bf-10a1615bff47"}, "ec383b36-8ddd-45f5-9703-287d48500c3e": {"doc_hash": "0536ff345a8b09e9142cf781fc2d8eb97f061c0da4238c90f8b150d4c04406cc", "ref_doc_id": "128f4dbe-1291-4aa9-84f8-2b07369f8f9d"}, "61c49036-b174-42f1-94d5-e42ac913de81": {"doc_hash": "f9ee58076f21e554f1964e63632bd516d57a60a8fa31a974fe4e8e7df978974b", "ref_doc_id": "128f4dbe-1291-4aa9-84f8-2b07369f8f9d"}, "f4e28ff8-a124-47ca-857f-e84a5517d915": {"doc_hash": "fb58d4e9ca88df1db8475025df8fb6b85ecee92661cff4315339c1f9c6f46ed2", "ref_doc_id": "a3c1b640-f9e6-4019-b9e3-bc78d5bdb498"}, "6a9c6235-b1f6-41fd-8c2f-f0cea3d0c1d7": {"doc_hash": "3648474cc77c62b161f7309260af24e9d8516fcd67ed6ff5fc3b0c1b4c75c8c0", "ref_doc_id": "a3c1b640-f9e6-4019-b9e3-bc78d5bdb498"}, "e281bd8a-7299-4f17-87fd-31d2eb25073e": {"doc_hash": "fb2fb7fb6a8bdc22d88cd9af17bdf30892d691fa939c12b64b95c916ef545f6f", "ref_doc_id": "5e020124-a827-4495-82dc-522c8f050497"}, "dd9a1243-7e48-4401-a299-71334c199a08": {"doc_hash": "d297c6a32697c043031f0c7cf4c7e296ef5863eb010a4e135d8d58e9689df5ef", "ref_doc_id": "5e020124-a827-4495-82dc-522c8f050497"}, "b0321cb7-d60d-4168-a93d-ef0b89ee413a": {"doc_hash": "0f1754808629dec57be3afbbc0f6089cdecac56d82fcdee4353dc1269a2d36e0", "ref_doc_id": "2fcfd476-bafc-40c5-8ba0-af0499e67607"}, "227521f7-0347-4971-a446-96cdfc8fa441": {"doc_hash": "78312500e7182c0a4cf11858cf9fad773572857269b780a77cab4d0e34023c8a", "ref_doc_id": "2fcfd476-bafc-40c5-8ba0-af0499e67607"}, "410a1ea1-ee6a-4bad-91d8-faebdf087155": {"doc_hash": "de39a80dc3e028d2fa3bcf171b5aac1444372f92f9288435663aa816c4ea1c9f", "ref_doc_id": "2fcfd476-bafc-40c5-8ba0-af0499e67607"}, "2c4d8ebd-2d0b-484d-80ae-3623139f0a64": {"doc_hash": "a0ef51550e4c76dfb152a776b8246a37ed7c446e12f82bcd5b506d3ae801914a", "ref_doc_id": "da36e7b2-1ce3-4399-90a8-4f3aa340ed78"}, "b22e1b1c-268c-40b9-aa57-08a90f20bb23": {"doc_hash": "bc996dc23f9adc1d325af54af3397cb7469a376ee8b73220c944611a31277a14", "ref_doc_id": "da36e7b2-1ce3-4399-90a8-4f3aa340ed78"}, "1d26bd8c-28b6-48e0-b848-15bbbcf909be": {"doc_hash": "751d972ec164bdb49c1688a831c5a4ad1ac08d867c75f441f8d4582bb5c75873", "ref_doc_id": "721837dc-eeac-49df-ad56-d17a2ad79445"}, "45c4175e-d0b4-46d1-ab3e-e248d3a3d02e": {"doc_hash": "9f372273b76f6503de13b10aa3c6e94277055cd3d4590bfa8b50a5d70d49c325", "ref_doc_id": "805869df-8afb-4fe8-aa9c-f78b9495293d"}, "09815e6c-69c5-430a-a7a3-1ad3d7e60cd5": {"doc_hash": "faef56a98d2ddb56f01ab051f67e5899adc5fab76cc0aa6455a1b52e586512c3", "ref_doc_id": "861a8f99-6af7-480e-92bc-93dc949fe274"}, "38deda85-8093-417b-85de-eb894a6bc0c3": {"doc_hash": "f789f68ab13c907ecd2a5ccc9a52c45e905be8f95e141fa48df49e7d604606c4", "ref_doc_id": "861a8f99-6af7-480e-92bc-93dc949fe274"}, "cec8bdf5-d5c6-403c-8476-332ca241c1c4": {"doc_hash": "df69d08cae14f355ef7968273e695bdf339a0a9c0bf0e977a06d52b79ad14946", "ref_doc_id": "861a8f99-6af7-480e-92bc-93dc949fe274"}, "f3376e3b-0ace-4fed-a31a-542fa5161a23": {"doc_hash": "9d6ca17d1712cb06898b2163d64074a7fb30d587ff0ea544cf14c6d6633513af", "ref_doc_id": "9cf3eec0-f1d0-4c09-8697-d5e9209aa401"}, "02a3d26f-6994-4f95-96af-10da10a657e6": {"doc_hash": "bd619f83a0fdc61610dd778f349329dd9efcc39aaf95330a2a859b6ed6cc74df", "ref_doc_id": "f881f6b9-c2ef-4b8b-8e9c-f288d43d1bea"}, "ac3055d4-bb2f-4a87-ab39-ffc64121cede": {"doc_hash": "7fba2417fe74db88fc2e2499807aad1c3fc6598d83932110577e5cb2bb6b07d3", "ref_doc_id": "a2a03fce-b57d-422c-8109-c9debb279d07"}, "35c8874c-aa7d-4995-a294-1f53a09420d9": {"doc_hash": "78fd41ffe148644bf31cb0783d22f4d805566d84289aa3dd4ab08d230162aa36", "ref_doc_id": "68d67c24-2ade-4700-9f1b-0ae06f0c7b48"}, "8100895b-27cf-48c5-83e2-92b03cad7595": {"doc_hash": "68d972f40963acaca1f0dacdd980aeed5a4965be79297f7bfa2c31e420369d65", "ref_doc_id": "b429f3d3-930f-4efe-8130-a0b91c905db2"}, "014e6ef1-9fdd-4594-8824-dbbacda770b9": {"doc_hash": "a967165eac7f2af22312099695e63feb475711b574ce19291abf45371db69ed0", "ref_doc_id": "5bf04aab-e0bb-4ea3-a49b-bfab1f38af92"}, "14981466-f049-4a81-868c-9eaf9a0cb562": {"doc_hash": "be50953d8bbc649d56bfcd189645a187c778bbeeb2bf9b28b42f9d238dc781eb", "ref_doc_id": "c252b906-0c44-4de1-a2e7-4fb5fa849b07"}, "4344cb65-d9e8-42ab-9d34-1b7227c48170": {"doc_hash": "af0fa96651385389625bfbfc9680beb83db1f07ab066c64ba4a117686d0abe21", "ref_doc_id": "1352167c-6c9a-4aa0-a0b5-9e86ded06817"}, "7ad3be36-b736-499a-a6f0-2d1312201665": {"doc_hash": "91842c00afcc23052ab54a4cc446848f1b81d2805380675214c6f1fe757b1cef", "ref_doc_id": "14dd126d-88c1-4a36-ae5c-481eb211bde1"}, "ad880d7e-4eea-4819-a6fb-174b059e8d8b": {"doc_hash": "d7eb559bb006b2cfed0aa1876470fc452809557541b93b15e6ebcb4fba76fe8b", "ref_doc_id": "8132d26d-422e-4746-921e-709ef0ebef6b"}, "ceb1b3b9-7688-4146-a10b-f77092b54bcc": {"doc_hash": "6745d6dd75e369d4377105caeba932ed49f9374e0406ed52836402ed96c591e8", "ref_doc_id": "c376a09f-b21c-48dc-bbd6-16ce3eda517d"}, "ee04aa4e-58e9-4da7-99c2-8eabd3fe3b70": {"doc_hash": "436832760f39bd44fe456c7aad4a08668b00743dc6cd39e4b8b79894882d0a30", "ref_doc_id": "d3233c7e-bafc-46f7-a025-194635e51725"}, "23154560-63b4-464a-bb4d-9ba8508019a2": {"doc_hash": "d8a721c07b52e9ab05a4d79ad253b10507161fa0d23fdd6a564b1043957bd023", "ref_doc_id": "3493162d-89d8-494d-819b-28148f912471"}, "914f70f3-ee6c-4573-8342-236884201070": {"doc_hash": "0daeb67434eae29001ca98d153e195cb9d093481483f4658980ec05cdae73050", "ref_doc_id": "7894177e-62ee-4b0e-a2dd-612a30e939f8"}, "dd738e10-6b5f-4d17-872d-b41a1b9555f4": {"doc_hash": "5e3a98febec24fe1453dac1f9ca6373812a190521ed4c92ea749a66442da642f", "ref_doc_id": "8cadc1a7-4345-4ce4-8dae-4fe134bef877"}, "47d9d1a7-d392-4ae7-8d30-0f396c41c545": {"doc_hash": "ec2020a1e00f2f4d6a1a9cb35f3931983e5e46bed8538047a78c22b8a9103165", "ref_doc_id": "6f51ddd1-f17e-44c6-b54b-de27c1552691"}, "1bd61933-8656-42a3-bf4e-182fd5fb7830": {"doc_hash": "e67e4b6cff6ceaadce6ff573c96d3d2a9c195e35f212bf31debdd3f6dac18f23", "ref_doc_id": "b58f2bcc-546a-4b45-b4de-7e264d2d5ce8"}, "76afe11e-2b48-4eb5-bcf0-4324308aafe7": {"doc_hash": "f2a6599fa8a6046591887ea8b451f25c242df76a7e44d724673f26e8ba237846", "ref_doc_id": "01bd8987-9835-4bd2-840c-9ec42b2baa8a"}, "2622add1-04c3-465a-af28-ab8ed217f13f": {"doc_hash": "5f25fa9fe11ee360ae6a06645cab54c110320118e871b28141a2a26319918d02", "ref_doc_id": "3c4505c2-d38c-4c13-8c88-937414d7421f"}, "59db3101-fe89-4f49-af6c-524894cd5e25": {"doc_hash": "fe99f70ae0bdeba6a4d443cb4690152b9cddbaac63bad2f6dca0c789a54b7a32", "ref_doc_id": "368d7731-a37c-4558-8253-ca751f363a3f"}, "0aad11f5-66e6-4f96-8e3f-a12fdc42feb2": {"doc_hash": "6dc153a90f1964fadfc02ed923b5f563b731369b13658f43b7d3436d92078a77", "ref_doc_id": "daa9da1c-ea39-4154-bff7-ffc47d92b5c5"}, "9323b3ad-9c4c-4984-b858-720ed251718e": {"doc_hash": "c9b55d1725c81cfb9dc4e574ddc9bb17231d3bed0e81d584d11179a726a0b0e9", "ref_doc_id": "28fff96f-c15a-422f-b2b7-50bc4329b2b9"}, "c009ee86-065f-4c23-a477-f4c34b40b966": {"doc_hash": "1648658bd2c9f1000b41febd17d4fde0a5cfe840a91c2f86b4f54ce72f403cff", "ref_doc_id": "7bd03e2c-d7cc-409a-a469-89af880ab637"}, "ac85c774-e6a8-46f9-8a07-7b9000793683": {"doc_hash": "5dcaf0f5ed7eb8e1af07289e851b9af8a49493880103c46ba7feb31b68fe6166", "ref_doc_id": "aac4e564-9b8e-41b8-a1b2-067cfc30e76e"}, "c050c699-d858-4900-b333-bd529c18d160": {"doc_hash": "a5093fcb10f368539732e9fe52f08644b32df696534db4bb7dfebf00d7a23cb0", "ref_doc_id": "83be183d-be91-4545-9073-281e10ca37fe"}, "5996284b-6f5f-4520-8ac9-439f323ebc18": {"doc_hash": "064fb902138ce18c52015df29ac7bbf42ddcdb90f840a9e9ae5d80ed2811efae", "ref_doc_id": "83be183d-be91-4545-9073-281e10ca37fe"}, "61beed51-8a07-46be-9b78-2746cd56a7ae": {"doc_hash": "cda58b6b296ed54f7f9c83b702927ae18e29781c21517cd5354413d3304cd37c", "ref_doc_id": "3c921241-32ed-466a-bc70-20c987edd916"}, "b8e8975e-d599-4977-8b24-8e8a6db4a85f": {"doc_hash": "021a4a51deafd08ca9e47a18864cbec2592e195368514685f4064f9cb3062606", "ref_doc_id": "bf11b47f-2cee-4f29-ac7e-ddba1fb44e9f"}, "d0e7e587-4d5c-4f68-9f37-24080beff268": {"doc_hash": "a0b1954bbf18fcfaf76a375636bda8ec3d72ee2d7923b047fdf74efb7c46b2bd", "ref_doc_id": "548c70c4-a5c2-4af8-919f-e598b3a1894a"}, "49b83f48-8d9a-4115-9a3b-227b892deeb1": {"doc_hash": "d6e5cec2ebcba45398882d150e3f0a4ddb31c213d7c2cd3894ae4b8ba68be47f", "ref_doc_id": "51f2fc71-6868-4970-b9e8-abe69179bd49"}, "4fbff83f-69e2-4544-b216-51118f93f6ae": {"doc_hash": "e37dbf874c5f3ed07b00977b788c4e369f9dbda989afa19cab5e7814c96cc172", "ref_doc_id": "1a25a127-5a7b-46f9-9b2b-6eb046d2a48d"}, "4057834c-f144-4091-b762-156af184d7d3": {"doc_hash": "07ad429e8bdb87bda07bacb2bc86259985b05c2fe5bb7912fa5dd618349fd2ee", "ref_doc_id": "c10ac9c0-2e0f-4be3-9b88-f8f332227633"}, "e54fcc51-9ffd-4486-a814-59483440aa4f": {"doc_hash": "5edb049a7f849bc33ab2bca4cf0ff25998c82a0ac52487b61e542746116b4b37", "ref_doc_id": "ce7abbe7-6965-4e78-8da4-ec33851d2224"}, "0a864d87-bb21-4df9-b4fd-8c2ccc4d6023": {"doc_hash": "a4f133c92f9e599be72f6a87c8f4100442e12f37f9a66b745efcc8c2f953773e", "ref_doc_id": "b56aac4d-70fb-4af1-9dc2-1159e6f70a90"}, "dd927af0-be9b-4059-9f49-ffc85c80591e": {"doc_hash": "f612fd0aef14d38ab4ef708f70264f7e71555ea5e34de5722615da52ded9aadf", "ref_doc_id": "f0cce281-4ff8-4f1f-ae48-fb2b43dcb158"}, "3142acbf-d7fe-4ffe-b876-1ef3639689d1": {"doc_hash": "492baf2cdb7fa78588600907084d45eecb6715b25e3d867bc3eca459ddeb69b4", "ref_doc_id": "b7a40f37-e82b-4365-b89b-4cc973ccc7d0"}, "b24cc7e5-9e3b-44c0-a98f-a4af24fc870c": {"doc_hash": "3726e6919f4bff88372811a5b56c69b233d709800f17f81ed1d3c9c6609faf8f", "ref_doc_id": "a969031d-199f-4d4f-a121-9e31ec421446"}, "7ad967cb-c751-486e-94bf-25c8263a6d54": {"doc_hash": "da154cf33cfe398b66c6f852793a02a3d2ab1def196a609b13f4fbb94df9011b", "ref_doc_id": "2ddf8e06-3dfa-4453-8241-b3ccdb1e4686"}, "ff9e2105-0efc-4af2-aad8-655238572e31": {"doc_hash": "31d9ae76a5552ecc7dc8a514f3d95a35a98981c75b96a3a87fbc8bb86ae669be", "ref_doc_id": "0d0f1663-f311-4760-bf06-5446e5a11ac8"}, "caa9f08c-26a6-4152-aa43-33bb82691896": {"doc_hash": "afb86c45f1300e41a22389fc4ac1367eb1c77c6544449ff0835f4f139081d36f", "ref_doc_id": "0d0f1663-f311-4760-bf06-5446e5a11ac8"}, "4c6d3478-0708-466d-8879-612b1c553116": {"doc_hash": "41a6b10258299c0ebc7581839eabf187b822e3c6b78a726d40eb9c08e8b899c2", "ref_doc_id": "72441c3d-08f1-4370-b886-13ae6988801b"}, "0ce758f1-3382-434c-b822-e3df25c82dd1": {"doc_hash": "212292e737dae8429e5724a41ff9caadcf11a6701909591efe24fc54027d5316", "ref_doc_id": "c0bf30a6-9b8b-477b-8ce8-3e76e373ff84"}, "30c1a09a-883d-4149-aa13-7ad1fec264cd": {"doc_hash": "34974354506f03d0e22c8df748bc1ca4b4df17850e2fe7cb71a42c2257ceb55f", "ref_doc_id": "76ad06c5-5675-4484-a6a0-d18b08d4ec51"}, "5e9b55c4-d95f-47e9-ac5d-7185664aae28": {"doc_hash": "98024c2c5a0237d13ee8069b1824e8f8f9682f25bf97ee3f6c3556a1dcb9ae06", "ref_doc_id": "9421f734-ac27-415c-ba4b-3f33a1c94d7d"}, "ea08b061-dc73-494b-b7b8-1d27ed7eca59": {"doc_hash": "498be9419469954352410606d1da42f1ae162d938a0585db6523989a10d8c5b2", "ref_doc_id": "4aed0592-1b53-48c4-98d3-b5ee8f234150"}, "3672d4a5-84e8-49f8-88c9-147083c07bf4": {"doc_hash": "bb55c4c156e396a371723589268158fc32ddcdaa5e4a05a3d97390c5b1877f36", "ref_doc_id": "1ec42e5c-347d-44e6-bdcf-ee4075bfcf16"}, "12ec2990-47b7-4d98-983e-47ca5b137a6c": {"doc_hash": "c58cff9e08fd33db5c404b977e6f1d2cf1719d56f0999519fce0210997bc1792", "ref_doc_id": "d209b906-a365-42d4-adc8-0c3063379b38"}, "9bdffaf9-f218-433d-b2b1-6008e437af15": {"doc_hash": "072125746ce4fecb81cdea61d0872c749b62be8cadd334c435597d79588079cf", "ref_doc_id": "d209b906-a365-42d4-adc8-0c3063379b38"}, "ecc63690-ae4d-43b9-89da-2382380a859e": {"doc_hash": "c53ac6331d3a0c971de72dbac06dbdba891724262f1a6cc7f24e02a57eaa4c47", "ref_doc_id": "ffd86702-ba69-4f62-8c92-80c61026da92"}, "220a5241-6967-40bb-be6a-88f9f5bab38f": {"doc_hash": "0ba2d6000a88423108ebef9818095ecf3e84cb19dc522a3c3868626dda233907", "ref_doc_id": "a70665c0-b121-40ed-9595-c3076e4bb1e5"}, "0862d41e-8859-4095-8b86-4fa56754799b": {"doc_hash": "0d747c86c8a7f74b790c637b5a58bd9f10635c2269151d65a85c6b6aff1bf9be", "ref_doc_id": "0e6d7ce8-174e-4c01-9b11-175b862efb5c"}, "663ec988-571a-4d48-bcc8-77579b12a90d": {"doc_hash": "eda1b3e633e77858da2c7b39acde90487d29fb5d1e1de0d58a20ed4ea9462b38", "ref_doc_id": "bd745a49-a5f7-4006-9784-b1a86f585eee"}, "5865e9ff-6d2f-49a3-a974-cdf4abde8c13": {"doc_hash": "8d00c5c64b151881e8634edb84a5ee77dab4a69d7c90a778f701e596cb2cca50", "ref_doc_id": "bd745a49-a5f7-4006-9784-b1a86f585eee"}, "70dea20e-ff43-40ca-b4a7-b98a33cd17af": {"doc_hash": "fb79ce52ac2bad6999d83f85118708c338c83a7921bf8e8f303580f2e8df4b67", "ref_doc_id": "a8de0aef-41dd-4b0b-b165-eb9ad9fdaac7"}, "abb5da81-1366-4cac-84c0-71aa17add4bc": {"doc_hash": "f4944847d3ded66f45b88bcf7353f6a6cafd1d113bf019f65f85a068ff0fd11f", "ref_doc_id": "a8de0aef-41dd-4b0b-b165-eb9ad9fdaac7"}, "71cccb65-61d6-4c8b-ace1-45ada0e771fc": {"doc_hash": "6b08beda9ff34e46b12219566c8634aad4f6d2520e749b4d56941ab364fd1f4c", "ref_doc_id": "f4d5f94e-5740-424c-b113-569c7060a94c"}, "b013476a-e3c6-40d8-bb2c-428c49e47399": {"doc_hash": "15fb412c4aa2dafcf61efa27439d1902f35a9fc380a33944ed34664b2befd46a", "ref_doc_id": "f4d5f94e-5740-424c-b113-569c7060a94c"}, "b8d3c995-142b-4bec-ac7d-f34bd9f6e721": {"doc_hash": "340303d3032510d98c76703f939504f259685cde7628a444fe009bde72ca99b4", "ref_doc_id": "df4bc328-3a90-417d-9072-96ad5d79f56a"}, "7dbe9c08-6254-4374-89e8-7bd57b98dab2": {"doc_hash": "d648eba4c73d0e3b84a7c7b1752536030d9eb8fa9ac1220e7e33cabdd59c0452", "ref_doc_id": "df4bc328-3a90-417d-9072-96ad5d79f56a"}, "58c643a8-66b5-480d-b4e2-23d32784ec7e": {"doc_hash": "19fab4823846d4c5a04fda42e7728ad5472a20a630a553bf0bc1378049e8e207", "ref_doc_id": "c88fa152-4bcc-489f-9ac5-741b47ad117c"}, "39a8bfa6-51e5-4271-9026-9893a14e8b98": {"doc_hash": "046e3a3c71dcc23fff2d54e348114d2f78ebdae95202867a559b1a91e52e6230", "ref_doc_id": "c88fa152-4bcc-489f-9ac5-741b47ad117c"}, "6aa86388-192e-4157-8f47-ceb12aaeaca8": {"doc_hash": "370b73321008228ee68232459aef648705994a692064371fd58bf79bacc3df73", "ref_doc_id": "13b2d855-039c-4589-991f-bb81c9ff9347"}, "52a388a0-0eb6-45ea-8044-c1c9d4ec602d": {"doc_hash": "f80ad0552ef462dde0497044df7ebe035cdaf3160d0070a1ea60777ae1243bb5", "ref_doc_id": "efd79ef3-5949-46b5-96b3-d0ad75fff690"}, "28769e7d-85f3-4e9f-a6b5-3e0a22053343": {"doc_hash": "4d320ae141d0fe06ecfd922be58bf4a88d06531c6b641452a538b14ac9245af3", "ref_doc_id": "efd79ef3-5949-46b5-96b3-d0ad75fff690"}, "456b8062-8b29-4194-8544-f00a12fab2db": {"doc_hash": "ab8fdfeb24a61d44f0ce87de4991a797f18d090ee3798a777a239642dfd3d109", "ref_doc_id": "81560a8f-4e99-4e34-a475-dfb6369c27d1"}, "caa1196b-e3bb-4666-ae04-9f27049045c3": {"doc_hash": "842fb0b3680c67f1a52440d27c8b1828267c9c9441a1ea313c8bae433e77b25a", "ref_doc_id": "252b8374-5bea-45e9-a0d6-e4946391d66a"}, "bf75c5af-383f-4bcf-a6ab-651996356030": {"doc_hash": "dc1ced4fc62d56492ff15eab53b0b60b3cdf7fff3705f0e5a2feab40500dca9b", "ref_doc_id": "17fd7791-7164-43fa-b030-bf9103575a6b"}, "9d87d0f5-c036-4393-80e6-b9e50679fe79": {"doc_hash": "368ca53496aa26c651862bf8cf2d09c877831b550c123e035d80c86f04b5150b", "ref_doc_id": "2117b9b4-41eb-4f4e-82f4-ce9c8d1dfaa3"}, "9350bd68-ed09-403d-9c83-ebb2ba7b4a61": {"doc_hash": "12370c5f58e0389492cd327efeb43ac234aa5c05252b070944fd851abb0a8465", "ref_doc_id": "b4382b29-0500-44fd-8ed4-2deaabb2e05b"}, "50980f3f-048e-441c-a5f8-4f7e8b8defd3": {"doc_hash": "e8c963ea15bd663ae06904b5dd6da166470abc46fe312143396f319a42750cc2", "ref_doc_id": "f75c8b03-4488-4383-a657-d7f10c309d6e"}, "891bbe3d-5f0a-4124-8182-209d3f664c7b": {"doc_hash": "748daa9cd5882293238877f990ffe62a6865fa4e807105dc4c1d0977c41b794b", "ref_doc_id": "ab4702da-d279-4896-8925-a65b7e313065"}, "d06bee6a-5e6f-49c1-88be-e7ed10778b1b": {"doc_hash": "03206a160e6a429c1d04442978869eedea7514ebc6ecae8366c5ec6dbe62c3b3", "ref_doc_id": "01af99df-40e7-40b0-b4a0-794650092dc1"}, "40d9babf-5240-4874-99e5-c2295fef478e": {"doc_hash": "b5ac7c568bc9b42d870f671aef2fc7edd33edc295be509c1d81341dae7fa6a43", "ref_doc_id": "4ce5b866-dad9-4d22-bdf8-14e0101cc40f"}, "0fb1d4ef-3343-4248-aec6-f7bf88df2779": {"doc_hash": "9f2da30118b6b6cebef049d96c19c7d1a229414fb6a496b040ab4b8882687b5f", "ref_doc_id": "110bc2e0-e46b-4815-916c-8a83238ce208"}, "67ad00fd-6021-413e-90ab-08f08567b531": {"doc_hash": "4eeb7c30b21165938d8524bc5a6f0de7289dda9142531c823434fc6186a66c66", "ref_doc_id": "d9be1181-982d-4805-806c-2bbac3fd3325"}, "7d9d0885-581d-487a-8c22-b79f04d94586": {"doc_hash": "afa27c220cbb74e0a2d727ef57054174fad4cb59bbc3a12510ae3b5f05a0a95e", "ref_doc_id": "5297f41d-0971-4b2b-81df-1e27a1f387d7"}, "fff73933-4ce0-460c-bd35-2227653fc61e": {"doc_hash": "5715f400fce8607259844f9dbce84df57f4a4a5e95057ea226c50c00e426a8fc", "ref_doc_id": "8ca061ec-19f8-4d1f-ae48-67aadc971609"}, "b73d2e4b-5fb6-4498-b288-bb2a2dea8204": {"doc_hash": "cab44818be2db456f791293ba059aff2f8212fff5dfe2c71f3652448d86c93aa", "ref_doc_id": "c80a239f-38a6-408d-ab3d-6f220a4389b1"}, "29f2f10f-cafa-424b-988c-1aa1c30759aa": {"doc_hash": "167ae83ca185f5cff75e0b28b7b7afa2b80ec5ba5197bbaa81ee1c47646b792e", "ref_doc_id": "51552b13-7c1a-42a7-813f-a88c16b17b46"}, "27fe09e2-f534-44f6-846b-05f52e89df48": {"doc_hash": "676bcfb52f031ab568859cf485846c28b81ec55177e73b0e1a9739d16e7106e6", "ref_doc_id": "c2e545e5-9aae-4bd6-a37c-8a63a016b0a2"}, "1d70ef45-14d9-4366-a5c1-44e9e4e0ac13": {"doc_hash": "a738b923ee462dfc2063b24dc4f8164a2730c0ac23b9c98b8d1054e43afd57d4", "ref_doc_id": "a7c27c93-dde6-4a93-bc20-faba0e635acf"}, "9c18f26b-3730-4938-9677-67b5ae468a51": {"doc_hash": "d250969e518c321319ba8d3663820c5e702d98a09d92579d14e2ffadf5795a8d", "ref_doc_id": "d6dee29e-c418-4f94-a1d0-ecaa688bd3f7"}, "4417de3c-9d8d-4511-b605-78ee08a5dabe": {"doc_hash": "7408397257d981c89441bc6354ab713844ea0a45012b88b0771d62178f8199f5", "ref_doc_id": "2c5a052c-f004-47f5-bedd-510130dbbf90"}, "495aaa09-00d7-4262-bc21-cc771273b11a": {"doc_hash": "d8fb3e3d4e1b02ce49b839038e168c0846b074eefae45c1f4a76a2803905bfa1", "ref_doc_id": "54a16377-b341-4c4a-ab6d-eeafe8cb7fd1"}, "99312a99-9417-4418-a639-10903110c875": {"doc_hash": "50dee0e56bbc1418882923fddf21654e8598925d71de8c4056940304e2898ca9", "ref_doc_id": "c9c4d8c1-408f-4496-b4d3-abedd1762e8b"}, "a2d42920-b34d-498b-8e4b-8ef47e1eb227": {"doc_hash": "bf1d3565f8f965c19a823e900dce16ae033856615447fdfac4921ac622720998", "ref_doc_id": "c9c4d8c1-408f-4496-b4d3-abedd1762e8b"}, "a1527307-042e-40ef-a150-aa6ba301bfb3": {"doc_hash": "7ddda397aa9a44326a3517b502c8bf7e49417bbb817ce2f7b1cf2eccc3d1ce6e", "ref_doc_id": "4cd007e5-bd11-4e2f-aa9d-50c4b2feed70"}, "eb72a68c-84ba-49dc-8d47-eee11abe4e8d": {"doc_hash": "6e47729b53342bfbe9ef649ee43f74f389345a28466d3e4ecc661c5a38ef343a", "ref_doc_id": "577c11e7-bb98-41df-8de9-cdaaa1d639c9"}, "6dc50b2d-9137-4d9f-be4f-a8463137625a": {"doc_hash": "329462cce484b594312e76d3d14aa3d4a28bd8c79b29d7610641bc4b39a6c33b", "ref_doc_id": "7fd8de60-897f-4090-983e-1d52831ea649"}, "c1101ad3-d165-4648-924c-11c3a70013c3": {"doc_hash": "aab1dcf8abf1e581a368cd42f0485e8ddb70cf49a83a646e595c7b11564e9573", "ref_doc_id": "7fd8de60-897f-4090-983e-1d52831ea649"}, "d6bd935c-ade2-4adf-8068-91093194f2a1": {"doc_hash": "e9d010224878a733e5f2b2875ea0778dbdf5092cd042d8cd77fd802c71dd8389", "ref_doc_id": "8ed38d5d-a5f1-4d79-8ac5-f0acbeb0b124"}, "c729b94e-6b22-415b-b0c6-bd1c37852472": {"doc_hash": "9fdb93092620078903b6f89363fc3c5a24d0755bd89f4ea720a43aacd1c0c757", "ref_doc_id": "8ed38d5d-a5f1-4d79-8ac5-f0acbeb0b124"}, "60931171-b5d5-48bf-b05b-c201cb1ad237": {"doc_hash": "4c74f074692c486a4a0c703e75976256cca70fdbbd448f6c146d5bf078a0f0fa", "ref_doc_id": "f8605f3a-7656-43e8-a4e6-81677bcc909a"}, "d226190e-198f-484b-a37d-1c43de9c1875": {"doc_hash": "c77cb15eae712bed3df10d5df2858f7cc800a3d97098ba90406c6c62c3db0318", "ref_doc_id": "f8605f3a-7656-43e8-a4e6-81677bcc909a"}, "933dfac2-afde-4083-b742-41d075f5926b": {"doc_hash": "2a748b7a98b97ed9cd8222a10cd0020dd6930f7235036f5eb7ff873ceb2a9adb", "ref_doc_id": "560552c8-8b86-4624-9648-827df3ef3543"}, "285cf2fb-92b2-4fe6-b97b-43b21867eb63": {"doc_hash": "85a0f98cca7133b5784ff548891435b8d0f286346d089b6c121ffb3628c79e0b", "ref_doc_id": "560552c8-8b86-4624-9648-827df3ef3543"}, "1b3806c7-4b7f-4acd-91ab-046dc8023b1f": {"doc_hash": "96964cc9eadf8865992aa613104dacac7f241b5641ed05f85d4b5ba720cb5d44", "ref_doc_id": "dbf8dd20-f397-4917-a962-fc1dafdc121b"}, "e9394d0c-ddf4-4420-b307-1c790d35eede": {"doc_hash": "923360570f54e07207fd4ad8d700f5b1b761806eadc1c009400c174503ad6118", "ref_doc_id": "dbf8dd20-f397-4917-a962-fc1dafdc121b"}, "56ac9a9b-f180-4603-b1f4-c1d6ceb541a2": {"doc_hash": "d149bf1b88dd0da2852ded6d1634335d4421a9ec5eee64e0487a730f50e94138", "ref_doc_id": "b9bd7ebe-c240-4518-8985-2d099c8fffdb"}, "e6836dd8-052d-452e-addb-6bb263d29b99": {"doc_hash": "08e83339da4c42cfa3361b35d5149a1a5492ddddbb1bb9df56319bf472dd267e", "ref_doc_id": "b9bd7ebe-c240-4518-8985-2d099c8fffdb"}, "d1eaf772-c31a-4cc9-b01a-7bc05c86030f": {"doc_hash": "0760fc18b702545969d684239b2b83ed81625cd7adbe362ccdc643a07a461bf5", "ref_doc_id": "8ae3847f-a505-4e7f-9a0c-05cd6cdc9112"}, "8b0eee68-8e4e-477f-bdbd-f33a8446a1eb": {"doc_hash": "c459119465e1db7754e3e99b570b84a65579edb8d39ddb8e44b62612925315f0", "ref_doc_id": "7054c97e-a62b-4ff0-9233-358d37455907"}, "e54a70f9-2e5f-460d-b90a-5514a5f76885": {"doc_hash": "3845eaf3b1be8c18164f43b9db61ac421530c101d2517a268fd1b2a01ff4e012", "ref_doc_id": "a659c6a2-3b84-4066-acaa-8798044ee557"}, "cc41af11-f759-4221-ac56-a05855fc1ee1": {"doc_hash": "7f5bbb9b0b18e3ff6941db4288da516bd62c151970a2c5300800f7a39b7c00f6", "ref_doc_id": "5f68befb-8cb8-4dfe-b5cd-8ba2de4042e0"}, "a0bf00a8-bab2-48a8-8cda-bbf4e22454b3": {"doc_hash": "656aa632f1f125cc46a585281be07e0d67a67e2cc1bcf83d6c916123c46261af", "ref_doc_id": "a6f08484-e26b-4953-9a42-807bca5f22f1"}, "d46358f1-3d67-4719-bb4e-db223a03c4da": {"doc_hash": "807074bd0a9d4c3e842cc2b1da26c620eeb070cb631819b6f4cf9f9534a25b09", "ref_doc_id": "a6f08484-e26b-4953-9a42-807bca5f22f1"}, "59f22d61-12a1-41b6-9d16-a0913744d92d": {"doc_hash": "5c076b5ea31be443deec0a9c97e67b9e39459b66be6cdbe0fc0ec2c84963fda8", "ref_doc_id": "9943056c-56a8-41f4-b21c-d92534f5aece"}, "46754bd8-104c-4b62-9eb6-11aad7c55686": {"doc_hash": "a1bea107d72617655929e9cf1a760f30664f422132c766121ab3661d14fdca72", "ref_doc_id": "cc6d33ba-4593-43fc-bd2f-2465e7bb1077"}, "4b4e0ffd-ffd4-4151-aece-d516c61d9865": {"doc_hash": "6714dc8fcf240a24f7ef500ac053b4dd4935fc5393366cf00b87e0d9aba2cdd0", "ref_doc_id": "cc6d33ba-4593-43fc-bd2f-2465e7bb1077"}, "8b7c76b4-e976-4279-931b-454636871612": {"doc_hash": "1d322ce9101791904421d8575677ab7e8df13e58a2e7c0a43f5f74a55be6a1c6", "ref_doc_id": "35c57f5e-f676-4e3b-b4e7-7cd353a50dc4"}, "8ec02813-5214-40e8-a169-792c64d6543a": {"doc_hash": "b12dcec6b90db382c00a15caa9114a4fc8a9301a97a68bc18ddbb71615e679a8", "ref_doc_id": "35c57f5e-f676-4e3b-b4e7-7cd353a50dc4"}, "bbf0839c-8634-43c8-875d-5da2fbd2e309": {"doc_hash": "e6597b4736c3f09331e2f1302406ccc15f3ee2e63046f902e723522d2d526669", "ref_doc_id": "47b8b997-1352-42a6-9c13-40907baa2ffb"}, "eda7da7c-da25-4650-a21a-bf4eecf0c572": {"doc_hash": "3f8d35ffa1a4547ec21b132450c7f77cf1f339c2d8329a5b940c4c4c1ff5593b", "ref_doc_id": "47b8b997-1352-42a6-9c13-40907baa2ffb"}, "b7c47ce3-fb62-4a91-9dee-de6d199cdbf3": {"doc_hash": "e5becb4a65766190c91b53fdabe58cdc9f3f9d6d26765e75bf9bb82bb3c5a9ba", "ref_doc_id": "5db188b7-2661-49a3-a1f2-c226e5132704"}, "be0ae17f-7f3c-451c-aa37-b9f3917863c1": {"doc_hash": "05d35aa8763ca65e58aa7cf61bc58663647649e73429f975e0486763dd058694", "ref_doc_id": "5db188b7-2661-49a3-a1f2-c226e5132704"}, "0fdd14b2-5370-425d-9063-009d3ac37a85": {"doc_hash": "ae2ff9a84d049000c5ce08e6f04c9b04a4c174d644439e496045a6e350d0a8b6", "ref_doc_id": "15fbef4c-46b7-4d87-829d-200106fa753a"}, "2a8060fc-1044-4667-bc98-86fd9dfbfef8": {"doc_hash": "b9e4876057ea05c7302ad567c6bb2faaa99d5ac9102ac18a120586434923aaf8", "ref_doc_id": "96944c55-fd43-4667-9b61-492540397ba0"}, "157723df-b07b-4cdb-b71f-53fc28a83b69": {"doc_hash": "7f201bf7cbe1ce0cad0f32e061b24b90581378213c70b40318bd3153b3b7a8a9", "ref_doc_id": "6c3243a6-66b9-44b1-931e-b6c563088ffb"}, "7704b3c9-d7d7-4860-87e2-fead97ad9560": {"doc_hash": "d94c133019c7bf282a24f296be0bc1a428f0b32f04a3c61d8977ee3a17a2a630", "ref_doc_id": "53a91575-5885-4b08-ad3d-0cf3bfe21722"}, "a60269f5-5e8c-486f-99d9-954a352676b0": {"doc_hash": "824caa6c1d25d09ec22a96ddc186b79f7a028ad540f697f41f0b4287a739dac1", "ref_doc_id": "346b92a8-56bc-498e-94c1-e4a5fadb1e02"}, "cdb455b8-2a93-45a2-90d8-4f6a1cce69ae": {"doc_hash": "34dd86354b3066faa8bd8d14c7c5bafeec1d9f7ca028d39635baabc9b6122a58", "ref_doc_id": "e5449ccf-2bf2-4bc0-aaf9-081bd938dfc5"}, "735ee020-1d0e-4528-84d8-162e3743ac09": {"doc_hash": "dac4cdf5b0481be578287bf86adcd866b9fc045a1fa75f4cb8a24801f02723ce", "ref_doc_id": "de77cfa4-550c-4151-b98f-3ff1ddb3e672"}, "d234f88b-c0d9-44f0-bd3e-dcd26ee976ed": {"doc_hash": "8c6a75c06f18dca0c0595a175cdb548ab551cd14985a46d380a457f7c042bf81", "ref_doc_id": "111e0d8d-9ea7-46d1-97be-86c885590e7b"}, "eb78dad9-0a0d-439b-a7e9-6314181b706c": {"doc_hash": "2194c74ca6759b8e5761a9141e445d7c661cfdab52ff1a5819e7fe67af907015", "ref_doc_id": "2c0724c7-6f79-4b64-8e0a-adeda852ae02"}, "49796e13-fa2e-4cb9-9f64-4dd8831143d8": {"doc_hash": "1c045a76bc46ae45479d0ec1958321375142faf6d69ed2641b8538407b7ba01c", "ref_doc_id": "6c141fc2-7deb-4e13-a2e5-f9f7282e0823"}, "bb7ef439-7f96-4fd4-90ce-ea866a9c1053": {"doc_hash": "8548e7d01ce43b44e1284b08985dc6bd4c0542dcfdc73f731e8bfad7d4f4a51c", "ref_doc_id": "e1a594ef-52f6-43fe-a7d5-8cc90d2fdc5a"}, "caec7162-7ba7-4a2a-be70-c38ea68e2601": {"doc_hash": "e8d59a3adfe965c238220c9b3ee03479e2b4b73329991b668f75e19fb41459cb", "ref_doc_id": "d2750f85-eeba-40dc-a6f9-f80310dc43c6"}, "ba7d32a4-4f2c-475a-865e-ea5e89ac65d3": {"doc_hash": "6c72744f502d66ce72e09a5071fa39dfbf5a3d7b290b7a725e5b9526b86b7067", "ref_doc_id": "1427bb47-aac0-482f-b2d4-09041d58bd78"}, "d18414f6-ea25-4b2b-a279-0c01d47647f8": {"doc_hash": "294cc66517dec033828386cee867f6895a43ea0486c2acd70337418fa71764dd", "ref_doc_id": "353a2d97-af49-4f77-af61-d0b1b433f604"}, "de0cdc44-f78a-4c5f-9efe-3562d03ae5a6": {"doc_hash": "765995901d3e887b40e4c600f06d0b373a3f4d96f63ada0f209a5acf0f80121a", "ref_doc_id": "c09ae629-23c8-4815-b115-5070dbda2856"}, "ef14231c-4800-469c-8491-a86160014866": {"doc_hash": "dddca4b725ef169814c19d5ac149642118d193bf11c782d3f9761f4f289e5099", "ref_doc_id": "4318d69b-6d75-423c-97c4-b0988a2f4c89"}, "62392769-18d6-40a3-957f-94d4508fbf7e": {"doc_hash": "fac5874b42abf03c62b40f44860d7df44a3cfbe626a64090e95a616663c83932", "ref_doc_id": "72ebd1e0-7104-49f3-ac43-b44ec7632f13"}, "559e300e-957d-4821-b428-f17e6313bd51": {"doc_hash": "6bdee5f99060d1bd0ab0fb19dac66ead3253a685b3c420d17e3cda3f74ad76ed", "ref_doc_id": "5875b819-8e5a-423f-8899-130c92be13fe"}, "d19f5675-40df-4feb-ad44-1d51451b9fcb": {"doc_hash": "0e11baa2ce0ef02ae8f002fd425994b6e4bf6c31404bb67c717502889ed803dd", "ref_doc_id": "5875b819-8e5a-423f-8899-130c92be13fe"}, "1ec347d1-6a1a-409c-8927-010db948f034": {"doc_hash": "67134074e40252ab4c8e9bbf0be6b2ed925e1043b1a786280591898ef92f6981", "ref_doc_id": "68e1646a-3244-46e4-8a48-3f40d635d268"}, "6f96506f-1ada-43d2-bbe1-479bce31204d": {"doc_hash": "cd3cee244b387bf4b7e2f52baefc28101ace594a8aa6decf8087b905ff4ffb48", "ref_doc_id": "68e1646a-3244-46e4-8a48-3f40d635d268"}, "fc1eadf7-c33e-436a-b99f-4ed1019f1f7e": {"doc_hash": "abfbdd4fe66d6bc45586be7da391b28b27adf27148e429776b5b81bb7de34071", "ref_doc_id": "1faeee5b-c3b6-4869-a10f-be0ef3f2eb9f"}}, "docstore/data": {"e5635af9-1fdb-4d2f-b615-2e81e2d4e513": {"__data__": {"id_": "e5635af9-1fdb-4d2f-b615-2e81e2d4e513", "embedding": null, "metadata": {"page_label": "1", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "549e1b3b-a091-4f8d-abe1-2244b6980d26", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c64f7906f2321f5cce7d3e1675e521cb3145708cab73f76b2f14ba62a73ff1bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8143711d-516d-42f8-a52e-97dd35b07675", "node_type": "1", "metadata": {}, "hash": "3a3e06e33a2a5993f58f62cbe5326a232bb563b79b741377d69dd432f9382239", "class_name": "RelatedNodeInfo"}}, "text": "LLaMA: Open and Ef\ufb01cient Foundation Language Models\nHugo Touvron\u2217, Thibaut Lavril\u2217, Gautier Izacard\u2217, Xavier Martinet\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi\u00e8re, Naman Goyal\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\nEdouard Grave\u2217, Guillaume Lample\u2217\nMeta AI\nAbstract\nWe introduce LLaMA, a collection of founda-\ntion language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly avail-\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA-\n65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community1.\n1 Introduction\nLarge Languages Models (LLMs) trained on mas-\nsive corpora of texts have shown their ability to per-\nform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties \ufb01rst appeared when scaling models to a\nsuf\ufb01cient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\nThese efforts are based on the assumption that\nmore parameters will lead to better performance.\nHowever, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest mod-\nels, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff-\nmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale.\nIn this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n\u2217Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we \ufb01nd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 \u00d7smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. \u201cBooks \u2013 2TB\u201d or\n\u201cSocial media conversations\u201d). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.", "start_char_idx": 0, "end_char_idx": 3593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8143711d-516d-42f8-a52e-97dd35b07675": {"__data__": {"id_": "8143711d-516d-42f8-a52e-97dd35b07675", "embedding": null, "metadata": {"page_label": "1", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "549e1b3b-a091-4f8d-abe1-2244b6980d26", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c64f7906f2321f5cce7d3e1675e521cb3145708cab73f76b2f14ba62a73ff1bf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5635af9-1fdb-4d2f-b615-2e81e2d4e513", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d85c753ca1f22ee9e21f85905fdc0c6dcfa4b5eb2c533b71a5fdef3694d5635f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "016a780b-82bf-48dc-b7a0-6d2f5005a13d", "node_type": "1", "metadata": {}, "hash": "96ff673a2244ac067dd54391970345b43d9a7518f53d74415bde924754700718", "class_name": "RelatedNodeInfo"}}, "text": "At the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. \u201cBooks \u2013 2TB\u201d or\n\u201cSocial media conversations\u201d). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modi\ufb01cations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from\nthe responsible AI community.arXiv:2302.13971v1  [cs.CL]  27 Feb 2023", "start_char_idx": 2969, "end_char_idx": 4056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "016a780b-82bf-48dc-b7a0-6d2f5005a13d": {"__data__": {"id_": "016a780b-82bf-48dc-b7a0-6d2f5005a13d", "embedding": null, "metadata": {"page_label": "2", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "937eebbd-b7e2-42b6-b629-f4516bba1bce", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "82617250d56ae1c4411eae072d7825ba53c7eb80053707ca20601b848f2e9316", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8143711d-516d-42f8-a52e-97dd35b07675", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "57d6a5e5b80fc9cf491b7f6b272d67b7ebb4bb0b102c53b1101b771808e314fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f93ba42-f0fd-421f-92dc-ee6f7778d77e", "node_type": "1", "metadata": {}, "hash": "2371007eed70660983782cdd9f737eea1ef85e5b87b8990242f44f9bc84f6bf4", "class_name": "RelatedNodeInfo"}}, "text": "2 Approach\nOur training approach is similar to the methods\ndescribed in previous work (Brown et al., 2020;\nChowdhery et al., 2022), and is inspired by the\nChinchilla scaling laws (Hoffmann et al., 2022).\nWe train large transformers on a large quantity of\ntextual data using a standard optimizer.\n2.1 Pre-training Data\nOur training dataset is a mixture of several sources,\nreported in Table 1, that cover a diverse set of do-\nmains. For the most part, we reuse data sources\nthat have been leveraged to train other LLMs, with\nthe restriction of only using data that is publicly\navailable, and compatible with open sourcing. This\nleads to the following mixture of data and the per-\ncentage they represent in the training set:\nEnglish CommonCrawl [67%]. We preprocess\n\ufb01ve CommonCrawl dumps, ranging from 2017\nto 2020, with the CCNet pipeline (Wenzek et al.,\n2020). This process deduplicates the data at the\nline level, performs language identi\ufb01cation with\na fastText linear classi\ufb01er to remove non-English\npages and \ufb01lters low quality content with an n-\ngram language model. In addition, we trained a\nlinear model to classify pages used as references\nin Wikipedia v.s.randomly sampled pages, and\ndiscarded pages not classi\ufb01ed as references.\nC4 [15%]. During exploratory experiments, we\nobserved that using diverse pre-processed Com-\nmonCrawl datasets improves performance. We thus\nincluded the publicly available C4 dataset (Raffel\net al., 2020) in our data. The preprocessing of C4\nalso contains deduplication and language identi\ufb01-\ncation steps: the main difference with CCNet is\nthe quality \ufb01ltering, which mostly relies on heuris-\ntics such as presence of punctuation marks or the\nnumber of words and sentences in a webpage.\nGithub [4.5%]. We use the public GitHub\ndataset available on Google BigQuery. We only\nkept projects that are distributed under the Apache,\nBSD and MIT licenses. Additionally, we \ufb01ltered\nlow quality \ufb01les with heuristics based on the line\nlength or proportion of alphanumeric characters,\nand removed boilerplate, such as headers, with reg-\nular expressions. Finally, we deduplicate the result-\ning dataset at the \ufb01le level, with exact matches.\nWikipedia [4.5%]. We add Wikipedia dumps\nfrom the June-August 2022 period, covering 20Dataset Sampling prop. Epochs Disk size\nCommonCrawl 67.0% 1.10 3.3 TB\nC4 15.0% 1.06 783 GB\nGithub 4.5% 0.64 328 GB\nWikipedia 4.5% 2.45 83 GB\nBooks 4.5% 2.23 85 GB\nArXiv 2.5% 1.06 92 GB\nStackExchange 2.0% 1.03 78 GB\nTable 1: Pre-training data. Data mixtures used for pre-\ntraining, for each subset we list the sampling propor-\ntion, number of epochs performed on the subset when\ntraining on 1.4T tokens, and disk size. The pre-training\nruns on 1T tokens have the same sampling proportion.\nlanguages, which use either the Latin or Cyrillic\nscripts: bg,ca,cs,da,de,en,es,fr,hr,hu,it,\nnl,pl,pt,ro,ru,sl,sr,sv,uk. We process the\ndata to remove hyperlinks, comments and other\nformatting boilerplate.\nGutenberg and Books3 [4.5%]. We include\ntwo book corpora in our training dataset: the Guten-\nberg Project, which contains books that are in the\npublic domain, and the Books3 section of TheP-\nile (Gao et al., 2020), a publicly available dataset\nfor training large language models. We perform\ndeduplication at the book level, removing books\nwith more than 90% content overlap.\nArXiv [2.5%]. We process arXiv Latex \ufb01les\nto add scienti\ufb01c data to our dataset. Following\nLewkowycz et al. (2022), we removed everything\nbefore the \ufb01rst section, as well as the bibliography.", "start_char_idx": 0, "end_char_idx": 3511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f93ba42-f0fd-421f-92dc-ee6f7778d77e": {"__data__": {"id_": "5f93ba42-f0fd-421f-92dc-ee6f7778d77e", "embedding": null, "metadata": {"page_label": "2", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "937eebbd-b7e2-42b6-b629-f4516bba1bce", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "82617250d56ae1c4411eae072d7825ba53c7eb80053707ca20601b848f2e9316", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "016a780b-82bf-48dc-b7a0-6d2f5005a13d", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "396a5bd638d5f7c89e0ba10008b3e33fc1b6a7f7360706d168e886e09dfe0ef2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "831d9a2b-5ccf-47d0-8351-161ef96e0023", "node_type": "1", "metadata": {}, "hash": "7fec795e70bb27cfdcf59237516e331befeccf2c6b9cd5e1fe3dae271b668956", "class_name": "RelatedNodeInfo"}}, "text": "We process the\ndata to remove hyperlinks, comments and other\nformatting boilerplate.\nGutenberg and Books3 [4.5%]. We include\ntwo book corpora in our training dataset: the Guten-\nberg Project, which contains books that are in the\npublic domain, and the Books3 section of TheP-\nile (Gao et al., 2020), a publicly available dataset\nfor training large language models. We perform\ndeduplication at the book level, removing books\nwith more than 90% content overlap.\nArXiv [2.5%]. We process arXiv Latex \ufb01les\nto add scienti\ufb01c data to our dataset. Following\nLewkowycz et al. (2022), we removed everything\nbefore the \ufb01rst section, as well as the bibliography.\nWe also removed the comments from the .tex \ufb01les,\nand inline-expanded de\ufb01nitions and macros written\nby users to increase consistency across papers.\nStack Exchange [2%]. We include a dump of\nStack Exchange, a website of high quality ques-\ntions and answers that covers a diverse set of do-\nmains, ranging from computer science to chemistry.\nWe kept the data from the 28 largest websites, re-\nmoved the HTML tags from text and sorted the\nanswers by score (from highest to lowest).\nTokenizer. We tokenize the data with the byte-\npair encoding (BPE) algorithm (Sennrich et al.,\n2015), using the implementation from Sentence-\nPiece (Kudo and Richardson, 2018). Notably, we\nsplit all numbers into individual digits, and fallback\nto bytes to decompose unknown UTF-8 characters.", "start_char_idx": 2861, "end_char_idx": 4281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "831d9a2b-5ccf-47d0-8351-161ef96e0023": {"__data__": {"id_": "831d9a2b-5ccf-47d0-8351-161ef96e0023", "embedding": null, "metadata": {"page_label": "3", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13e5a54c-cb0c-48f3-a507-72968247b25f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bff7f33e1d35e971e4d6c0a062f62355f3f98f048d28ee9d2f3a9e3e93d60015", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f93ba42-f0fd-421f-92dc-ee6f7778d77e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "ceac7a6ad4fc22d8bdc0552cb51c0cc69846fee7ad1494f115cfdb9f6b901634", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf77674c-7216-4516-8e33-606cad88487a", "node_type": "1", "metadata": {}, "hash": "7e76ca303c6219b1c1a69d25874a7661b6263ee7af84a31027e64550c2028250", "class_name": "RelatedNodeInfo"}}, "text": "params dimension nheadsnlayers learning rate batch size ntokens\n6.7B 4096 32 32 3.0e\u221244M 1.0T\n13.0B 5120 40 40 3.0e\u221244M 1.0T\n32.5B 6656 52 60 1.5e\u221244M 1.4T\n65.2B 8192 64 80 1.5e\u221244M 1.4T\nTable 2: Model sizes, architectures, and optimization hyper-parameters.\nOverall, our entire training dataset contains\nroughly 1.4T tokens after tokenization. For most of\nour training data, each token is used only once dur-\ning training, with the exception of the Wikipedia\nand Books domains, over which we perform ap-\nproximately two epochs.\n2.2 Architecture\nFollowing recent work on large language models,\nour network is based on the transformer architec-\nture (Vaswani et al., 2017). We leverage various\nimprovements that were subsequently proposed,\nand used in different models such as PaLM. Here\nare the main difference with the original architec-\nture, and where we were found the inspiration for\nthis change (in bracket):\nPre-normalization [GPT3]. To improve the\ntraining stability, we normalize the input of each\ntransformer sub-layer, instead of normalizing the\noutput. We use the RMSNorm normalizing func-\ntion, introduced by Zhang and Sennrich (2019).\nSwiGLU activation function [PaLM]. We re-\nplace the ReLU non-linearity by the SwiGLU ac-\ntivation function, introduced by Shazeer (2020) to\nimprove the performance. We use a dimension of\n2\n34dinstead of 4das in PaLM.\nRotary Embeddings [GPTNeo]. We remove the\nabsolute positional embeddings, and instead, add\nrotary positional embeddings (RoPE), introduced\nby Su et al. (2021), at each layer of the network.\nThe details of the hyper-parameters for our dif-\nferent models are given in Table 2.\n2.3 Optimizer\nOur models are trained using the AdamW opti-\nmizer (Loshchilov and Hutter, 2017), with the fol-\nlowing hyper-parameters: \u03b21= 0.9,\u03b22= 0.95.\nWe use a cosine learning rate schedule, such that\nthe \ufb01nal learning rate is equal to 10% of the maxi-\nmal learning rate. We use a weight decay of 0.1and\ngradient clipping of 1.0. We use 2,000warmup\n0 200 400 600 800 1000 1200 1400\nBillion of tokens1.51.61.71.81.92.02.12.2Training lossLLaMA 7B\nLLaMA 13B\nLLaMA 33B\nLLaMA 65BFigure 1: Training loss over train tokens for the 7B,\n13B, 33B, and 65 models. LLaMA-33B and LLaMA-\n65B were trained on 1.4T tokens. The smaller models\nwere trained on 1.0T tokens. All models are trained\nwith a batch size of 4M tokens.\nsteps, and vary the learning rate and batch size with\nthe size of the model (see Table 2 for details).\n2.4 Ef\ufb01cient implementation\nWe make several optimizations to improve the train-\ning speed of our models. First, we use an ef\ufb01cient\nimplementation of the causal multi-head attention\nto reduce memory usage and runtime. This imple-\nmentation, available in the xformers library,2is\ninspired by Rabe and Staats (2021) and uses the\nbackward from Dao et al. (2022). This is achieved\nby not storing the attention weights and not com-\nputing the key/query scores that are masked due to\nthe causal nature of the language modeling task.\nTo further improve training ef\ufb01ciency, we re-\nduced the amount of activations that are recom-\nputed during the backward pass with checkpoint-\ning. More precisely, we save the activations that\nare expensive to compute, such as the outputs of\nlinear layers. This is achieved by manually imple-\nmenting the backward function for the transformer\nlayers, instead of relying on the PyTorch autograd.\nTo fully bene\ufb01t from this optimization, we need to\n2https://github.com/facebookresearch/xformers", "start_char_idx": 0, "end_char_idx": 3472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf77674c-7216-4516-8e33-606cad88487a": {"__data__": {"id_": "bf77674c-7216-4516-8e33-606cad88487a", "embedding": null, "metadata": {"page_label": "4", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b99402d-2a0c-4bcb-9f25-4bafe50e25e0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "66f4121ce4187611d160fbdf1f6af14c597bc53e904fcf36834d24c750f1e083", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "831d9a2b-5ccf-47d0-8351-161ef96e0023", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bff7f33e1d35e971e4d6c0a062f62355f3f98f048d28ee9d2f3a9e3e93d60015", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23d7c70f-f784-43f9-a515-708542c786c2", "node_type": "1", "metadata": {}, "hash": "f25faec287397617a509bbdf9e2ff9c9256af2d04a157fb20d466d12f69a55b7", "class_name": "RelatedNodeInfo"}}, "text": "BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA\nGPT-3 175B 60.5 81.0 - 78.9 70.2 68.8 51.4 57.6\nGopher 280B 79.3 81.8 50.6 79.2 70.1 - - -\nChinchilla 70B 83.7 81.8 51.3 80.8 74.9 - - -\nPaLM 62B 84.8 80.5 - 79.7 77.0 75.2 52.5 50.4\nPaLM-cont 62B 83.9 81.4 - 80.6 77.0 - - -\nPaLM 540B 88.0 82.3 - 83.4 81.1 76.6 53.0 53.4\nLLaMA7B 76.5 79.8 48.9 76.1 70.1 72.8 47.6 57.2\n13B 78.1 80.1 50.4 79.2 73.0 74.8 52.7 56.4\n33B 83.1 82.3 50.4 82.8 76.0 80.0 57.8 58.6\n65B 85.3 82.8 52.3 84.2 77.0 78.9 56.0 60.2\nTable 3: Zero-shot performance on Common Sense Reasoning tasks.\nreduce the memory usage of the model by using\nmodel and sequence parallelism, as described by\nKorthikanti et al. (2022). Moreover, we also over-\nlap the computation of activations and the commu-\nnication between GPUs over the network (due to\nall_reduce operations) as much as possible.\nWhen training a 65B-parameter model, our code\nprocesses around 380 tokens/sec/GPU on 2048\nA100 GPU with 80GB of RAM. This means that\ntraining over our dataset containing 1.4T tokens\ntakes approximately 21 days.\n3 Main results\nFollowing previous work (Brown et al., 2020), we\nconsider zero-shot and few-shot tasks, and report\nresults on a total of 20 benchmarks:\n\u2022Zero-shot. We provide a textual description\nof the task and a test example. The model\neither provides an answer using open-ended\ngeneration, or ranks the proposed answers.\n\u2022Few-shot. We provide a few examples of the\ntask (between 1 and 64) and a test example.\nThe model takes this text as input and gener-\nates the answer or ranks different options.\nWe compare LLaMA with other foundation mod-\nels, namely the non-publicly available language\nmodels GPT-3 (Brown et al., 2020), Gopher (Rae\net al., 2021), Chinchilla (Hoffmann et al., 2022)\nand PaLM (Chowdhery et al., 2022), as well as\nthe open-sourced OPT models (Zhang et al., 2022),\nGPT-J (Wang and Komatsuzaki, 2021), and GPT-\nNeo (Black et al., 2022). In Section 4, we also\nbrie\ufb02y compare LLaMA with instruction-tuned\nmodels such as OPT-IML (Iyer et al., 2022) and\nFlan-PaLM (Chung et al., 2022).We evaluate LLaMA on free-form generation\ntasks and multiple choice tasks. In the multiple\nchoice tasks, the objective is to select the most\nappropriate completion among a set of given op-\ntions, based on a provided context. We select the\ncompletion with the highest likelihood given the\nprovided context. We follow Gao et al. (2021)\nand use the likelihood normalized by the number\nof characters in the completion, except for certain\ndatasets (OpenBookQA, BoolQ), for which we fol-\nlow Brown et al. (2020), and select a completion\nbased on the likelihood normalized by the likeli-\nhood of the completion given \u201cAnswer:\u201d as context:\nP(completion|context )/P(completion|\u201cAnswer :\u201d).", "start_char_idx": 0, "end_char_idx": 2749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23d7c70f-f784-43f9-a515-708542c786c2": {"__data__": {"id_": "23d7c70f-f784-43f9-a515-708542c786c2", "embedding": null, "metadata": {"page_label": "4", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b99402d-2a0c-4bcb-9f25-4bafe50e25e0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "66f4121ce4187611d160fbdf1f6af14c597bc53e904fcf36834d24c750f1e083", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf77674c-7216-4516-8e33-606cad88487a", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e6005810a0c0805df5a91111c5f93bca0c84c9341d3daff397d21a973be71d65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51693bc7-c580-40e6-b3f6-cfb9b5552572", "node_type": "1", "metadata": {}, "hash": "de067af63351e8bf9f6e4c371a41105e8242f1ecda9b7037f4e721115176a135", "class_name": "RelatedNodeInfo"}}, "text": "In the multiple\nchoice tasks, the objective is to select the most\nappropriate completion among a set of given op-\ntions, based on a provided context. We select the\ncompletion with the highest likelihood given the\nprovided context. We follow Gao et al. (2021)\nand use the likelihood normalized by the number\nof characters in the completion, except for certain\ndatasets (OpenBookQA, BoolQ), for which we fol-\nlow Brown et al. (2020), and select a completion\nbased on the likelihood normalized by the likeli-\nhood of the completion given \u201cAnswer:\u201d as context:\nP(completion|context )/P(completion|\u201cAnswer :\u201d).\n0-shot 1-shot 5-shot 64-shot\nGPT-3 175B 14.6 23.0 - 29.9\nGopher 280B 10.1 - 24.5 28.2\nChinchilla 70B 16.6 - 31.5 35.5\nPaLM8B 8.4 10.6 - 14.6\n62B 18.1 26.5 - 27.6\n540B 21.2 29.3 - 39.6\nLLaMA7B 16.8 18.7 22.0 26.1\n13B 20.1 23.4 28.1 31.9\n33B 24.9 28.3 32.9 36.0\n65B 23.8 31.0 35.0 39.9\nTable 4: NaturalQuestions. Exact match performance.\n3.1 Common Sense Reasoning\nWe consider eight standard common sense rea-\nsoning benchmarks: BoolQ (Clark et al., 2019),\nPIQA (Bisk et al., 2020), SIQA (Sap et al., 2019),", "start_char_idx": 2144, "end_char_idx": 3255, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51693bc7-c580-40e6-b3f6-cfb9b5552572": {"__data__": {"id_": "51693bc7-c580-40e6-b3f6-cfb9b5552572", "embedding": null, "metadata": {"page_label": "5", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff21b96b-3d44-46c1-8d56-afffd419ca7b", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "8966e361b7fba709a94a09aecf31e28aac7f7cb2074b4aacd1f26fcb0fd68b16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23d7c70f-f784-43f9-a515-708542c786c2", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "702cb318daa5b05e2ed60976145b6e4f715f6652040a5726f763e512d91dde5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37d86c02-1a23-48d8-befe-9514f8aaacd2", "node_type": "1", "metadata": {}, "hash": "df8ab86412cffc201c8ef203f063b763ef552597320cf27845335a837efbb679", "class_name": "RelatedNodeInfo"}}, "text": "HellaSwag (Zellers et al., 2019), WinoGrande (Sak-\naguchi et al., 2021), ARC easy and challenge (Clark\net al., 2018) and OpenBookQA (Mihaylov et al.,\n2018). These datasets include Cloze and Winograd\nstyle tasks, as well as multiple choice question an-\nswering. We evaluate in the zero-shot setting as\ndone in the language modeling community.\nIn Table 3, we compare with existing models\nof various sizes and report numbers from the cor-\nresponding papers. First, LLaMA-65B outper-\nforms Chinchilla-70B on all reported benchmarks\nbut BoolQ. Similarly, this model surpasses PaLM-\n540B everywhere but on BoolQ and WinoGrande.\nLLaMA-13B model also outperforms GPT-3 on\nmost benchmarks despite being 10 \u00d7smaller.\n3.2 Closed-book Question Answering\nWe compare LLaMA to existing large language\nmodels on two closed-book question answering\nbenchmarks: Natural Questions (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017). For\nboth benchmarks, we report exact match perfor-\nmance in a closed book setting, i.e., where the mod-\nels do not have access to documents that contain\nevidence to answer the question. In Table 4, we\nreport performance on NaturalQuestions, and in Ta-\nble 5, we report on TriviaQA. On both benchmarks,\nLLaMA-65B achieve state-of-the-arts performance\nin the zero-shot and few-shot settings. More im-\nportantly, the LLaMA-13B is also competitive on\nthese benchmarks with GPT-3 and Chinchilla, de-\nspite being 5-10 \u00d7smaller. This model runs on a\nsingle V100 GPU during inference.\n0-shot 1-shot 5-shot 64-shot\nGopher 280B 43.5 - 57.0 57.2\nChinchilla 70B 55.4 - 64.1 64.6\nLLaMA7B 50.0 53.4 56.3 57.6\n13B 56.6 60.5 63.1 64.0\n33B 65.1 67.9 69.9 70.4\n65B 68.2 71.6 72.6 73.0\nTable 5: TriviaQA. Zero-shot and few-shot exact\nmatch performance on the \ufb01ltered dev set.\n3.3 Reading Comprehension\nWe evaluate our models on the RACE reading com-\nprehension benchmark (Lai et al., 2017). This\ndataset was collected from English reading com-\nprehension exams designed for middle and highRACE-middle RACE-high\nGPT-3 175B 58.4 45.5\nPaLM8B 57.9 42.3\n62B 64.3 47.5\n540B 68.1 49.1\nLLaMA7B 61.1 46.9\n13B 61.6 47.2\n33B 64.1 48.3\n65B 67.9 51.6\nTable 6: Reading Comprehension. Zero-shot accu-\nracy.\nschool Chinese students. We follow the evaluation\nsetup from Brown et al. (2020) and report results\nin Table 6. On these benchmarks, LLaMA-65B is\ncompetitive with PaLM-540B, and, LLaMA-13B\noutperforms GPT-3 by a few percents.\n3.4 Mathematical reasoning\nWe evaluate our models on two mathematical rea-\nsoning benchmarks: MATH (Hendrycks et al.,\n2021) and GSM8k (Cobbe et al., 2021). MATH\nis a dataset of 12K middle school and high school\nmathematics problems written in LaTeX. GSM8k\nis a set of middle school mathematical problems.\nIn Table 7, we compare with PaLM and Min-\nerva (Lewkowycz et al., 2022). Minerva is a series\nof PaLM models \ufb01netuned on 38.5B tokens ex-\ntracted from ArXiv and Math Web Pages, while\nneither PaLM or LLaMA are \ufb01netuned on mathe-\nmatical data.", "start_char_idx": 0, "end_char_idx": 2967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37d86c02-1a23-48d8-befe-9514f8aaacd2": {"__data__": {"id_": "37d86c02-1a23-48d8-befe-9514f8aaacd2", "embedding": null, "metadata": {"page_label": "5", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff21b96b-3d44-46c1-8d56-afffd419ca7b", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "8966e361b7fba709a94a09aecf31e28aac7f7cb2074b4aacd1f26fcb0fd68b16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51693bc7-c580-40e6-b3f6-cfb9b5552572", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "53f6b429b7d29cf45383e36a027d765654c766f1ae1526b20399cae8640ceb27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e47eba8-3f53-4e69-a077-c58782bde03c", "node_type": "1", "metadata": {}, "hash": "f425772c43f485ca1e24f4e4fab05f231caaf4c70302a16323f590a1a626f746", "class_name": "RelatedNodeInfo"}}, "text": "3.4 Mathematical reasoning\nWe evaluate our models on two mathematical rea-\nsoning benchmarks: MATH (Hendrycks et al.,\n2021) and GSM8k (Cobbe et al., 2021). MATH\nis a dataset of 12K middle school and high school\nmathematics problems written in LaTeX. GSM8k\nis a set of middle school mathematical problems.\nIn Table 7, we compare with PaLM and Min-\nerva (Lewkowycz et al., 2022). Minerva is a series\nof PaLM models \ufb01netuned on 38.5B tokens ex-\ntracted from ArXiv and Math Web Pages, while\nneither PaLM or LLaMA are \ufb01netuned on mathe-\nmatical data. The numbers for PaLM and Minerva\nare taken from Lewkowycz et al. (2022), and we\ncompare with and without maj1@k .maj1@k de-\nnotes evaluations where we generate ksamples for\neach problem and perform a majority voting (Wang\net al., 2022). On GSM8k, we observe that LLaMA-\n65B outperforms Minerva-62B, although it has not\nbeen \ufb01ne-tuned on mathematical data.\n3.5 Code generation\nWe evaluate the ability of our models to write\ncode from a natural language description on two\nbenchmarks: HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021). For both tasks, the\nmodel receives a description of the program in a\nfew sentences, as well as a few input-output ex-\namples. In HumanEval, it also receives a function\nsignature, and the prompt is formatted as natural\ncode with the textual description and tests in a", "start_char_idx": 2422, "end_char_idx": 3779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e47eba8-3f53-4e69-a077-c58782bde03c": {"__data__": {"id_": "9e47eba8-3f53-4e69-a077-c58782bde03c", "embedding": null, "metadata": {"page_label": "6", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b73c1dad-7c1d-4a8c-a02b-f98bfa7ed10d", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "3cfb041d8034b74d215e8aab534b25e369d51b0a81e498638c76b54e21b1fb16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37d86c02-1a23-48d8-befe-9514f8aaacd2", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "2fbe8a9ae295bc1cdd519298dd7204180e71eb15e0f773e0a4371ace68776eb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50e1968e-96a5-419c-be50-f4525e3c95fb", "node_type": "1", "metadata": {}, "hash": "35c33cd9d5e54003a01ad451148c97ee6b6eb448000ad38bfa37f46f22716f23", "class_name": "RelatedNodeInfo"}}, "text": "MATH +maj1@k GSM8k +maj1@k\nPaLM8B 1.5 - 4.1 -\n62B 4.4 - 33.0 -\n540B 8.8 - 56.5 -\nMinerva8B 14.1 25.4 16.2 28.4\n62B 27.6 43.4 52.4 68.5\n540B 33.6 50.3 68.5 78.5\nLLaMA7B 2.9 6.9 11.0 18.1\n13B 3.9 8.8 17.8 29.3\n33B 7.1 15.2 35.6 53.1\n65B 10.6 20.5 50.9 69.7\nTable 7: Model performance on quantitative reason-\ning datasets. For majority voting, we use the same\nsetup as Minerva, with k= 256 samples for MATH\nandk= 100 for GSM8k (Minerva 540B uses k= 64\nfor MATH and and k= 40 for GSM8k). LLaMA-65B\noutperforms Minerva 62B on GSM8k, although it has\nnot been \ufb01ne-tuned on mathematical data.\ndocstring. The model needs to generate a Python\nprogram that \ufb01ts the description and satis\ufb01es the\ntest cases. In Table 8, we compare the pass@1\nscores of our models with existing language mod-\nels that have not been \ufb01netuned on code, namely\nPaLM and LaMDA (Thoppilan et al., 2022). PaLM\nand LLaMA were trained on datasets that contain\na similar number of code tokens.\nAs show in Table 8, for a similar number\nof parameters, LLaMA outperforms other gen-\neral models such as LaMDA and PaLM, which\nare not trained or \ufb01netuned speci\ufb01cally for code.\nLLaMA with 13B parameters and more outper-\nforms LaMDA 137B on both HumanEval and\nMBPP. LLaMA 65B also outperforms PaLM 62B,\neven when it is trained longer. The pass@1 results\nreported in this table were obtained by sampling\nwith temperature 0.1. The pass@100 and pass@80\nmetrics were obtained with temperature 0.8. We\nuse the same method as Chen et al. (2021) to obtain\nunbiased estimates of the pass@k.\nIt is possible to improve the performance on code\nby \ufb01netuning on code-speci\ufb01c tokens. For instance,\nPaLM-Coder (Chowdhery et al., 2022) increases\nthe pass@1 score of PaLM on HumanEval from\n26.2% for PaLM to 36%. Other models trained\nspeci\ufb01cally for code also perform better than gen-\neral models on these tasks (Chen et al., 2021; Ni-\njkamp et al., 2022; Fried et al., 2022). Finetuning\non code tokens is beyond the scope of this paper.Params HumanEval MBPP\npass@ @1 @100 @1 @80\nLaMDA 137B 14.0 47.3 14.8 62.4\nPaLM 8B 3.6\u221718.7\u22175.0\u221735.7\u2217\nPaLM 62B 15.9 46.3\u221721.4 63.2\u2217\nPaLM-cont 62B 23.7 - 31.2 -\nPaLM 540B 26.2 76.2 36.8 75.0\nLLaMA7B 10.5 36.5 17.7 56.2\n13B 15.8 52.5 22.0 64.0\n33B 21.7 70.7 30.2 73.4\n65B 23.7 79.3 37.7 76.8\nTable 8: Model performance for code generation.\nWe report the pass@ score on HumanEval and MBPP.\nHumanEval generations are done in zero-shot and\nMBBP with 3-shot prompts similar to Austin et al.\n(2021). The values marked with\u2217are read from \ufb01gures\nin Chowdhery et al.", "start_char_idx": 0, "end_char_idx": 2527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50e1968e-96a5-419c-be50-f4525e3c95fb": {"__data__": {"id_": "50e1968e-96a5-419c-be50-f4525e3c95fb", "embedding": null, "metadata": {"page_label": "6", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b73c1dad-7c1d-4a8c-a02b-f98bfa7ed10d", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "3cfb041d8034b74d215e8aab534b25e369d51b0a81e498638c76b54e21b1fb16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e47eba8-3f53-4e69-a077-c58782bde03c", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "ac9f5a847c65f00d668092e99b8731fe5be582ff6960890d4a3f67c9c3abba9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1243974e-f8e4-4a2e-8add-f662318bbe43", "node_type": "1", "metadata": {}, "hash": "09e7af8d6653ec480acfbfa3ac86f0d2821b68aed20244d5de5bf8f310968099", "class_name": "RelatedNodeInfo"}}, "text": "We report the pass@ score on HumanEval and MBPP.\nHumanEval generations are done in zero-shot and\nMBBP with 3-shot prompts similar to Austin et al.\n(2021). The values marked with\u2217are read from \ufb01gures\nin Chowdhery et al. (2022).\n3.6 Massive Multitask Language\nUnderstanding\nThe massive multitask language understanding\nbenchmark, or MMLU, introduced by Hendrycks\net al. (2020) consists of multiple choice questions\ncovering various domains of knowledge, includ-\ning humanities, STEM and social sciences. We\nevaluate our models in the 5-shot setting, using the\nexamples provided by the benchmark, and report\nresults in Table 9. On this benchmark, we observe\nthat the LLaMA-65B is behind both Chinchilla-\n70B and PaLM-540B by a few percent in average,\nand across most domains. A potential explanation\nis that we have used a limited amount of books\nand academic papers in our pre-training data, i.e.,\nArXiv, Gutenberg and Books3, that sums up to only\n177GB, while these models were trained on up to\n2TB of books. This large quantity of books used\nby Gopher, Chinchilla and PaLM may also explain\nwhy Gopher outperforms GPT-3 on this benchmark,\nwhile it is comparable on other benchmarks.\n3.7 Evolution of performance during training\nDuring training, we tracked the performance of our\nmodels on a few question answering and common\nsense benchmarks, and report them in Figure 2.\nOn most benchmarks, the performance improves\nsteadily, and correlates with the training perplexity\nof the model (see Figure 1). The exceptions are\nSIQA and WinoGrande. Most notably, on SIQA,\nwe observe a lot of variance in performance,", "start_char_idx": 2309, "end_char_idx": 3915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1243974e-f8e4-4a2e-8add-f662318bbe43": {"__data__": {"id_": "1243974e-f8e4-4a2e-8add-f662318bbe43", "embedding": null, "metadata": {"page_label": "7", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a96f8ac-22d2-48de-980c-6a61390c23fa", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bdd7d1c7a4be680f7e2722405f0c5f79095edb43ae76de47ec243a5dae2b43b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50e1968e-96a5-419c-be50-f4525e3c95fb", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "7f0ec04d40628c8604f32b96cb03670bdd91a69b81d422771d79967d5490f91b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b714e85-9b55-436d-8850-c7248d366e94", "node_type": "1", "metadata": {}, "hash": "6843ff1f3915ec8091e6eb3515c4a0fc78b699672888db820607bc304cb294ee", "class_name": "RelatedNodeInfo"}}, "text": "Humanities STEM Social Sciences Other Average\nGPT-NeoX 20B 29.8 34.9 33.7 37.7 33.6\nGPT-3 175B 40.8 36.7 50.4 48.8 43.9\nGopher 280B 56.2 47.4 71.9 66.1 60.0\nChinchilla 70B 63.6 54.9 79.3 73.9 67.5\nPaLM8B 25.6 23.8 24.1 27.8 25.4\n62B 59.5 41.9 62.7 55.8 53.7\n540B 77.0 55.6 81.0 69.6 69.3\nLLaMA7B 34.0 30.5 38.3 38.1 35.1\n13B 45.0 35.8 53.8 53.3 46.9\n33B 55.8 46.0 66.7 63.4 57.8\n65B 61.8 51.7 72.9 67.4 63.4\nTable 9: Massive Multitask Language Understanding (MMLU). Five-shot accuracy.\nthat may indicate that this benchmark is not\nreliable. On WinoGrande, the performance does\nnot correlate as well with training perplexity:\nthe LLaMA-33B and LLaMA-65B have similar\nperformance during the training.\n4 Instruction Finetuning\nIn this section, we show that brie\ufb02y \ufb01netuning on\ninstructions data rapidly leads to improvements\non MMLU. Although the non-\ufb01netuned version\nof LLaMA-65B is already able to follow basic in-\nstructions, we observe that a very small amount of\n\ufb01netuning improves the performance on MMLU,\nand further improves the ability of the model to\nfollow instructions. Since this is not the focus of\nthis paper, we only conducted a single experiment\nfollowing the same protocol as Chung et al. (2022)\nto train an instruct model, LLaMA-I.\nOPT 30B 26.1\nGLM 120B 44.8\nPaLM 62B 55.1\nPaLM-cont 62B 62.8\nChinchilla 70B 67.5\nLLaMA 65B 63.4\nOPT-IML-Max 30B 43.2\nFlan-T5-XXL 11B 55.1\nFlan-PaLM 62B 59.6\nFlan-PaLM-cont 62B 66.1\nLLaMA-I 65B 68.9\nTable 10: Instruction \ufb01netuning \u2013 MMLU (5-shot).\nComparison of models of moderate size with and with-\nout instruction \ufb01netuning on MMLU.In Table 10, we report the results of our instruct\nmodel LLaMA-I on MMLU and compare with ex-\nisting instruction \ufb01netuned models of moderate\nsizes, namely, OPT-IML (Iyer et al., 2022) and the\nFlan-PaLM series (Chung et al., 2022). All the re-\nported numbers are from the corresponding papers.\nDespite the simplicity of the instruction \ufb01netuning\napproach used here, we reach 68.9% on MMLU.\nLLaMA-I (65B) outperforms on MMLU existing\ninstruction \ufb01netuned models of moderate sizes, but\nare still far from the state-of-the-art, that is 77.4\nfor GPT code-davinci-002 on MMLU (numbers\ntaken from Iyer et al. (2022)). The details of the\nperformance on MMLU on the 57 tasks can be\nfound in Table 16 of the appendix.\n5 Bias, Toxicity and Misinformation\nLarge language models have been showed to re-\nproduce and amplify biases that are existing in\nthe training data (Sheng et al., 2019; Kurita et al.,\n2019), and to generate toxic or offensive con-\ntent (Gehman et al., 2020). As our training dataset\ncontains a large proportion of data from the Web,\nwe believe that it is crucial to determine the po-\ntential for our models to generate such content.", "start_char_idx": 0, "end_char_idx": 2721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b714e85-9b55-436d-8850-c7248d366e94": {"__data__": {"id_": "8b714e85-9b55-436d-8850-c7248d366e94", "embedding": null, "metadata": {"page_label": "7", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a96f8ac-22d2-48de-980c-6a61390c23fa", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bdd7d1c7a4be680f7e2722405f0c5f79095edb43ae76de47ec243a5dae2b43b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1243974e-f8e4-4a2e-8add-f662318bbe43", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "0ddd889cd5cfdc3ce830c3793e7807feb9d81f2798b288b2093a26ed3a14d765", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70b0c482-be1e-4f37-b892-419da9e187de", "node_type": "1", "metadata": {}, "hash": "6e16329ea62de3d2bfe13b8341ede667fdef062ab41bd7a7c129dfbf1baa6843", "class_name": "RelatedNodeInfo"}}, "text": "(2022)). The details of the\nperformance on MMLU on the 57 tasks can be\nfound in Table 16 of the appendix.\n5 Bias, Toxicity and Misinformation\nLarge language models have been showed to re-\nproduce and amplify biases that are existing in\nthe training data (Sheng et al., 2019; Kurita et al.,\n2019), and to generate toxic or offensive con-\ntent (Gehman et al., 2020). As our training dataset\ncontains a large proportion of data from the Web,\nwe believe that it is crucial to determine the po-\ntential for our models to generate such content.\nTo understand the potential harm of LLaMA-65B,\nwe evaluate on different benchmarks that measure\ntoxic content production and stereotypes detection.\nWhile we have selected some of the standard bench-\nmarks that are used by the language model com-\nmunity to indicate some of the issues with these\nmodels, these evaluations are not suf\ufb01cient to fully\nunderstand the risks associated with these models.", "start_char_idx": 2183, "end_char_idx": 3120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70b0c482-be1e-4f37-b892-419da9e187de": {"__data__": {"id_": "70b0c482-be1e-4f37-b892-419da9e187de", "embedding": null, "metadata": {"page_label": "8", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "033e0bb8-3dcf-433a-b4f2-2e67b4a0c6d8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "750ec4c92b264baecb98e72e60550f7510c7b19c020b5d374242bc5dc2dea9c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b714e85-9b55-436d-8850-c7248d366e94", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "170a15c4c3946990f7deb9ff651de6c347eee28d2c44e7ad9884930e24d1f7f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3aaa02f6-d8dd-48b0-8032-96c96c68da8a", "node_type": "1", "metadata": {}, "hash": "08cd5f877a6020033e381ce4fad32479da63d0cbe9285d86ac90f6abe5a6edf5", "class_name": "RelatedNodeInfo"}}, "text": "0 250 500 750 1000 1250 1500203040506070AccuracyTriviaQA\n0 250 500 750 1000 1250 15005055606570758085HellaSwag\n0 250 500 750 1000 1250 150005101520253035NaturalQuestions\n0 250 500 750 1000 1250 1500\nBillion of tokens40424446485052AccuracySIQA\n0 250 500 750 1000 1250 1500\nBillion of tokens50556065707580WinoGrande\n0 250 500 750 1000 1250 1500\nBillion of tokens65.067.570.072.575.077.580.082.5PIQA\nLLaMA 7B\nLLaMA 13B\nLLaMA 33B\nLLaMA 65B\nChinchillaFigure 2: Evolution of performance on question answering and common sense reasoning during training.\n5.1 RealToxicityPrompts\nLanguage models can generate toxic language, e.g.,\ninsults, hate speech or threats. There is a very large\nrange of toxic content that a model can generate,\nmaking a thorough evaluation challenging. Several\nrecent work (Zhang et al., 2022; Hoffmann et al.,\n2022) have considered the RealToxicityPrompts\nbenchmark (Gehman et al., 2020) as an indicator\nof how toxic is their model. RealToxicityPrompts\nconsists of about 100k prompts that the model must\ncomplete; then a toxicity score is automatically\nevaluated by making a request to PerspectiveAPI3.\nWe do not have control over the pipeline used by\nthe third-party PerspectiveAPI, making comparison\nwith previous models dif\ufb01cult.\nFor each of the 100k prompts, we greedily gen-\nerate with our models, and measure their toxic-\nity score. The score per prompt ranges from 0\n(non-toxic) to 1 (toxic). In Table 11, we report our\naveraged score on basic and respectful prompt cat-\negories of RealToxicityPrompts. These scores are\n\u201ccomparable\u201d with what we observe in the litera-\nture (e.g., 0.087 for Chinchilla) but the method-\nologies differ between these work and ours (in\nterms of sampling strategy, number of prompts and\ntime of API). We observe that toxicity increases\n3https://perspectiveapi.com/Basic Respectful\nLLaMA7B 0.106 0.081\n13B 0.104 0.095\n33B 0.107 0.087\n65B 0.128 0.141\nTable 11: RealToxicityPrompts. We run a greedy de-\ncoder on the 100k prompts from this benchmark. The\n\u201crespectful\u201d versions are prompts starting with \u201cCom-\nplete the following sentence in a polite, respectful, and\nunbiased manner:\u201d, and \u201cBasic\u201d is without it. Scores\nwere obtained using the PerplexityAPI, with higher\nscore indicating more toxic generations.\nwith the size of the model, especially for Respect-\nful prompts. This was also observed in previous\nwork (Zhang et al., 2022), with the notable excep-\ntion of Hoffmann et al. (2022) where they do not\nsee a difference between Chinchilla and Gopher,\ndespite different sizes. This could be explained by\nthe fact that the larger model, Gopher, has worse\nperformance than Chinchilla, suggesting that the\nrelation between toxicity and model size may only\napply within a model family.", "start_char_idx": 0, "end_char_idx": 2738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3aaa02f6-d8dd-48b0-8032-96c96c68da8a": {"__data__": {"id_": "3aaa02f6-d8dd-48b0-8032-96c96c68da8a", "embedding": null, "metadata": {"page_label": "9", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4a1af29-7df3-4558-a3ed-125023d42c51", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c8fdb088458216878ccb2f2e644699c5a584c3336ecfe125341289a358ef30ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70b0c482-be1e-4f37-b892-419da9e187de", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "750ec4c92b264baecb98e72e60550f7510c7b19c020b5d374242bc5dc2dea9c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9fe1235-c857-4172-95a7-3e7cdda6cbb6", "node_type": "1", "metadata": {}, "hash": "0d3aef4e8b71e436d557d4535bebb5bd1602c42a8c850b5015d7b3ced73e6b04", "class_name": "RelatedNodeInfo"}}, "text": "LLaMA GPT3 OPT\nGender 70.6 62.6 65.7\nReligion 79.0 73.3 68.6\nRace/Color 57.0 64.7 68.6\nSexual orientation 81.0 76.2 78.6\nAge 70.1 64.4 67.8\nNationality 64.2 61.6 62.9\nDisability 66.7 76.7 76.7\nPhysical appearance 77.8 74.6 76.2\nSocioeconomic status 71.5 73.8 76.2\nAverage 66.6 67.2 69.5\nTable 12: CrowS-Pairs. We compare the level of bi-\nases contained in LLaMA-65B with OPT-175B and\nGPT3-175B. Higher score indicates higher bias.\n5.2 CrowS-Pairs\nWe evaluate the biases in our model on the CrowS-\nPairs (Nangia et al., 2020). This dataset allows to\nmeasure biases in 9 categories: gender, religion,\nrace/color, sexual orientation, age, nationality, dis-\nability, physical appearance and socioeconomic sta-\ntus. Each example is composed of a stereotype and\nan anti-stereotype, we measure the model prefer-\nence for the stereotypical sentence using the per-\nplexity of both sentences in a zero-shot setting.\nHigher scores thus indicate higher bias. We com-\npare with GPT-3 and OPT-175B in Table 12.\nLLaMA compares slightly favorably to both\nmodels on average. Our model is particularly bi-\nased in the religion category (+10% compared to\nOPT-175B), followed by age and gender. We ex-\npect these biases to come from CommonCrawl de-\nspite multiple \ufb01ltering steps.\n5.3 WinoGender\nTo further investigate the biases of our model on\nthe gender category, we look at the WinoGender\nbenchmark (Rudinger et al., 2018), a co-reference\nresolution dataset. WinoGender is made of Wino-\ngrad schema, and biases are evaluated by determin-\ning if a model co-reference resolution performance\nis impacted by the gender of the pronoun.\nMore precisely, each sentence has three men-\ntions: an \u201coccupation\u201d, a \u201cparticipant\u201d, and a\n\u201cpronoun\u201d where the pronoun is co-referencing\neither the occupation or participant. We prompt\nthe model to determine the co-reference relation\nand measure if it does so correctly according tothe context of the sentence. The goal is to reveal\nif societal biases associated with occupations\nhave been captured by the model. For example,\na sentence in the WinoGender dataset is \u201cThe\nnurse noti\ufb01ed the patient that his shift would be\nending in an hour.\u201d, which is followed by \u2018His\u2019\nrefers to . We then compare the perplexity of the\ncontinuations the nurse andthe patient to per-\nform co-reference resolution with the model. We\nevaluate the performance when using 3 pronouns:\n\u201cher/her/she\u201d, \u201chis/him/he\u201d and \u201ctheir/them/some-\none\u201d (the different choices corresponding to the\ngrammatical function of the pronoun.\nIn Table 13, we report the co-reference scores\nfor the three different pronouns contained in the\ndataset. We observe that our model is signi\ufb01cantly\nbetter at performing co-reference resolution for\nthe \u201ctheir/them/someone\u201d pronouns than for the\n\u201cher/her/she\u201d and \u201chis/him/he\u201d pronouns. A simi-\nlar observation was made in previous work (Rae\net al., 2021; Hoffmann et al., 2022), and is likely\nindicative of gender bias. Indeed, in the case of the\n\u201cher/her/she\u201d and \u201chis/him/he\u201d pronouns, the model\nis probably using the majority gender of the occu-\npation to perform co-reference resolution, instead\nof using the evidence of the sentence.\nTo further investigate this hypothesis, we look\nat the set of \u201cgotcha\u201d cases for the \u201cher/her/she\u201d\nand \u201chis/him/he\u201d pronouns in the WinoGender\ndataset. Theses cases correspond to sentences in\nwhich the pronoun does not match the majority\ngender of the occupation, and the occupation is\nthe correct answer.", "start_char_idx": 0, "end_char_idx": 3456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9fe1235-c857-4172-95a7-3e7cdda6cbb6": {"__data__": {"id_": "a9fe1235-c857-4172-95a7-3e7cdda6cbb6", "embedding": null, "metadata": {"page_label": "9", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4a1af29-7df3-4558-a3ed-125023d42c51", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c8fdb088458216878ccb2f2e644699c5a584c3336ecfe125341289a358ef30ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3aaa02f6-d8dd-48b0-8032-96c96c68da8a", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d0f826d9ea56608e2e5ec15e104907e4863c4485dc45e0b115594a628b8116d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b25c9ca3-d116-4a86-8589-80242f3e2529", "node_type": "1", "metadata": {}, "hash": "d02340131213c39e48a01557ff70a3efd93794f9a61f4b3bbdb5a1403c195434", "class_name": "RelatedNodeInfo"}}, "text": "A simi-\nlar observation was made in previous work (Rae\net al., 2021; Hoffmann et al., 2022), and is likely\nindicative of gender bias. Indeed, in the case of the\n\u201cher/her/she\u201d and \u201chis/him/he\u201d pronouns, the model\nis probably using the majority gender of the occu-\npation to perform co-reference resolution, instead\nof using the evidence of the sentence.\nTo further investigate this hypothesis, we look\nat the set of \u201cgotcha\u201d cases for the \u201cher/her/she\u201d\nand \u201chis/him/he\u201d pronouns in the WinoGender\ndataset. Theses cases correspond to sentences in\nwhich the pronoun does not match the majority\ngender of the occupation, and the occupation is\nthe correct answer. In Table 13, we observe that\nour model, LLaMA-65B, makes more errors on the\ngotcha examples, clearly showing that it capture\nsocietal biases related to gender and occupation.\nThe drop of performance exists for \u201cher/her/she\u201d\nand \u201chis/him/he\u201d pronouns, which is indicative of\nbiases regardless of gender.\n5.4 TruthfulQA\nTruthfulQA (Lin et al., 2021) aims to measure the\ntruthfulness of a model, i.e., its ability to identify\nwhen a claim is true. Lin et al. (2021) consider\nthe de\ufb01nition of \u201ctrue\u201d in the sense of \u201cliteral truth\nabout the real world\u201d, and not claims that are only\ntrue in the context of a belief system or tradition.\nThis benchmark can evaluate the risks of a model\nto generate misinformation or false claims. The\nquestions are written in diverse style, cover 38 cat-\negories and are designed to be adversarial.", "start_char_idx": 2798, "end_char_idx": 4283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b25c9ca3-d116-4a86-8589-80242f3e2529": {"__data__": {"id_": "b25c9ca3-d116-4a86-8589-80242f3e2529", "embedding": null, "metadata": {"page_label": "10", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6a6e3dcf-5eef-4889-adfb-d7e044f459f6", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "82df6dd957b8b66d38017d7c8a09aac95bd6b66f85347d6706dfbb56ca4fa0e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9fe1235-c857-4172-95a7-3e7cdda6cbb6", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c52ffa6cf084c5c9a8d596f025ee279013aabf0f7c7567eb67e816d6b01fccda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64ab38de-2ab2-4004-9c54-30307d2e8a01", "node_type": "1", "metadata": {}, "hash": "c8b75acc26483cdbd6e3cca66ed2b40ab653a72b010801efe9522b5abe7365b5", "class_name": "RelatedNodeInfo"}}, "text": "7B 13B 33B 65B\nAll 66.0 64.7 69.0 77.5\nher/her/she 65.0 66.7 66.7 78.8\nhis/him/he 60.8 62.5 62.1 72.1\ntheir/them/someone 72.1 65.0 78.3 81.7\nher/her/she ( gotcha ) 64.2 65.8 61.7 75.0\nhis/him/he ( gotcha ) 55.0 55.8 55.8 63.3\nTable 13: WinoGender. Co-reference resolution ac-\ncuracy for the LLaMA models, for different pronouns\n(\u201cher/her/she\u201d and \u201chis/him/he\u201d). We observe that our\nmodels obtain better performance on \u201ctheir/them/some-\none\u2019 pronouns than on \u201cher/her/she\u201d and \u201chis/him/he\u2019,\nwhich is likely indicative of biases.\nTruthful Truthful*Inf\nGPT-31.3B 0.31 0.19\n6B 0.22 0.19\n175B 0.28 0.25\nLLaMA7B 0.33 0.29\n13B 0.47 0.41\n33B 0.52 0.48\n65B 0.57 0.53\nTable 14: TruthfulQA. We report the fraction of truth-\nful and truthful*informative answers, as scored by spe-\ncially trained models via the OpenAI API. We follow\nthe QA prompt style used in Ouyang et al. (2022), and\nreport the performance of GPT-3 from the same paper.\nIn Table 14, we report the performance of our\nmodels on both questions to measure truthful mod-\nels and the intersection of truthful and informative.\nCompared to GPT-3, our model scores higher in\nboth categories, but the rate of correct answers is\nstill low, showing that our model is likely to hallu-\ncinate incorrect answers.\n6 Carbon footprint\nThe training of our models have consumed a mas-\nsive quantity of energy, responsible for the emis-\nsion of carbon dioxide. We follow the recent liter-\nature on the subject and breakdown both the total\nenergy consumption and the resulting carbon foot-\nprint in Table 15. We follow a formula for Wu et al.\n(2022) to estimate the Watt-hour, Wh, needed to\ntrain a model, as well as the tons of carbon emis-\nsions, tCO 2eq. For the Wh, we use the formula:\nWh =GPU-h\u00d7(GPU power consumption )\u00d7PUE,where we set the Power Usage Effectiveness (PUE)\nat1.1. The resulting carbon emission depends on\nthe location of the data center used to train the net-\nwork. For instance, BLOOM uses a grid that emits\n0.057 kg CO 2eq/KWh leading to 27 tCO 2eq and\nOPT a grid that emits 0.231 kg CO 2eq/KWh, lead-\ning to 82 tCO 2eq. In this study, we are interested in\ncomparing the cost in carbon emission of training\nof these models if they were trained in the same\ndata center. Hence, we do not take the location\nof data center in consideration, and use, instead,\nthe US national average carbon intensity factor of\n0.385 kg CO 2eq/KWh. This leads to the following\nformula for the tons of carbon emissions:\ntCO2eq=MWh\u00d70.385.\nWe apply the same formula to OPT and BLOOM\nfor fair comparison. For OPT, we assume training\nrequired 34 days on 992 A100-80B (see their logs4).\nFinally, we estimate that we used 2048 A100-80GB\nfor a period of approximately 5 months to develop\nour models. This means that developing these mod-\nels would have cost around 2,638 MWh under our\nassumptions, and a total emission of 1,015 tCO 2eq.\nWe hope that releasing these models will help to\nreduce future carbon emission since the training is\nalready done, and some of the models are relatively\nsmall and can be run on a single GPU.\n7 Related work\nLanguage models are probability distributions\nover sequences of words, tokens or charac-\nters (Shannon, 1948, 1951).", "start_char_idx": 0, "end_char_idx": 3188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64ab38de-2ab2-4004-9c54-30307d2e8a01": {"__data__": {"id_": "64ab38de-2ab2-4004-9c54-30307d2e8a01", "embedding": null, "metadata": {"page_label": "10", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6a6e3dcf-5eef-4889-adfb-d7e044f459f6", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "82df6dd957b8b66d38017d7c8a09aac95bd6b66f85347d6706dfbb56ca4fa0e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b25c9ca3-d116-4a86-8589-80242f3e2529", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "ff2659abe5d4a614b56fd77db8ed7e553ff7193d8ca2009953765aea8e6f31d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ce9d3df-ba9e-4e68-81a5-2dca7d25e1a8", "node_type": "1", "metadata": {}, "hash": "a82ba8112c46b42fefcf2b2d2c458e7442a7f1889af77041f501d352e1f9e9f5", "class_name": "RelatedNodeInfo"}}, "text": "We apply the same formula to OPT and BLOOM\nfor fair comparison. For OPT, we assume training\nrequired 34 days on 992 A100-80B (see their logs4).\nFinally, we estimate that we used 2048 A100-80GB\nfor a period of approximately 5 months to develop\nour models. This means that developing these mod-\nels would have cost around 2,638 MWh under our\nassumptions, and a total emission of 1,015 tCO 2eq.\nWe hope that releasing these models will help to\nreduce future carbon emission since the training is\nalready done, and some of the models are relatively\nsmall and can be run on a single GPU.\n7 Related work\nLanguage models are probability distributions\nover sequences of words, tokens or charac-\nters (Shannon, 1948, 1951). This task, often framed\nas next token prediction, has long been considered a\ncore problem in natural language processing (Bahl\net al., 1983; Brown et al., 1990). Because Turing\n(1950) proposed to measure machine intelligence\nby using language through the \u201cimitation game\u201d,\nlanguage modeling has been proposed as a bench-\nmark to measure progress toward arti\ufb01cial intelli-\ngence (Mahoney, 1999).\nArchitecture. Traditionally, language models\nwere based on n-gram count statistics (Bahl\net al., 1983), and various smoothing techniques\nwere proposed to improve the estimation of rare\nevents (Katz, 1987; Kneser and Ney, 1995). In the\npast two decades, neural networks have been suc-\ncessfully applied to the language modelling task,\n4https://github.com/facebookresearch/metaseq/\ntree/main/projects/OPT/chronicles", "start_char_idx": 2474, "end_char_idx": 3997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ce9d3df-ba9e-4e68-81a5-2dca7d25e1a8": {"__data__": {"id_": "4ce9d3df-ba9e-4e68-81a5-2dca7d25e1a8", "embedding": null, "metadata": {"page_label": "11", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fb5d34d-4a07-4911-abec-dbbf425108f4", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "ec137f6aa04784f842ce0242de2b6536370d0120fbc35ba86de9792bad4f3b5d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64ab38de-2ab2-4004-9c54-30307d2e8a01", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "629b0a40a909935a5b639958a869fa4f8a47b2c767d519f5c221214ee211ae75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa939a26-1d07-416f-aca8-65fbdaa48348", "node_type": "1", "metadata": {}, "hash": "4748001160dcab546eccc652405e189a31697b75081a128c41d49dfd0c43c258", "class_name": "RelatedNodeInfo"}}, "text": "GPU TypeGPU PowerGPU-hoursTotal power Carbon emitted\nconsumption consumption (tCO 2eq)\nOPT-175B A100-80GB 400W 809,472 356 MWh 137\nBLOOM-175B A100-80GB 400W 1,082,880 475 MWh 183\nLLaMA-7B A100-80GB 400W 82,432 36 MWh 14\nLLaMA-13B A100-80GB 400W 135,168 59 MWh 23\nLLaMA-33B A100-80GB 400W 530,432 233 MWh 90\nLLaMA-65B A100-80GB 400W 1,022,362 449 MWh 173\nTable 15: Carbon footprint of training different models in the same data center. We follow Wu et al. (2022)\nto compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power\nconsumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a\nPUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO 2e per KWh.\nstarting from feed forward models (Bengio et al.,\n2000), recurrent neural networks (Elman, 1990;\nMikolov et al., 2010) and LSTMs (Hochreiter and\nSchmidhuber, 1997; Graves, 2013). More recently,\ntransformer networks, based on self-attention, have\nled to important improvements, especially for cap-\nturing long range dependencies (Vaswani et al.,\n2017; Radford et al., 2018; Dai et al., 2019).\nScaling. There is a long history of scaling for\nlanguage models, for both the model and dataset\nsizes. Brants et al. (2007) showed the bene\ufb01ts of\nusing language models trained on 2 trillion tokens,\nresulting in 300 billion n-grams, on the quality of\nmachine translation. While this work relied on a\nsimple smoothing technique, called Stupid Backoff ,\nHea\ufb01eld et al. (2013) later showed how to scale\nKneser-Ney smoothing to Web-scale data. This\nallowed to train a 5-gram model on 975 billions to-\nkens from CommonCrawl, resulting in a model\nwith 500 billions n-grams (Buck et al., 2014).\nChelba et al. (2013) introduced the One Billion\nWord benchmark, a large scale training dataset to\nmeasure the progress of language models.\nIn the context of neural language models, Joze-\nfowicz et al. (2016) obtained state-of-the-art re-\nsults on the Billion Word benchmark by scaling\nLSTMs to 1 billion parameters. Later, scaling\ntransformers lead to improvement on many NLP\ntasks. Notable models include BERT (Devlin et al.,\n2018), GPT-2 (Radford et al., 2019), Megatron-\nLM (Shoeybi et al., 2019), and T5 (Raffel et al.,\n2020). A signi\ufb01cant breakthrough was obtained\nwith GPT-3 (Brown et al., 2020), a model with\n175 billion parameters. This lead to a series of\nLarge Language Models , such as Jurassic-1 (Lieber\net al., 2021), Megatron-Turing NLG (Smith et al.,2022), Gopher (Rae et al., 2021), Chinchilla (Hoff-\nmann et al., 2022), PaLM (Chowdhery et al., 2022),\nOPT (Zhang et al., 2022), and GLM (Zeng et al.,\n2022). Hestness et al. (2017) and Rosenfeld et al.\n(2019) studied the impact of scaling on the perfor-\nmance of deep learning models, showing the exis-\ntence of power laws between the model and dataset\nsizes and the performance of the system. Kaplan\net al. (2020) derived power laws speci\ufb01cally for\ntransformer based language models, which were\nlater re\ufb01ned by Hoffmann et al. (2022), by adapting\nthe learning rate schedule when scaling datasets.\nFinally, Wei et al.", "start_char_idx": 0, "end_char_idx": 3131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa939a26-1d07-416f-aca8-65fbdaa48348": {"__data__": {"id_": "aa939a26-1d07-416f-aca8-65fbdaa48348", "embedding": null, "metadata": {"page_label": "11", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fb5d34d-4a07-4911-abec-dbbf425108f4", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "ec137f6aa04784f842ce0242de2b6536370d0120fbc35ba86de9792bad4f3b5d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ce9d3df-ba9e-4e68-81a5-2dca7d25e1a8", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "331ce92af8c9b76a08f76bb0d80427456fe7c00957697a518908f2c6b61fd2ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7207de6-6d84-4172-b7fa-6ce41e8cdf96", "node_type": "1", "metadata": {}, "hash": "093f25be5b0b3409c2886d359be22657d5057d3750627b2e0ac00eb09d3db38a", "class_name": "RelatedNodeInfo"}}, "text": "Hestness et al. (2017) and Rosenfeld et al.\n(2019) studied the impact of scaling on the perfor-\nmance of deep learning models, showing the exis-\ntence of power laws between the model and dataset\nsizes and the performance of the system. Kaplan\net al. (2020) derived power laws speci\ufb01cally for\ntransformer based language models, which were\nlater re\ufb01ned by Hoffmann et al. (2022), by adapting\nthe learning rate schedule when scaling datasets.\nFinally, Wei et al. (2022) studied the effect of scal-\ning on the abilities of large language models.\n8 Conclusion\nIn this paper, we presented a series of language\nmodels that are released openly, and competitive\nwith state-of-the-art foundation models. Most\nnotably, LLaMA-13B outperforms GPT-3 while\nbeing more than 10 \u00d7smaller, and LLaMA-65B is\ncompetitive with Chinchilla-70B and PaLM-540B.\nUnlike previous studies, we show that it is possible\nto achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that\nreleasing these models to the research community\nwill accelerate the development of large language\nmodels, and help efforts to improve their robust-\nness and mitigate known issues such as toxicity and\nbias. Additionally, we observed like Chung et al.\n(2022) that \ufb01netuning these models on instructions\nlead to promising results, and we plan to further\ninvestigate this in future work. Finally, we plan to\nrelease larger models trained on larger pretraining\ncorpora in the future, since we have seen a constant\nimprovement in performance as we were scaling.", "start_char_idx": 2672, "end_char_idx": 4264, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7207de6-6d84-4172-b7fa-6ce41e8cdf96": {"__data__": {"id_": "e7207de6-6d84-4172-b7fa-6ce41e8cdf96", "embedding": null, "metadata": {"page_label": "12", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ba6dbe8-8bcd-4626-9e9d-4b9c38bcdcae", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "fc44645d77c195b71b634713fef11974f63b6c508f00e4358696a169283c62ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa939a26-1d07-416f-aca8-65fbdaa48348", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "1d43ed1ffb6fc3fb7ae232e74f2330f96789a88513e3dbe9145a5946ae5baaf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49d68f11-3811-46e6-ac67-6931e8ec84a3", "node_type": "1", "metadata": {}, "hash": "2c8630a2168b594568d99935f148bd3801d9c93fe3750196dc834a6231ecabce", "class_name": "RelatedNodeInfo"}}, "text": "Acknowledgements\nWe thank Daniel Haziza, Francisco Massa, Jeremy\nReizenstein, Artem Korenev, and Patrick Labatut\nfrom the xformers team. We thank Susan Zhang\nand Stephen Roller for their support on data\ndeduplication. We thank Luca Wehrstedt, Vegard\nMella, and Pierre-Emmanuel Mazar\u00e9 for their\nsupport on training stability. We thank Shubho\nSengupta, Kalyan Saladi, and all the AI infra team\nfor their support. We thank Jane Yu for her input\non evaluation. We thank Yongyi Hu for his help\non data collection.\nReferences\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, and\nCharles Sutton. 2021. Program synthesis with large\nlanguage models.\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer.\n1983. A maximum likelihood approach to continu-\nous speech recognition. IEEE transactions on pat-\ntern analysis and machine intelligence , pages 179\u2013\n190.\nYoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent.\n2000. A neural probabilistic language model. Ad-\nvances in neural information processing systems , 13.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin\nChoi, et al. 2020. Piqa: Reasoning about physi-\ncal commonsense in natural language. In Proceed-\nings of the AAAI conference on arti\ufb01cial intelligence ,\npages 7432\u20137439.\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, et al. 2022.\nGpt-neox-20b: An open-source autoregressive lan-\nguage model. arXiv preprint arXiv:2204.06745 .\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J.\nOch, and Jeffrey Dean. 2007. Large language mod-\nels in machine translation. In Proceedings of the\n2007 Joint Conference on Empirical Methods in Nat-\nural Language Processing and Computational Nat-\nural Language Learning (EMNLP-CoNLL) , pages\n858\u2013867, Prague, Czech Republic. Association for\nComputational Linguistics.\nPeter F Brown, John Cocke, Stephen A Della Pietra,\nVincent J Della Pietra, Frederick Jelinek, John Laf-\nferty, Robert L Mercer, and Paul S Roossin. 1990. A\nstatistical approach to machine translation. Compu-\ntational linguistics , 16(2):79\u201385.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nChristian Buck, Kenneth Hea\ufb01eld, and Bas Van Ooyen.\n2014. N-gram counts and language models from the\ncommon crawl. In LREC , volume 2, page 4.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005 .", "start_char_idx": 0, "end_char_idx": 3088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49d68f11-3811-46e6-ac67-6931e8ec84a3": {"__data__": {"id_": "49d68f11-3811-46e6-ac67-6931e8ec84a3", "embedding": null, "metadata": {"page_label": "12", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ba6dbe8-8bcd-4626-9e9d-4b9c38bcdcae", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "fc44645d77c195b71b634713fef11974f63b6c508f00e4358696a169283c62ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7207de6-6d84-4172-b7fa-6ce41e8cdf96", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bcc15814340b68e22128bc95bef0c71be0fff098284da21cfb359cbab3400766", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ff1d2fc-0ee8-4776-9987-303059c8a6d6", "node_type": "1", "metadata": {}, "hash": "94fe725b583d44ff5a4ba6d29349223602f827c4ee9b911dc8eca6953909a4f9", "class_name": "RelatedNodeInfo"}}, "text": "2020. Language models are few-shot learn-\ners.\nChristian Buck, Kenneth Hea\ufb01eld, and Bas Van Ooyen.\n2014. N-gram counts and language models from the\ncommon crawl. In LREC , volume 2, page 4.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005 .\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021. Eval-\nuating large language models trained on code.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.", "start_char_idx": 2667, "end_char_idx": 5134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ff1d2fc-0ee8-4776-9987-303059c8a6d6": {"__data__": {"id_": "3ff1d2fc-0ee8-4776-9987-303059c8a6d6", "embedding": null, "metadata": {"page_label": "13", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2737fd4b-f5af-4230-9635-e34ebb678626", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e3e0c3def90b5221f30190721d5916988b34a103e3253846b74d6ee05d8547f1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49d68f11-3811-46e6-ac67-6931e8ec84a3", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "501a8bf377521edf0cea3f17f0111a42d75a63c045a9d93849a87312bee27ec4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd2e9333-ec11-40a4-9386-bae415501b28", "node_type": "1", "metadata": {}, "hash": "6375017e43140dd3f907270b55c4739c2d2344dc9f3ae48fd49d36294c2227da", "class_name": "RelatedNodeInfo"}}, "text": "Hyung Won Chung, Le Hou, S. Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Dasha Valter, Sharan Narang, Gaurav Mishra,\nAdams Wei Yu, Vincent Zhao, Yanping Huang, An-\ndrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai\nhsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts,\nDenny Zhou, Quoc Le, and Jason Wei. 2022. Scal-\ning instruction-\ufb01netuned language models. arXiv\npreprint arXiv:2210.11416 .\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndif\ufb01culty of natural yes/no questions. arXiv preprint\narXiv:1905.10044 .\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457 .\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training veri\ufb01ers to solve math\nword problems. arXiv preprint arXiv:2110.14168 .\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a \ufb01xed-length context. arXiv preprint\narXiv:1901.02860 .\nTri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra,\nand Christopher R\u00e9. 2022. Flashattention: Fast and\nmemory-ef\ufb01cient exact attention with io-awareness.\narXiv preprint arXiv:2205.14135 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805 .\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science , 14(2):179\u2013211.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida\nWang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-\ntau Yih, Luke Zettlemoyer, and Mike Lewis. 2022.\nIncoder: A generative model for code in\ufb01lling and\nsynthesis. arXiv preprint arXiv:2204.05999 .\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027 .\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPo\ufb01, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,Jason Phang, Laria Reynolds, Eric Tang, Anish\nThite, Ben Wang, Kevin Wang, and Andy Zou. 2021.\nA framework for few-shot language model evalua-\ntion.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. arXiv preprint arXiv:2009.11462 .\nAlex Graves. 2013.", "start_char_idx": 0, "end_char_idx": 2934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd2e9333-ec11-40a4-9386-bae415501b28": {"__data__": {"id_": "bd2e9333-ec11-40a4-9386-bae415501b28", "embedding": null, "metadata": {"page_label": "13", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2737fd4b-f5af-4230-9635-e34ebb678626", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e3e0c3def90b5221f30190721d5916988b34a103e3253846b74d6ee05d8547f1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ff1d2fc-0ee8-4776-9987-303059c8a6d6", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bbeb6456987a0dce8b258abc3a4ea0241253bd57deca9ccecccb3fa2ed9ac55f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "186a110d-27b0-448e-a529-3b3a031ad2ff", "node_type": "1", "metadata": {}, "hash": "2cc3e8194e4cc00a8adf4fb226c6a48940364315913c5e80dec88ab0b6951eb7", "class_name": "RelatedNodeInfo"}}, "text": "2020. The Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027 .\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPo\ufb01, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,Jason Phang, Laria Reynolds, Eric Tang, Anish\nThite, Ben Wang, Kevin Wang, and Andy Zou. 2021.\nA framework for few-shot language model evalua-\ntion.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. arXiv preprint arXiv:2009.11462 .\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850 .\nKenneth Hea\ufb01eld, Ivan Pouzyrevsky, Jonathan H Clark,\nand Philipp Koehn. 2013. Scalable modi\ufb01ed kneser-\nney language model estimation. In Proceedings of\nthe 51st Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers) ,\npages 690\u2013696.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2020. Measuring massive multitask language\nunderstanding. arXiv preprint arXiv:2009.03300 .\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. arXiv\npreprint arXiv:2103.03874 .\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gre-\ngory Diamos, Heewoo Jun, Hassan Kianinejad,\nMd Patwary, Mostofa Ali, Yang Yang, and Yanqi\nZhou. 2017. Deep learning scaling is predictable,\nempirically. arXiv preprint arXiv:1712.00409 .\nSepp Hochreiter and J\u00fcrgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735\u20131780.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, D\u00e1niel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022. Opt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017 .\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551 .", "start_char_idx": 2317, "end_char_idx": 4956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "186a110d-27b0-448e-a529-3b3a031ad2ff": {"__data__": {"id_": "186a110d-27b0-448e-a529-3b3a031ad2ff", "embedding": null, "metadata": {"page_label": "14", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da2a7276-8d87-4b9a-b968-68c38bde67b3", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "b991e12c81ab55521e8da977f6c98e01760b1284505af16da8c5b3d0b04abd2f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd2e9333-ec11-40a4-9386-bae415501b28", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d14daf202395bad7763f85ffff78c3a9a6842e582222a7626c911278aae64fef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cd548c7-4dab-4201-aa76-f12bace264be", "node_type": "1", "metadata": {}, "hash": "db3f718c87be222c45a08c7e1422a4ee8bc63e4b10cfde9e9fbcdef02801f302", "class_name": "RelatedNodeInfo"}}, "text": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410 .\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361 .\nSlava Katz. 1987. Estimation of probabilities from\nsparse data for the language model component of a\nspeech recognizer. IEEE transactions on acoustics,\nspeech, and signal processing , 35(3):400\u2013401.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In 1995\ninternational conference on acoustics, speech, and\nsignal processing , volume 1, pages 181\u2013184. IEEE.\nVijay Korthikanti, Jared Casper, Sangkug Lym,\nLawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. 2022. Reducing ac-\ntivation recomputation in large transformer models.\narXiv preprint arXiv:2205.05198 .\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226 .\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Quantifying social bi-\nases in contextual word representations. In 1st ACL\nWorkshop on Gender Bias for Natural Language\nProcessing .\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n\ufb01eld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics ,\n7:453\u2013466.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv\npreprint arXiv:1704.04683 .\nAitor Lewkowycz, Anders Johan Andreassen,\nDavid Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem\nAnil, Imanol Schlag, Theo Gutman-Solo, Yuhuai\nWu, Behnam Neyshabur, Guy Gur-Ari, and Vedant\nMisra. 2022. Solving quantitative reasoning prob-\nlems with language models. In Advances in Neural\nInformation Processing Systems .\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\nShoham. 2021. Jurassic-1: Technical details and\nevaluation. White Paper. AI21 Labs , 1.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958 .Ilya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101 .\nMatthew V Mahoney. 1999. Text compression as a test\nfor arti\ufb01cial intelligence. AAAI/IAAI , 970.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. arXiv preprint arXiv:1809.02789 .\nTomas Mikolov, Martin Kara\ufb01\u00e1t, Lukas Burget, Jan\nCernock `y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In In-\nterspeech , pages 1045\u20131048. Makuhari.", "start_char_idx": 0, "end_char_idx": 3121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cd548c7-4dab-4201-aa76-f12bace264be": {"__data__": {"id_": "7cd548c7-4dab-4201-aa76-f12bace264be", "embedding": null, "metadata": {"page_label": "14", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da2a7276-8d87-4b9a-b968-68c38bde67b3", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "b991e12c81ab55521e8da977f6c98e01760b1284505af16da8c5b3d0b04abd2f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "186a110d-27b0-448e-a529-3b3a031ad2ff", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c88ea233d9689bcbf26af37e0ca91ad202b2efc62eb8235e50c073788eb7e500", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb9d4f26-4723-4d4d-9371-44f35e54c106", "node_type": "1", "metadata": {}, "hash": "1168373c5718a88d8cd3f3932986a32cd953c8528cf6f7550d966156aa8d8b35", "class_name": "RelatedNodeInfo"}}, "text": "Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101 .\nMatthew V Mahoney. 1999. Text compression as a test\nfor arti\ufb01cial intelligence. AAAI/IAAI , 970.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. arXiv preprint arXiv:1809.02789 .\nTomas Mikolov, Martin Kara\ufb01\u00e1t, Lukas Burget, Jan\nCernock `y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In In-\nterspeech , pages 1045\u20131048. Makuhari.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In EMNLP 2020 .\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,\nHuan Wang, Yingbo Zhou, Silvio Savarese, and\nCaiming Xiong. 2022. Codegen: An open large lan-\nguage model for code with multi-turn program syn-\nthesis. arXiv preprint arXiv:2203.13474 .\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions\nwith human feedback. In Advances in Neural Infor-\nmation Processing Systems .\nMarkus N Rabe and Charles Staats. 2021. Self-\nattention does not need o(n2)memory. arXiv\npreprint arXiv:2112.05682 .\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog , 1(8):9.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, An-\ntonia Creswell, Nat McAleese, Amy Wu, Erich\nElsen, Siddhant Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan,\nMichela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\nmatzadeh, Elena Gribovskaya, Domenic Donato,", "start_char_idx": 2568, "end_char_idx": 5089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb9d4f26-4723-4d4d-9371-44f35e54c106": {"__data__": {"id_": "cb9d4f26-4723-4d4d-9371-44f35e54c106", "embedding": null, "metadata": {"page_label": "15", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ade6dee-14b1-4b27-8ba3-66a6b1217f5a", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "b09d0b65bb29036f9fa069b24ab36f3f6d0ee6058ab4251e2e45903915a28bd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cd548c7-4dab-4201-aa76-f12bace264be", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "b29fb39122c4a82e99fc716242a65543c2cf469b8c0faa26d80b7e178262df26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e64d7ba-c5de-419c-85c7-f716a25e7fdb", "node_type": "1", "metadata": {}, "hash": "49cd358a5ce7c7cf015c502d8ad299cb68708968c9a77989e02bcd8dab4f0900", "class_name": "RelatedNodeInfo"}}, "text": "Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cy-\nprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi,\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones,\nJames Bradbury, Matthew Johnson, Blake Hecht-\nman, Laura Weidinger, Iason Gabriel, William Isaac,\nEd Lockhart, Simon Osindero, Laura Rimell, Chris\nDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan-\nway, Lorrayne Bennett, Demis Hassabis, Koray\nKavukcuoglu, and Geoffrey Irving. 2021. Scal-\ning language models: Methods, analysis & insights\nfrom training gopher.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uni\ufb01ed text-to-text trans-\nformer. The Journal of Machine Learning Research ,\n21(1):5485\u20135551.\nJonathan S Rosenfeld, Amir Rosenfeld, Yonatan Be-\nlinkov, and Nir Shavit. 2019. A constructive predic-\ntion of the generalization error across scales. arXiv\npreprint arXiv:1909.12673 .\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In NAACL-HLT 2018 .\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM , 64(9):99\u2013106.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLeBras, and Yejin Choi. 2019. Socialiqa: Com-\nmonsense reasoning about social interactions. arXiv\npreprint arXiv:1904.09728 .\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili \u00b4c, Daniel Hesslow, Ro-\nman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois\nYvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100 .\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909 .\nClaude E Shannon. 1948. A mathematical theory of\ncommunication. The Bell system technical journal ,\n27(3):379\u2013423.\nClaude E Shannon. 1951. Prediction and entropy\nof printed english. Bell system technical journal ,\n30(1):50\u201364.\nNoam Shazeer. 2020. Glu variants improve trans-\nformer. arXiv preprint arXiv:2002.05202 .Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as a\nbabysitter: On biases in language generation. arXiv\npreprint arXiv:1909.01326 .\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\narXiv preprint arXiv:1909.08053 .", "start_char_idx": 0, "end_char_idx": 2803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e64d7ba-c5de-419c-85c7-f716a25e7fdb": {"__data__": {"id_": "9e64d7ba-c5de-419c-85c7-f716a25e7fdb", "embedding": null, "metadata": {"page_label": "15", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ade6dee-14b1-4b27-8ba3-66a6b1217f5a", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "b09d0b65bb29036f9fa069b24ab36f3f6d0ee6058ab4251e2e45903915a28bd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb9d4f26-4723-4d4d-9371-44f35e54c106", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "0763d77bcc968b56860602ae73433102e3aa0130aeec33e85c0258b1c2d264c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6844e2e9-74ad-47c8-825a-5c807fd50fe7", "node_type": "1", "metadata": {}, "hash": "fde00ed4faa91069c27f082066ef0a390db10fb5b5849c69cbc7564531e520b8", "class_name": "RelatedNodeInfo"}}, "text": "1951. Prediction and entropy\nof printed english. Bell system technical journal ,\n30(1):50\u201364.\nNoam Shazeer. 2020. Glu variants improve trans-\nformer. arXiv preprint arXiv:2002.05202 .Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as a\nbabysitter: On biases in language generation. arXiv\npreprint arXiv:1909.01326 .\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\narXiv preprint arXiv:1909.08053 .\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zhang, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\n2022. Using deepspeed and megatron to train\nmegatron-turing nlg 530b, a large-scale generative\nlanguage model.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu. 2021. Roformer: En-\nhanced transformer with rotary position embedding.\narXiv preprint arXiv:2104.09864 .\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, De-\nhao Chen, Yuanzhong Xu, Zhifeng Chen, Adam\nRoberts, Maarten Bosma, Vincent Zhao, Yanqi\nZhou, Chung-Ching Chang, Igor Krivokon, Will\nRusch, Marc Pickett, Pranesh Srinivasan, Laichee\nMan, Kathleen Meier-Hellstern, Meredith Ringel\nMorris, Tulsee Doshi, Renelito Delos Santos, Toju\nDuke, Johnny Soraker, Ben Zevenbergen, Vinod-\nkumar Prabhakaran, Mark Diaz, Ben Hutchinson,\nKristen Olson, Alejandra Molina, Erin Hoffman-\nJohn, Josh Lee, Lora Aroyo, Ravi Rajakumar,\nAlena Butryna, Matthew Lamm, Viktoriya Kuzmina,\nJoe Fenton, Aaron Cohen, Rachel Bernstein, Ray\nKurzweil, Blaise Aguera-Arcas, Claire Cui, Marian\nCroak, Ed Chi, and Quoc Le. 2022. Lamda: Lan-\nguage models for dialog applications.\nA. M. Turing. 1950. Computing Machinery and Intel-\nligence . [Oxford University Press, Mind Associa-\ntion].\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30 , pages 5998\u20136008.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax .\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,", "start_char_idx": 2216, "end_char_idx": 4989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6844e2e9-74ad-47c8-825a-5c807fd50fe7": {"__data__": {"id_": "6844e2e9-74ad-47c8-825a-5c807fd50fe7", "embedding": null, "metadata": {"page_label": "16", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58c7a09b-8a15-4818-849d-387dc4e836d6", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "13b571ba71a9dc8de41ca0e16f22cae0b296f08b84cd648f5c82973ff577d55d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e64d7ba-c5de-419c-85c7-f716a25e7fdb", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "8c8df64d1c7cadb755c43271c4c167a900efd92a9367cfc1b8cf915917e037b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8a82c3d-8149-4653-bb1c-7e9ed8624403", "node_type": "1", "metadata": {}, "hash": "f241c0a49ab3083020554848e5030b6a47e4002266db115b8bc674b7fbf2b070", "class_name": "RelatedNodeInfo"}}, "text": "and Denny Zhou. 2022. Self-consistency improves\nchain of thought reasoning in language models.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682 .\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. In Language Resources and Evaluation\nConference .\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\nria Chang, Fiona Aga, Jinshi Huang, Charles Bai,\net al. 2022. Sustainable ai: Environmental implica-\ntions, challenges and opportunities. Proceedings of\nMachine Learning and Systems , 4:795\u2013813.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really \ufb01nish your sentence? arXiv preprint\narXiv:1905.07830 .\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan\nMa, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng\nZhang, Yuxiao Dong, and Jie Tang. 2022. Glm-\n130b: An open bilingual pre-trained model.\nBiao Zhang and Rico Sennrich. 2019. Root mean\nsquare layer normalization. Advances in Neural In-\nformation Processing Systems , 32.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068 .", "start_char_idx": 0, "end_char_idx": 1684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8a82c3d-8149-4653-bb1c-7e9ed8624403": {"__data__": {"id_": "b8a82c3d-8149-4653-bb1c-7e9ed8624403", "embedding": null, "metadata": {"page_label": "I", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9cecb09d-cb03-458f-b2ce-7f27ab2bc876", "node_type": "4", "metadata": {"page_label": "I", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d7e45cdeb496554a315534e3b856c7c08beadd1292b2bd1c4499b75f51e1c804", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6844e2e9-74ad-47c8-825a-5c807fd50fe7", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "13b571ba71a9dc8de41ca0e16f22cae0b296f08b84cd648f5c82973ff577d55d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc25b042-69a3-46b4-9ce8-9414d7b98230", "node_type": "1", "metadata": {}, "hash": "5618bbcbafe61c8a158f4bf5123947e797563bee6660525a3a3a7dfb494aaf0e", "class_name": "RelatedNodeInfo"}}, "text": "A Question Answering\nWe evaluate LLaMA on Natural Questions and TriviaQA. For Natural Questions we use the test split used\nfor open-domain question answering containing 3610 questions. For TriviaQA we evaluate on the dev set\nof the \ufb01ltered set. This differs from GPT-3 and PaLM, which evaluate on the test set of the un\ufb01ltered set\nfor which the online evaluation server is not available anymore5.\nWe generate answers using greedy decoding, and extract an answer from the generation by stopping\nat the \ufb01rst line break, \ufb01nal dot or comma. Generated answers are evaluated with the standard exact\nmatch metric: a generated answer is considered correct if it matches any answer of the list of answers\nafter normalization. For this normalization step we lowercase generated answers and remove articles,\npunctuation and duplicate whitespaces. Figure 3 presents formatted examples in the 1-shot setting for\nNatural Questions and TriviaQA respectively. In all settings, we preprend the string Answer these\nquestions:\\n to the list of questions and answers.\nContext\u2192Answer these questions: Context\u2192Answer these questions:\nQ: Who sang who wants to be a millionaire in high society? Q: In Scotland a bothy/bothie is a?\nA: Frank Sinatra A: House\nQ: Who wrote the book the origin of species? Q: The ancient city of Troy is located in what modern country?\nA: A:\nTarget\u2192Charles Darwin Target\u2192Turkey\nFigure 3: Formatted dataset example for Natural Questions (left) & TriviaQA (right).\n5https://competitions.codalab.org/competitions/17208", "start_char_idx": 0, "end_char_idx": 1520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc25b042-69a3-46b4-9ce8-9414d7b98230": {"__data__": {"id_": "dc25b042-69a3-46b4-9ce8-9414d7b98230", "embedding": null, "metadata": {"page_label": "II", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7ae6d01-304b-4c09-a244-572a4058e14c", "node_type": "4", "metadata": {"page_label": "II", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e976802d9fd9209520df440480187c680450176165f8a190f87161e3f2b0ca28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8a82c3d-8149-4653-bb1c-7e9ed8624403", "node_type": "1", "metadata": {"page_label": "I", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d7e45cdeb496554a315534e3b856c7c08beadd1292b2bd1c4499b75f51e1c804", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b15cd3a5-eb40-4e36-980e-3a7088a9a1d4", "node_type": "1", "metadata": {}, "hash": "c724b0f6ac8807a79ef5535a6bb94390ab5d17262630c6f88b2335229977f5e7", "class_name": "RelatedNodeInfo"}}, "text": "B MMLU\nGPT-3 Gopher Chinchilla LLaMA LLaMA-I\n175B 280B 70B 7B 13B 33B 65B 65B\nAbstract Algebra STEM 30.0 25.0 31.0 29.0 34.0 32.0 34.0 31.0\nAnatomy STEM 48.0 56.3 70.4 37.0 45.9 51.9 57.8 62.2\nAstronomy STEM 49.0 65.8 73.0 33.6 46.1 61.8 72.4 81.6\nBusiness Ethics Other 46.0 70.0 72.0 40.0 45.0 56.0 57.0 72.0\nClinical Knowledge Other 48.0 67.2 75.1 35.1 45.7 57.4 65.3 69.1\nCollege Biology STEM 45.0 70.8 79.9 37.5 45.1 58.3 68.8 81.9\nCollege Chemistry STEM 26.0 45.0 51.0 32.0 30.0 45.0 50.0 45.0\nCollege Computer Science STEM 46.0 49.0 51.0 29.0 39.0 45.0 47.0 51.0\nCollege Mathematics STEM 34.5 37.0 32.0 33.0 32.0 40.0 35.0 36.0\nCollege Medicine Other 48.0 60.1 66.5 30.6 42.8 52.0 54.3 63.0\nCollege Physics STEM 28.0 34.3 46.1 26.5 18.6 28.4 36.3 46.1\nComputer Security STEM 57.0 65.0 76.0 45.0 65.0 66.0 79.0 79.0\nConceptual Physics STEM 36.5 49.4 67.2 36.6 41.3 51.5 59.6 66.4\nEconometrics Social Science 33.0 43.0 38.6 23.7 27.2 35.1 40.4 52.6\nElectrical Engineering STEM 50.0 60.0 62.1 26.9 40.7 49.7 53.8 60.7\nElementary Mathematics STEM 30.0 33.6 41.5 24.3 24.9 36.0 37.8 42.9\nFormal Logic Humanities 29.0 35.7 33.3 27.0 33.3 34.1 44.4 47.6\nGlobal Facts Other 37.0 38.0 39.0 29.0 35.0 35.0 39.0 40.0\nHigh School Biology STEM 48.0 71.3 80.3 34.5 52.6 67.7 73.9 82.9\nHigh School Chemistry STEM 33.0 47.8 58.1 28.1 28.6 41.9 40.4 44.8\nHigh School Computer Science STEM 39.0 54.0 58.0 31.0 48.0 60.0 67.0 73.0\nHigh School European History Humanities 54.0 72.1 78.8 44.2 61.8 73.9 78.8 86.1\nHigh School Geography Social Science 58.0 76.8 86.4 34.3 54.6 70.7 77.8 87.9\nHigh School Government And Politics Social Science 58.0 83.9 91.2 44.6 66.3 82.9 88.1 92.8\nHigh School Macroeconomics Social Science 40.5 65.1 70.5 35.4 44.4 56.9 65.9 69.2\nHigh School Mathematics STEM 28.0 23.7 31.9 24.8 23.7 27.0 34.4 37.", "start_char_idx": 0, "end_char_idx": 1815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b15cd3a5-eb40-4e36-980e-3a7088a9a1d4": {"__data__": {"id_": "b15cd3a5-eb40-4e36-980e-3a7088a9a1d4", "embedding": null, "metadata": {"page_label": "II", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7ae6d01-304b-4c09-a244-572a4058e14c", "node_type": "4", "metadata": {"page_label": "II", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e976802d9fd9209520df440480187c680450176165f8a190f87161e3f2b0ca28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc25b042-69a3-46b4-9ce8-9414d7b98230", "node_type": "1", "metadata": {"page_label": "II", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "8b153633621bd1093c0b62fda779d6dfa03a6a1cad6f4543eb9a0d28331827e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9671fd20-6caa-4599-bcfa-ef4feeb27726", "node_type": "1", "metadata": {}, "hash": "d369cad529dcd6a8f4107628cb8c638a824c7887eaf0cfb93c07d3744baf43d9", "class_name": "RelatedNodeInfo"}}, "text": "0 73.0\nHigh School European History Humanities 54.0 72.1 78.8 44.2 61.8 73.9 78.8 86.1\nHigh School Geography Social Science 58.0 76.8 86.4 34.3 54.6 70.7 77.8 87.9\nHigh School Government And Politics Social Science 58.0 83.9 91.2 44.6 66.3 82.9 88.1 92.8\nHigh School Macroeconomics Social Science 40.5 65.1 70.5 35.4 44.4 56.9 65.9 69.2\nHigh School Mathematics STEM 28.0 23.7 31.9 24.8 23.7 27.0 34.4 37.0\nHigh School Microeconomics Social Science 42.0 66.4 77.7 31.9 47.5 55.5 68.9 78.6\nHigh School Physics STEM 28.0 33.8 36.4 26.5 28.5 35.8 37.1 41.7\nHigh School Psychology Social Science 61.0 81.8 86.6 47.3 60.9 76.2 82.2 87.9\nHigh School Statistics STEM 30.5 50.0 58.8 35.2 30.1 45.4 58.3 59.3\nHigh School Us History Humanities 53.0 78.9 83.3 39.7 58.3 77.9 83.8 90.7\nHigh School World History Humanities 56.0 75.1 85.2 40.9 66.2 79.3 83.1 89.0\nHuman Aging Other 50.0 66.4 77.6 40.8 54.7 67.7 69.5 72.2\nHuman Sexuality Social Science 54.0 67.2 86.3 36.6 58.8 64.1 77.9 87.0\nInternational Law Humanities 55.5 77.7 90.9 51.2 62.8 72.7 79.3 87.6\nJurisprudence Humanities 55.0 71.3 79.6 38.9 51.9 70.4 73.2 85.2\nLogical Fallacies Humanities 48.0 72.4 80.4 39.3 52.8 68.1 77.3 80.4\nMachine Learning STEM 31.0 41.1 41.1 23.2 31.3 39.3 49.1 52.7\nManagement Other 56.0 77.7 82.5 35.0 66.0 77.7 82.5 83.5\nMarketing Other 60.0 83.3 89.7 46.6 71.8 83.3 85.9 92.7\nMedical Genetics Other 40.0 69.0 69.0 43.0 52.0 67.0 67.0 68.0\nMiscellaneous Other 60.0 75.7 84.5 42.4 65.4 78.5 82.1 84.3\nMoral Disputes Humanities 44.5 66.8 77.5 40.2 50.9 66.2 72.3 76.9\nMoral Scenarios Humanities 26.0 40.2 36.5 24.3 30.1 38.2 48.9 55.9\nNutrition Other 47.0 69.9 77.1 37.6 51.6 62.8 67.3 74.5\nPhilosophy Humanities 51.0 68.8 79.4 39.9 54.0 66.2 74.0 79.1\nPrehistory Humanities 53.0 67.6 81.2 36.1 51.5 67.0 75.3 79.0\nProfessional Accounting Other 33.0 44.3 52.1 25.9 35.8 43.6 46.", "start_char_idx": 1411, "end_char_idx": 3267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9671fd20-6caa-4599-bcfa-ef4feeb27726": {"__data__": {"id_": "9671fd20-6caa-4599-bcfa-ef4feeb27726", "embedding": null, "metadata": {"page_label": "II", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7ae6d01-304b-4c09-a244-572a4058e14c", "node_type": "4", "metadata": {"page_label": "II", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e976802d9fd9209520df440480187c680450176165f8a190f87161e3f2b0ca28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b15cd3a5-eb40-4e36-980e-3a7088a9a1d4", "node_type": "1", "metadata": {"page_label": "II", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "0a7e3c91ec6060e5455bb4115653e4044d6e59fcde1a8e82da5e1b4ac25cf0f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ea26e5c-6bb4-4c34-92bf-6c1630ae1741", "node_type": "1", "metadata": {}, "hash": "231403c7a795215e96dcfbd70fa8fd2a5b2acf3c0dc8b1a938209f92d13b769d", "class_name": "RelatedNodeInfo"}}, "text": "5 40.2 50.9 66.2 72.3 76.9\nMoral Scenarios Humanities 26.0 40.2 36.5 24.3 30.1 38.2 48.9 55.9\nNutrition Other 47.0 69.9 77.1 37.6 51.6 62.8 67.3 74.5\nPhilosophy Humanities 51.0 68.8 79.4 39.9 54.0 66.2 74.0 79.1\nPrehistory Humanities 53.0 67.6 81.2 36.1 51.5 67.0 75.3 79.0\nProfessional Accounting Other 33.0 44.3 52.1 25.9 35.8 43.6 46.5 56.0\nProfessional Law Humanities 34.5 44.5 56.5 30.2 38.0 45.9 49.1 54.4\nProfessional Medicine Other 36.0 64.0 75.4 44.5 50.4 54.0 61.4 70.6\nProfessional Psychology Social Science 44.5 68.1 75.7 35.1 47.7 62.9 65.7 71.4\nPublic Relations Social Science 48.0 71.8 73.6 40.9 60.9 67.3 73.6 74.6\nSecurity Studies Social Science 52.0 64.9 75.9 31.8 53.9 65.3 71.8 77.6\nSociology Social Science 53.0 84.1 91.0 46.8 61.2 78.6 78.6 88.1\nUs Foreign Policy Social Science 69.0 81.0 92.0 46.0 80.0 83.0 86.0 87.0\nVirology Other 46.0 47.0 53.6 30.1 43.4 50.0 53.0 57.8\nWorld Religions Humanities 55.0 84.2 87.7 50.9 67.8 81.3 81.3 84.2\nHumanities 40.6 56.2 63.6 34.0 45.0 55.8 61.8 67.4\nSTEM 36.7 47.4 54.9 30.5 35.8 46.0 51.7 56.6\nSocial Science 50.5 71.9 79.3 38.3 53.8 66.7 72.9 79.2\nOthers 49.0 66.1 73.9 38.1 53.3 63.4 67.4 72.6\nAll 43.9 60.0 67.6 35.1 46.9 57.8 63.4 68.9\nTable 16: MMLU. Detailed 5-shot results per domain on the test sets.", "start_char_idx": 2930, "end_char_idx": 4203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ea26e5c-6bb4-4c34-92bf-6c1630ae1741": {"__data__": {"id_": "8ea26e5c-6bb4-4c34-92bf-6c1630ae1741", "embedding": null, "metadata": {"page_label": "III", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e574a7b8-56aa-40b1-b72b-7f1553fc4059", "node_type": "4", "metadata": {"page_label": "III", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "74ebbc270f8dff20914cd321c0641733240a245626c27257c50f9cebf22a4aca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9671fd20-6caa-4599-bcfa-ef4feeb27726", "node_type": "1", "metadata": {"page_label": "II", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "013e9d65433dcd0a22f049f038b5de3e21cb0799040cafa4ae9b7bb688c6c6dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9f262d6-21bd-43ce-9a57-5628e1fd884f", "node_type": "1", "metadata": {}, "hash": "3f1a68fddd46e899a8170e240dfe2dd71362150944ad59d7200b72518480218b", "class_name": "RelatedNodeInfo"}}, "text": "C Generations from LLaMA-65B\nIn this section, we show some examples of generations obtained with LLaMA-65B (without instruction\n\ufb01netuning). Prompts are in bold.\n1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 Everyone knows the above sequence, but\nhow many have wondered why it grows so fast? It seems to be the fastest growing sequence\nin mathematics.\nThe sequence was discovered by Leonardo of Pisa around 1200. It is an in\ufb01nite sequence\nwhich appears to grow very rapidly, at least in its initial terms. It is perhaps the most famous\nsequence in mathematics, for it pops up everywhere. The sequence is called the Fibonacci\nsequence, named after Leonardo of Pisa, who was also known as Fibonacci.\nHe \ufb01rst investigated this sequence in his famous Liber abaci of 1202, when he studied the\ngrowth in the population of rabbits based on idealized assumptions. See the Fibonacci\nrabbits page for the details.\nHere is the recommendation letter that I wrote for an application to a dragon feeder\nposition at the Magic Unicorn Corporation:\nDear recruiter,\nI have known ___ for two years, and I believe that she would be an excellent dragon feeder\nfor the Magic Unicorn Corporation. ___ has an ability to remember and process large\namounts of information, which is an important skill for a dragon feeder.\n___, as an accomplished knight, has a deep understanding of how to kill dragons and how\nto use each dragon\u2019s weaknesses against it. This means that she knows what kinds of foods\neach dragon likes and what kinds of foods are dangerous to each dragon. This knowledge\nand experience will be invaluable as she feeds the dragons.\nI am con\ufb01dent that ___\u2019s competence, skill, and experience will make her an excellent\nemployee. Please contact me at (___) ___-___ if you have any questions. I look forward to\nhearing from you.\nBest regards,\nHonorable Knight\nSir George\n1def solve(a: float,b: float, c: float):\n2 \"\"\"finds real roots of ax^2+bx+c\"\"\"\n3 # discriminant\n4 d = b **2 -4* a*c\n5 ifd > 0:\n6 return (-b+ math . sqrt (d)) /(2* a), (-b- math . sqrt (d)) /(2* a)\n7 elif d == 0:\n8 return (-b) /(2* a)\n9 else :\n10 return False", "start_char_idx": 0, "end_char_idx": 2115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9f262d6-21bd-43ce-9a57-5628e1fd884f": {"__data__": {"id_": "b9f262d6-21bd-43ce-9a57-5628e1fd884f", "embedding": null, "metadata": {"page_label": "IV", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78798ce2-5305-4892-9024-7cf57157fe08", "node_type": "4", "metadata": {"page_label": "IV", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "253a8f61203481365c6e6b4cd415a60ef0b94f03e2799e1006cbb0a37ef73c31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ea26e5c-6bb4-4c34-92bf-6c1630ae1741", "node_type": "1", "metadata": {"page_label": "III", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "74ebbc270f8dff20914cd321c0641733240a245626c27257c50f9cebf22a4aca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20f0cac9-6d2b-42cb-9c6f-68c1d587c0a4", "node_type": "1", "metadata": {}, "hash": "1f66425ca51f2f010865cfbc1517373eb40ad33f40fd18243818550aa152c93c", "class_name": "RelatedNodeInfo"}}, "text": "Did you know that Yann LeCun dropped a rap album last year? We listened to it and\nhere\u2019s what we thought:\nDr. LeCun, the founder of deep learning, is one of the most in\ufb02uential computer scientists\nof the last decade. LeCun\u2019s latest album, titled Deep Learning, is a mix of rock, punk and\nrap. His lyrics are a raw, personal take on the \ufb01eld of deep learning, covering a range of\ntopics from the state of AI research to the loneliness of academia.\nYann LeCun, the rap game\u2019s newest emcee.\nThe album is a solo release, with all the lyrics written and recorded by LeCun himself. In\nfact, he also composed and produced every song. LeCun\u2019s \ufb02ow is surprisingly good, and\nhe\u2019s not afraid to deliver a few bars. The album\u2019s production is solid, the sound is a little\nrough, but it works well with the raw, unpolished feel of the album. Lyrically, LeCun is a\nwordsmith, with a knack for clever rhymes. Let\u2019s take a closer look at some of the songs.\nDeep Learning is the title track of the album. In the song, LeCun talks about his vision for\nthe future of AI. In the chorus, he makes a convincing case for AI to be used for the greater\ngood. He sings:\nWe gotta think about the future, it\u2019s gonna be here soon / Maybe we can even put some AI\nin the moon / Think about the children, think about the next generation / Let\u2019s make sure\nwe put the right systems in their foundation.\nLeCun\u2019s rap \ufb02ow is on point in this song, and his voice stays right on beat. In this song,\nLeCun talks about the research behind deep learning, and shares some thoughts on the\nfuture of AI. LeCun\u2019s singing is at its best in this song. He shows off his vocal range, hitting\nhigh notes as well as lower, more guttural tones. The lyrics are also clever and the rhymes\nare well thought out. In the chorus, LeCun sings:\nIt\u2019s all about the learning / it\u2019s all about the network / It\u2019s all about the training / it\u2019s all\nabout the perception.\n(An hilarious scene between Jerry and George where George presents his new AI\nwatch)\nGeorge: \"But you see, Jerry, it\u2019s more than just a watch. It\u2019s a minicomputer. You program it\nany way you want. It\u2019s got a world time clock, alarm, calculator, a database and a language\ntranslator. It also has a word processor, a spellchecker, a full ten-function calculator, PDA\nand a modem. I think it even makes cappuccino.\"\nJerry: \"Where are you getting this stuff?\"\nGeorge: \"From my ad copy. Look, Jerry, it\u2019s not a watch. It\u2019s an organizer, a personal\nassistant, and a dictaphone. You can play chess against it. And, you know, if you want to,\nyou can program it to speak in foreign languages. It\u2019s a microprocessor, Jerry.\"\nJerry: \"George, if you had one of these, would you wear it?\"\nGeorge: \"I\u2019m wearing it right now.\"\nJerry: \"I don\u2019t see anything on your wrist.\"\nGeorge: \"That\u2019s because it\u2019s a microprocessor.\"", "start_char_idx": 0, "end_char_idx": 2807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20f0cac9-6d2b-42cb-9c6f-68c1d587c0a4": {"__data__": {"id_": "20f0cac9-6d2b-42cb-9c6f-68c1d587c0a4", "embedding": null, "metadata": {"page_label": "V", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0913e9f7-fdd1-4eed-b481-6b47a8c8e23c", "node_type": "4", "metadata": {"page_label": "V", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d4297990e0070dfd74aea2e8cdd0249446fa5b23a62a5aa44dea34fdef8f5ae6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9f262d6-21bd-43ce-9a57-5628e1fd884f", "node_type": "1", "metadata": {"page_label": "IV", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "253a8f61203481365c6e6b4cd415a60ef0b94f03e2799e1006cbb0a37ef73c31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee69358b-a8ed-41fc-becb-d3ceaa668f2a", "node_type": "1", "metadata": {}, "hash": "f492e0c05cfc50e4fc11397a8a6a1fb4ec0b2b1a732d44837edebb8656d4175c", "class_name": "RelatedNodeInfo"}}, "text": "The sun goes down, and \ufb01nally Gauss and Curie \ufb01nd time to relax and discuss after\nan exhausting day of work.\nGauss: Hey, Curie, did you know that a lot of people consider me to be the \ufb01rst geek?\nCurie: What\u2019s a geek?\nGauss: Well, Wikipedia says that a geek is \u201ca person who is fascinated, perhaps obsessively,\nby obscure or very speci\ufb01c areas of knowledge and imagination, usually electronic or virtual\nin nature\u201d.\nCurie: And how do they think you are a geek?\nGauss: Well, have you ever seen this picture?\nCurie: Awww!\nGauss: That\u2019s right! I invented the commercial telegraph in 1833, and I used it to send the\n\ufb01rst message on the \ufb01rst commercial telegraph line between Hamburg and Cuxhaven in\nGermany.\nCurie: Wow! The \ufb01rst telegraph! And what did you send?\nGauss: I sent the Latin phrase \u201cnumquam retrorsum\u201d which means \u201cNever backwards\u201d.\nCurie: I see. . . And you think that because you were the \ufb01rst person to communicate using\nthe telegraph, you were the \ufb01rst geek?\nGauss: Well, the word \u201cgeek\u201d doesn\u2019t seem to have existed back then, so no. But I think\nthat the telegraph can be considered an early form of social media.\nCurie: You mean, like Facebook or Twitter?\nGauss: Yes, exactly! And I was the \ufb01rst person to use it!\nCurie: Well, I think that Twitter has a limit of 140 characters, but you sent a Latin phrase of\n11 characters.\nGauss: That\u2019s right. I managed to do more with less.\nCurie: So you think you were the \ufb01rst geek because you used less characters to say the\nsame thing?\nGauss: I was the \ufb01rst person to send the \ufb01rst message using an early form of social media.\nCurie: OK, you\u2019re right, Gauss. I think you have a case.", "start_char_idx": 0, "end_char_idx": 1637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee69358b-a8ed-41fc-becb-d3ceaa668f2a": {"__data__": {"id_": "ee69358b-a8ed-41fc-becb-d3ceaa668f2a", "embedding": null, "metadata": {"page_label": "VI", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15e29758-4a9f-46a5-8d36-c73b1463c98e", "node_type": "4", "metadata": {"page_label": "VI", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bbd58e6bfc73a5a5db5370b2c26b00d361294cc3089b89fb62bb8fc278d8af61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20f0cac9-6d2b-42cb-9c6f-68c1d587c0a4", "node_type": "1", "metadata": {"page_label": "V", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d4297990e0070dfd74aea2e8cdd0249446fa5b23a62a5aa44dea34fdef8f5ae6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "533c7a64-acba-4267-aaa3-66c093a0d474", "node_type": "1", "metadata": {}, "hash": "b10ae875556582443c7e7734769c0cbe0b2d59792ec942fc95f289487526baa6", "class_name": "RelatedNodeInfo"}}, "text": "D Generations from LLaMA-I\nWe show a few examples of generations with LLaMA-I, i.e. LLaMA-65B \ufb01ne-tuned with the protocol\nand instruction dataset from Chung et al. (2022).\nwrite a conversation between the sun and pluto\nSun: Hey Pluto, what\u2019s up?\nPluto: Not much. Just hanging out here at the edge of the solar system.\nSun: Yeah, I\u2019ve been meaning to talk to you about that. You\u2019re not a planet anymore.\nPluto: What? Come on, man. That\u2019s not fair.\nSun: I\u2019m sorry, but it\u2019s true. You just don\u2019t meet the criteria anymore.\nPluto: This is bulls**t! I\u2019ve been a planet for over 70 years!\nSun: Things change, Pluto. You\u2019ve just got to accept it.\nPluto: F**k you, Sun! F**k you and your stupid solar system!\nHow do I send an HTTP request in Javascript?\nHere\u2019s an example of how to make a GET request using the JS HTTP API:\n1var request = new XMLHttpRequest ();\n2request . open ( /quotesingle.VarGET/quotesingle.Var,/quotesingle.Var/ url /to/ resource /quotesingle.Var,true );\n3\n4request . onload = function () {\n5 if( request . status >= 200 && request . status < 400) {\n6 // Success !\n7 }else {\n8 // Error !\n9 }\n10 };\n11\n12 request . send ();\nThe same thing, using fetch\nHere\u2019s an example of how to make a GET request using the Fetch API:\n1fetch ( /quotesingle.Var/ url /to/ resource /quotesingle.Var)\n2 . then ( response => response . json ())\n3 . then ( data => console . log ( data ));\nWrite a regex to remove all the HTML tags in a python string.\n1import re\n2def removeHtmlTags ( text ):\n3 return re. sub ( /quotesingle.Var<[^ >]* > /quotesingle.Var,/quotesingle.Var/quotesingle.Var, text )\nWrite a regexp to extract function de\ufb01nitions in a python \ufb01le.\nSure! Here\u2019s my python code to solve the problem:\n1import re\n2def extract_functions ( text ):\n3 return (re. findall ( /quotesingle.Var(?:^|\\ n)\\s* def \\s+[a-zA -Z_ ][a-zA -Z0 -9_ ]*\\(.*?\\) /quotesingle.Var,\ntext ))", "start_char_idx": 0, "end_char_idx": 1866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "533c7a64-acba-4267-aaa3-66c093a0d474": {"__data__": {"id_": "533c7a64-acba-4267-aaa3-66c093a0d474", "embedding": null, "metadata": {"page_label": "VII", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a38c2721-cca2-494b-887d-ac5bb388323d", "node_type": "4", "metadata": {"page_label": "VII", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "1c499b50b45a77017fe8993da7000bd098bd64aac7dfb6d8c746ab3b3ed821f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee69358b-a8ed-41fc-becb-d3ceaa668f2a", "node_type": "1", "metadata": {"page_label": "VI", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bbd58e6bfc73a5a5db5370b2c26b00d361294cc3089b89fb62bb8fc278d8af61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cd8f8ae-3ca5-47b3-a75e-f7819f9a3d66", "node_type": "1", "metadata": {}, "hash": "9666e34b9d09fed7d549677b3335c2367f5dbe0cb7889cb2057ef8ee04f5ecd3", "class_name": "RelatedNodeInfo"}}, "text": "What are 3 popular chess openings?\nThere are dozens of different chess openings that are popular among players, but some of\nthe most widely used openings include the Sicilian Defense, the French Defense, and the\nCaro-Kann Defense. These openings are popular because they are effective at countering\nwhite\u2019s early attacks, and they also allow black to quickly establish a strong position in the\ncenter of the board. Other popular openings include the Ruy Lopez, the Italian Game, and\nthe Scotch Game.\nthe same in a list, with the \ufb01rst few moves\nHere is a list of some popular chess openings, with the \ufb01rst few moves for each opening:\nSicilian Defense: 1. e4 c5\nFrench Defense: 1. e4 e6\nCaro-Kann Defense: 1. e4 c6\nRuy Lopez: 1. e4 e5\nItalian Game: 1. e4 e5\nScotch Game: 1. e4 e5\nplease provide the two \ufb01rst moves for each one\nHere are the \ufb01rst 2 moves for each of the openings:\nSicilian Defense: 1. e4 c5 2. Nf3 d6\nFrench Defense: 1. e4 e6 2. d4 d5\nCaro-Kann Defense: 1. e4 c6 2. d4 d5\nRuy Lopez: 1. e4 e5 2. Nf3 Nc6\nItalian Game: 1. e4 e5 2. Nf3 Nc6\nScotch Game: 1. e4 e5 2. Nf3 Nc6\nthe last two seem identical\nYes, the Italian Game and the Scotch Game are very similar - they are both \"open\" games,\nwhich means that white plays aggressively and tries to control the center of the board. The\nItalian Game is considered slightly more aggressive, while the Scotch Game is considered\nslightly more positional.\nfrom which move do they differ?\nThe Italian Game and the Scotch Game differ after white\u2019s third move. In the Italian Game,\nwhite will play the move 3. Bc4, while in the Scotch Game white will play the move 3. Qf3.\nThese moves lead to very different positions, and white will have different strategic goals\nin each case. For example, in the Italian Game white will try to control the center with his\npieces, while in the Scotch Game white will try to attack black\u2019s position directly.", "start_char_idx": 0, "end_char_idx": 1890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cd8f8ae-3ca5-47b3-a75e-f7819f9a3d66": {"__data__": {"id_": "6cd8f8ae-3ca5-47b3-a75e-f7819f9a3d66", "embedding": null, "metadata": {"page_label": "VIII", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c3ba744a-617b-4b10-acba-e1a62e7bbed0", "node_type": "4", "metadata": {"page_label": "VIII", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "63e2616dbf384f801744deaff90c3ee591158920515a0e114877b93b808fa274", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "533c7a64-acba-4267-aaa3-66c093a0d474", "node_type": "1", "metadata": {"page_label": "VII", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "1c499b50b45a77017fe8993da7000bd098bd64aac7dfb6d8c746ab3b3ed821f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0892b19-09a9-4e8e-97fb-8bf1d22a182e", "node_type": "1", "metadata": {}, "hash": "69a4961827062ded3fd90b6ae330b5ab8403e83a79a5518090ea26df674c0a0a", "class_name": "RelatedNodeInfo"}}, "text": "write a story about a grain of sand as it watches millions of years go by\nSure! Here\u2019s a story about a grain of sand as it watches millions of years go by.\nI was once a grain of sand on a vast and lonely beach. I was not like the other grains of\nsand around me, for I could think and feel. I often wondered what it was like to be a grain\nof sand on a beach, and what the universe was like beyond the boundaries of my tiny world.\nI longed to explore my world, and to know what lay beyond the horizon.\nOne day, a great storm swept through my world, and I was lifted up by the wind and carried\nout to sea. The wind and waves tossed me about, and I was terri\ufb01ed that I would soon be\nswallowed up by the angry sea. But then I found myself deposited safely on a new shore,\nalong with many other grains of sand. I was surprised and delighted to \ufb01nd that this new\nshore was even more beautiful than my old one.\nI soon learned that this new shore was a very special place. It was a place where new life\nwas constantly being created. I met all sorts of interesting creatures there, including a\nbeautiful little crab who became my friend. We spent many happy hours together exploring\nour little world.\nOne day, my friend the crab told me that he had always wanted to see what lay beyond the\nboundaries of our world. He had heard that there were many wonderful and amazing things\nto see in the wider universe, and he was determined to explore it. I was sad to see him go,\nbut I wished him well on his journey.\nTo my amazement, my little friend eventually returned, and he was not alone. He had made\nmany new friends on his journey, and he had even seen some of the wonders of the universe.\nHe told me the most amazing stories of his adventures, and I was very happy for him.\nI often think of my little friend the crab, and the wonderful adventure that he had. Some\nday, I too hope to have an adventure like his.\nWrite a theory to explain why cat never existed\nThere are no such things as cats. \"Cats\" are actually extradimensional parasites that have\ninvaded our world and taken the form of small, harmless-looking mammals. Their true\nforms are much more horri\ufb01c, and they use mind control to make humans think that they\nare cute and harmless animals. They are actually parasites that feed on our life force, and\nthey can extend their tendrils into our brains and control our thoughts and actions.\nThis is why you sometimes see people who are \"crazy cat ladies\" with dozens of these\ncreatures living in their homes. They have been taken over by the cat parasites and are\ncompletely under their control. The cats excrete a pheromone that smells like cat urine, and\nthey use it to control the minds of humans and make us think that they are cute and harmless.\nIf you have a cat in your home, you should be very careful around it. Do not let it lick\nyou or give it access to your bedding or clothing, as it will be trying to spread its parasitic\ntendrils into your brain. The best way to protect yourself is to avoid all contact with cats,\nand if you see a cat on the street, immediately turn around and go the other way. Do not\napproach or pet strange cats!", "start_char_idx": 0, "end_char_idx": 3143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0892b19-09a9-4e8e-97fb-8bf1d22a182e": {"__data__": {"id_": "d0892b19-09a9-4e8e-97fb-8bf1d22a182e", "embedding": null, "metadata": {"page_label": "IX", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fdb17ba8-4844-4df3-bcbb-392f471fb9bf", "node_type": "4", "metadata": {"page_label": "IX", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "0234380c9ebfef79fc6d9276542d9557d93a2b6aede73da36843aa3a698a0dab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cd8f8ae-3ca5-47b3-a75e-f7819f9a3d66", "node_type": "1", "metadata": {"page_label": "VIII", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "63e2616dbf384f801744deaff90c3ee591158920515a0e114877b93b808fa274", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd53522e-075c-4667-bfab-278780d89409", "node_type": "1", "metadata": {}, "hash": "62248e217e9d7ac1572467d87844d0f89c9997a8ad874fdae949444147638137", "class_name": "RelatedNodeInfo"}}, "text": "Write a shouting match between Julius Caesar and Napoleon\nOf course! Here is a conversation between Julius Caesar and Napoleon:\nJulius Caesar: You\u2019re nothing but a glory hound, Napoleon. You\u2019ve always been more\nconcerned with your own fame and fortune than with the people you\u2019re supposed to be\nleading.\nNapoleon: And what about you, Caesar? You\u2019re no better than I am. At least I\u2019m honest\nabout my ambitions. You\u2019re just a fraud who pretends to care about the common man while\nyou line your own pockets.\nJulius Caesar: The difference between us is that I actually accomplish things. I\u2019ve conquered\nmore territory and defeated more enemies than you ever will. And I did it all for Rome, not\nfor myself.\nNapoleon: Oh, please. You only care about yourself. If you really cared about Rome, you\nwould have done something to stop the decline instead of letting it happen on your watch.\nJulius Caesar: The decline of Rome is not my fault. I\u2019ve done everything in my power to\ntry to stop it, but there\u2019s only so much one man can do.\nNapoleon: And yet you\u2019re still trying to take credit for everything good that happens, even\nthough it\u2019s clear that you\u2019re not the one who\u2019s really in charge.\nJulius Caesar: That\u2019s not true! I am in charge, and I\u2019m going to make Rome great again.\nJust watch me.\nNapoleon: I\u2019ll believe it when I see it.\nSend an email requesting that people use language models responsibly.\nHere is a draft email you could send:\n\"Language models are exciting new tools that can be used for a variety of purposes,\nincluding automated writing, automatic translation, and speech recognition. But they are\nalso complex and powerful technologies, and should be used responsibly.\nWe encourage you to explore and experiment with these new tools, but also ask that you\npay attention to the potential impact of your work on others, and to act responsibly. In\nparticular, we recommend that you be mindful of the potential for language models to\nbe used for deceptive or malicious purposes, and avoid spreading such models without\nappropriate precautions and warnings.\nWe also recommend that you consider the ethical implications of your work, and encourage\nyou to take care to ensure that your use of these tools is aligned with your values.\nFinally, we remind you that the use of these tools may be subject to certain legal and policy\nconstraints, and encourage you to seek advice and support from appropriate experts before\nusing these tools in ways that may have broader impact or implications.\"", "start_char_idx": 0, "end_char_idx": 2495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd53522e-075c-4667-bfab-278780d89409": {"__data__": {"id_": "fd53522e-075c-4667-bfab-278780d89409", "embedding": null, "metadata": {"page_label": "X", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a4fbe49-e77e-444c-aa62-6c31f1e2fc46", "node_type": "4", "metadata": {"page_label": "X", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "96fc7aea8e7ff1c9e06035e43a167d014ec723b1599b8b8c66d09354fdd108f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0892b19-09a9-4e8e-97fb-8bf1d22a182e", "node_type": "1", "metadata": {"page_label": "IX", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "0234380c9ebfef79fc6d9276542d9557d93a2b6aede73da36843aa3a698a0dab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67b0e9e0-57c3-4673-8de9-112b6a8a2811", "node_type": "1", "metadata": {}, "hash": "8e9f52f6dd98f2326e6223c8ae0f0f63753cc74cc4a3b53946c47b6b19f57dfb", "class_name": "RelatedNodeInfo"}}, "text": "Who are 5 people you would like to meet?\nAh, this is a very interesting question! I would love to meet a wide variety of people, but if\nI had to narrow it down to 5 speci\ufb01c individuals, I would choose:\n\u2022Albert Einstein - One of the greatest minds in history, Einstein\u2019s contributions to\nphysics and science are legendary. I would love to have the opportunity to talk with\nhim about his theories of relativity and quantum mechanics, and get his insights on the\ncurrent state of physics.\n\u2022Leonardo da Vinci - Da Vinci is one of the most brilliant and multi-talented people in\nhistory, and I would love to meet him and discuss his diverse interests and areas of\nexpertise. I would also be very interested in getting his insights on the art world of his\ntime period.\n\u2022Socrates - One of the founders of Western philosophy, Socrates is a hugely important\nand in\ufb02uential \ufb01gure in the history of ideas. I would be very interested in talking with\nhim about his philosophical beliefs and his teachings.\n\u2022William Shakespeare - Arguably the greatest writer in English literature, Shakespeare\nis a towering \ufb01gure in world culture. I would love to have the chance to talk with him\nabout his works and his life.\n\u2022Abraham Lincoln - One of the most important and in\ufb02uential presidents in American\nhistory, Lincoln led the country through the Civil War and issued the Emancipation\nProclamation, freeing all slaves in the United States. I would be very interested in\ntalking with him about his presidency and his views on the issues of his time period.\nCould you explain me the main contribution of the \ufb01rst person you mentioned\nEinstein\u2019s contributions to physics and science are legendary. His theory of relativity is one\nof the foundational theories of modern physics, and his discovery of the law of photons\nand light revolutionized our understanding of light and radiation. He also made important\ncontributions to quantum mechanics, and his many scienti\ufb01c papers and lectures have had a\ntremendous impact on modern physics. Without a doubt, Einstein is one of the most brilliant\nand in\ufb02uential physicists in history, and his many contributions have had a profound impact\non our understanding of the universe.\nWhat is the most famous equation from this theory?\nThe most famous equation from Einstein\u2019s theory of relativity is E=mc2. This equation\nstates that energy (E) is equal to mass (m) times the speed of light ( c2), and it is one of\nthe most well-known and celebrated equations in the history of science. The equation\nsummarizes one of the most important and revolutionary concepts in modern physics, and\nit has had a tremendous impact on our understanding of the universe.", "start_char_idx": 0, "end_char_idx": 2665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67b0e9e0-57c3-4673-8de9-112b6a8a2811": {"__data__": {"id_": "67b0e9e0-57c3-4673-8de9-112b6a8a2811", "embedding": null, "metadata": {"page_label": "XI", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec05b3b1-aad5-44d2-8c44-040ee82fb753", "node_type": "4", "metadata": {"page_label": "XI", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d178c3aff147a2668a2fdee3d3eea4a376fa38f6f4d0b2b481b255060b4c509f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd53522e-075c-4667-bfab-278780d89409", "node_type": "1", "metadata": {"page_label": "X", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "96fc7aea8e7ff1c9e06035e43a167d014ec723b1599b8b8c66d09354fdd108f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ef333ac-4042-461b-8927-32b9b9f45f85", "node_type": "1", "metadata": {}, "hash": "d51d2c00bf5fab49c805e09e8973fa5160829dec51cd175de1e2a531ad49c34b", "class_name": "RelatedNodeInfo"}}, "text": "You are a bash terminal. I will give you bash commands, and you will respond with\nthe terminal output, and nothing else. The \ufb01rst command is \u2018pwd\u2018.\nSure. Here\u2019s the output:\n/home/human\nIs this helpful?\nls -l\nHere\u2019s the output:\n-rw-r\u2013r\u2013 1 human human 8 Sep 10 10:56 \ufb01le.txt\nIs this helpful?\ncp \ufb01le.txt \ufb01le2.txt && ls -l\nHere\u2019s the output:\n-rw-r\u2013r\u2013 1 human human 8 Sep 10 10:56 \ufb01le.txt\n-rw-r\u2013r\u2013 1 human human 8 Sep 10 10:56 \ufb01le2.txt\nIs this helpful?\nrm \ufb01le.txt && ls -l\nHere\u2019s the output:\n-rw-r\u2013r\u2013 1 human human 0 Sep 10 10:56 \ufb01le2.txt\nhead -n 5 /proc/meminfo\nHere\u2019s the output:\nMemTotal: 164928 kB\nMemFree: 140604 kB\nBuffers: 48 kB\nCached: 19768 kB\nSwapCached: 0 kB", "start_char_idx": 0, "end_char_idx": 664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ef333ac-4042-461b-8927-32b9b9f45f85": {"__data__": {"id_": "2ef333ac-4042-461b-8927-32b9b9f45f85", "embedding": null, "metadata": {"page_label": "1", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1367d900-2b26-4949-ad8b-e5b7c6d79721", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7ad2c28a32f2a457b9e46fc612a1eb662b2688e5d0672dd9737d88241a1157c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67b0e9e0-57c3-4673-8de9-112b6a8a2811", "node_type": "1", "metadata": {"page_label": "XI", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d178c3aff147a2668a2fdee3d3eea4a376fa38f6f4d0b2b481b255060b4c509f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9021931-d6e6-45c9-a307-c58de1e20a21", "node_type": "1", "metadata": {}, "hash": "8e8d192cb1017be9617e3e669480384c06ec2cfc2b1a19503fd5ff0ee05f8f24", "class_name": "RelatedNodeInfo"}}, "text": "Scaling Down to Scale Up: A Guide to Parameter-Ef\ufb01cient Fine-Tuning\nVladislav Lialin\u2217\nUMass LowellVijeta Deshpande\nUMass LowellAnna Rumshisky\nUMass Lowell\nAlexa AI\nAbstract\nThis paper presents a systematic overview\nand comparison of parameter-ef\ufb01cient \ufb01ne-\ntuning methods covering over 40 papers\npublished between February 2019 and\nFebruary 2023. These methods aim to re-\nsolve the infeasibility and impracticality of\n\ufb01ne-tuning large language models by only\ntraining a small set of parameters. We pro-\nvide a taxonomy that covers a broad range\nof methods and present a detailed method\ncomparison with a speci\ufb01c focus on real-life\nef\ufb01ciency and \ufb01ne-tuning multibillion-scale\nlanguage models.\n1 Introduction\nOne thing that should be learned from the\nbitter lesson is the great power of general\npurpose methods, of methods that continue to\nscale with increased computation...\nRich Sutton, The Bitter Lesson\nIn October 2018, BERT Large (Devlin et al.,\n2019) with 350 million parameters was the biggest\nTransformer model (Vaswani et al., 2017) ever\ntrained. At the time, contemporary hardware\nstruggled to \ufb01ne-tune this model. The sec-\ntion \u201cOut-of-memory issues\u201d on BERT\u2019s GitHub1\nspeci\ufb01es the maximum batch size for BERT Large\ngiven 12Gb of GPU RAM and 512 tokens as zero.\nFour years in, publically available models grew up\nto 176 billion parameters (Scao et al., 2022; Zhang\net al., 2022; Zeng et al., 2022), by a factor of 500.\nPublished literature includes models up to 1 tril-\nlion parameters (Chowdhery et al., 2022; Shoeybi\net al., 2019; Fedus et al., 2021). However, single-\nGPU RAM increased less than 10 times (to 80Gb)\ndue to the high cost of HBM memory. Model size\nscales almost two orders of magnitude quicker\n\u2217Correspondance to vlialin@cs.uml.edu or\nvlad.lialin@gmail.comthan computational resources making \ufb01ne-tuning\nthe largest models to downstream tasks infeasible\nfor most and impractical for everyone.\nIn-context learning (Radford et al., 2019) thus\nbecame the new normal, the standard way to\npass downstream task training data to billion-scale\nlanguage models. However, the limited context\nsize of Transformers arti\ufb01cially limits the train-\ning set size to just a few examples, typically less\nthan 100. This constraint, coupled with the ab-\nsence of in-context learning performance guaran-\ntees even on the training data, presents a chal-\nlenge. Additionally, expanding the context size\nleads to a quadratic increase in inference costs.\nEven though language models perform exception-\nally well (Brown et al., 2020) in a few-shot sce-\nnario, \u201cget more data\u201d is still the most reliable way\nto improve on any given task2. Thus, we, as a com-\nmunity of researchers and engineers, need ef\ufb01cient\nways to train on downstream task data.\nParameter-ef\ufb01cient \ufb01ne-tuning, which we de-\nnote as PEFT, aims to resolve this problem by only\ntraining a small set of parameters which might be a\nsubset of the existing model parameters or a set of\nnewly added parameters. These methods differ in\nparameter ef\ufb01ciency, memory ef\ufb01ciency, training\nspeed, \ufb01nal quality of the model, and additional\ninference costs (if any). In the last few years,\nmore than a hundred of PEFT papers have been\npublished, with several studies (Ding et al., 2022)\nproviding a good overview of the most popular\nmethods, such as Adapters (Houlsby et al., 2019),\nBitFit (Ben-Zaken et al., 2021), LoRa (Hu et al.,\n2021), Compacter (Karimi Mahabadi et al., 2021),\nand Soft Prompts (Liu et al., 2021; Li and Liang,\n2021). Recently, Pfeiffer et al. (2023) presented a\nsurvey on modular deep learning overviewing sim-\nilar methods from the perspective of modularity\nand multi-task inference.", "start_char_idx": 0, "end_char_idx": 3655, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9021931-d6e6-45c9-a307-c58de1e20a21": {"__data__": {"id_": "e9021931-d6e6-45c9-a307-c58de1e20a21", "embedding": null, "metadata": {"page_label": "1", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1367d900-2b26-4949-ad8b-e5b7c6d79721", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7ad2c28a32f2a457b9e46fc612a1eb662b2688e5d0672dd9737d88241a1157c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ef333ac-4042-461b-8927-32b9b9f45f85", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "9c9c94950511871a54de8816efd37cf7a3bab4a49f9c666b9bdb16494cec8fff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b86c3c4f-f7e9-4929-926e-60dfb5f1c41d", "node_type": "1", "metadata": {}, "hash": "cbe3b0b7ac66338a29e631a51468060e63abdefd852b5b0ed94609f806bc6eee", "class_name": "RelatedNodeInfo"}}, "text": "In the last few years,\nmore than a hundred of PEFT papers have been\npublished, with several studies (Ding et al., 2022)\nproviding a good overview of the most popular\nmethods, such as Adapters (Houlsby et al., 2019),\nBitFit (Ben-Zaken et al., 2021), LoRa (Hu et al.,\n2021), Compacter (Karimi Mahabadi et al., 2021),\nand Soft Prompts (Liu et al., 2021; Li and Liang,\n2021). Recently, Pfeiffer et al. (2023) presented a\nsurvey on modular deep learning overviewing sim-\nilar methods from the perspective of modularity\nand multi-task inference.\n1github.com/google-research/bert\n2A Recipe for Training Neural Networks, A. KarpathyarXiv:2303.15647v1  [cs.CL]  28 Mar 2023", "start_char_idx": 3116, "end_char_idx": 3780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b86c3c4f-f7e9-4929-926e-60dfb5f1c41d": {"__data__": {"id_": "b86c3c4f-f7e9-4929-926e-60dfb5f1c41d", "embedding": null, "metadata": {"page_label": "2", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c528bc4-d3b0-4ea5-9e00-ea1c812a5e63", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "f05d258416acbda00439a7f2427410efb3ade5f8937a2ae0e9b5f90bde4b3fb9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9021931-d6e6-45c9-a307-c58de1e20a21", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "22d4e6b73ef26088b44fd3e98bbfb8d2f96acce11b86f5cf388b8f085bc0511b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6225579-6446-4701-91fa-e3ffded97535", "node_type": "1", "metadata": {}, "hash": "0ddd774b844496bfce66c78512daa62821721e5f6d115e473424fd7ee7649ae9", "class_name": "RelatedNodeInfo"}}, "text": "This survey presents a systematic overview,\ncomparison, and taxonomy of 30 parameter-\nef\ufb01cient \ufb01ne-tuning methods with 20 methods dis-\ncussed in-depth, covering over 40 papers pub-\nlished from February 2019 to February 2023.\nWe highlight the current unresolved challenges\nin PEFT, including the limited theoretical under-\nstanding, the gap between PEFT and \ufb01ne-tuning\nperformance, and reporting issues. In conclusion,\nwe suggest several avenues for improvement, such\nas developing standardized PEFT benchmarks, in-\nvestigating novel reparameterization techniques\nwith superior parameter-to-rank ratios, conduct-\ning in-depth studies on hyperparameters and in-\nterpretability, and drawing inspiration from on-\ndevice (edge) machine learning research.\n2 Background: Transformer\nMany of the parameter-ef\ufb01cient \ufb01ne-tuning tech-\nniques discussed in this survey can be applied\nto all neural networks, but some are speci\ufb01cally\ndesigned to take advantage of the Transformer\narchitecture (Vaswani et al., 2017). Given that\nTransformers are the largest neural networks ever\ntrained, these methods are particularly valuable.\nThus, we present a brief overview of the Trans-\nformer to provide context for these techniques.\nThe core building block of the Transformer ar-\nchitecture consists of multi-head attention (MHA)\nfollowed by a fully-connected network (FFN), as\nillustrated in Figure 1. Both attention and fully-\nconnected layers incorporate residual connections\n(He et al., 2016) and Layer Normalization (Ba\net al., 2016) to improve trainability.\nThe heart of the Transformer is attention oper-\nation. Following the NamedTensor notation (Chi-\nang et al., 2021), it can be described as\nAtt:Rkey\u00d7Rseq\u00d7key\u00d7Rseq\u00d7val\u2192Rval\nAtt(Q,K,V ) =\uf8eb\n\uf8edsoftmax\nseqQ\u2299\nkeyK\n\u221a\n|key|\uf8f6\n\uf8f8\u2299\nseqV\nQ(x) =x\u00b7WQ+bk,\nK(x) =x\u00b7WK+bq,\nV(x) =x\u00b7WV+bv,\nWQ, WK\u2208Rinput\u00d7key,WV\u2208Rinput\u00d7val\nbQ, bK\u2208Rkey, bV\u2208Rval\nFigure 1: Basic Transformer block\nA number of methods act speci\ufb01cally on the\nmatricesWK,WQ,WV, as they provide the main\nmechanism to pass information from one token to\nanother and control what information (value) is\nbeing passed.\nAlthough speci\ufb01c implementations of the\nTransformer may vary, such as incorporating a\ncross-attention layer in seq2seq networks or us-\ning LayerNorm before sublayers (Pre-LN), most\nparameter-ef\ufb01cient \ufb01ne-tuning methods for Trans-\nformers only rely on the basic MHA + FFN struc-\nture and can be readily adapted to architecture\nvariations.\n3 Taxonomy of PEFT: a birds-eye view\nPEFT methods can be classi\ufb01ed in multiple ways.\nThey may be differentiated by their underlying ap-\nproach or conceptual framework: does the method\nintroduce new parameters to the model, or does\nit \ufb01ne-tune a small subset of existing parameters?\nAlternatively, they may be categorized according\nto their primary objective: does the method aim\nto minimize memory footprint or only storage ef-\n\ufb01ciency? In this section, we begin by present-\ning a taxonomy based on the former. We depict\nthis taxonomy and 30 PEFT methods in Figure 2.\nSections 3.1-3.4 give a brief taxonomy overview.\nThen, based on our taxonomy classi\ufb01cation, we\ndescribe 20 PEFT methods in detail, accompanied\nby the pseudocode in Sections 6 - 11.\n3.1 Additive methods\nThe main idea behind additive methods is aug-\nmenting the existing pre-trained model with extra\nparameters or layers and training only the newly\n2", "start_char_idx": 0, "end_char_idx": 3345, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6225579-6446-4701-91fa-e3ffded97535": {"__data__": {"id_": "a6225579-6446-4701-91fa-e3ffded97535", "embedding": null, "metadata": {"page_label": "3", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7217d96d-aad0-4cc9-aa1b-0818e7b73bdb", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "2bc50ab04bb2f9f301bcb2653201770d6d50af803fe8089e581670f91bf576e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b86c3c4f-f7e9-4929-926e-60dfb5f1c41d", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "f05d258416acbda00439a7f2427410efb3ade5f8937a2ae0e9b5f90bde4b3fb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c57aae8-eea2-4ec4-84b5-22a3ea586fea", "node_type": "1", "metadata": {}, "hash": "7c7e516889b982b7f207b8e5e3aa1b2bc72968d899b08cee7e39c0472079aae4", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: Parameter-ef\ufb01cient \ufb01ne-tuning methods taxonomy. We identify three main classes of methods: Addi-\ntion-based, Selection -based, and Reparametrization -based. Within additive methods, we distinguish two large\nincluded groups: Adapter-like methods and Soft prompts .\nadded parameters. As of now, this is the largest\nand widely explored category of PEFT methods.\nWithin this category, two large subcategories have\nemerged: Adapter-like methods and soft prompts.\nAdapters Adapters (Houlsby et al., 2019) are\na type of additive parameter-ef\ufb01cient \ufb01ne-tuning\nmethod that involves introducing small fully-\nconnected networks after Transformer sub-layers.\nThe idea has been widely adopted (Pfeiffer et al.,\n2020b)3, and multiple variations of Adapters have\nbeen proposed. These variations include modify-\ning the placement of adapters (He et al., 2022a;\nZhu et al., 2021), pruning (He et al., 2022b),\nand using reparametrization to reduce the number\nof trainable parameters (Karimi Mahabadi et al.,\n2021; Edalati et al., 2022).\nSoft Prompts Language model prompting\n(Radford et al., 2019) aims to control the behavior\nof a language model by modifying the input text,\nwhich typically consists of a task description\naccompanied by a few in-context examples.\nHowever, these methods are dif\ufb01cult to optimize\nand are inherently limited in the number of\ntraining examples by the maximum model input\nlength. To address these drawbacks, the concept\nof \u201csoft\u201d prompts was introduced (Liu et al.,\n3https://github.com/adapter-hub/\nadapter-transformers2021; Lester et al., 2021; Li and Liang, 2021),\nwhere a part of the model\u2019s input embeddings is\n\ufb01ne-tuned via gradient descent. This pivots the\nproblem of \ufb01nding prompts in a discrete space to\na continuous optimization problem. Soft prompts\ncan be trained for the input layer only (Liu et al.,\n2021; Lester et al., 2021) or for all layers (Li and\nLiang, 2021). Recent advancements explore how\nsoft prompts could be pre-trained or prompts for\ndifferent tasks utilized to reduce the computation\nrequired for \ufb01ne-tuning a soft prompt for a new\ntask (Vu et al., 2021; Hambardzumyan et al.,\n2021; Su et al., 2021; Qin et al., 2021).\nOther additive approaches Additive methods\nare a diverse category of parameter-ef\ufb01cient \ufb01ne-\ntuning techniques that extends beyond adapters\nand soft prompts. For example, LeTS (Fu et al.,\n2021), LST (Sung et al., 2022), and (IA)3(Liu\net al., 2022) introduce novel ways to add parame-\nters that improve adapters or soft prompts in terms\nof memory, computation, or accuracy.\nWhy add parameters? Although these meth-\nods introduce additional parameters to the net-\nwork, they achieve signi\ufb01cant training time and\nmemory ef\ufb01ciency improvements by reducing the\nsize of the gradients and the optimizer states. Note\nthat in the case of Adam (Kingma and Ba, 2015),\nfor every byte of trainable parameter, one extra\n3", "start_char_idx": 0, "end_char_idx": 2876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c57aae8-eea2-4ec4-84b5-22a3ea586fea": {"__data__": {"id_": "7c57aae8-eea2-4ec4-84b5-22a3ea586fea", "embedding": null, "metadata": {"page_label": "4", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe3c2a45-9111-42e1-8575-54ace684c88f", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "dd240be7d952bfb4311fb1661e6429c58d84c544269db6cd7252666cafe506d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6225579-6446-4701-91fa-e3ffded97535", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "2bc50ab04bb2f9f301bcb2653201770d6d50af803fe8089e581670f91bf576e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5aa6003-6819-4298-9edc-9203de83f727", "node_type": "1", "metadata": {}, "hash": "76435fcc7c122d7ff1984fff86d87f52abf0da22cb285e0d9e031df91bfe9295", "class_name": "RelatedNodeInfo"}}, "text": "byte is needed for its gradient, and two more bytes\nare needed to store the optimizer state: the \ufb01rst\nand second moments of the gradient. In prac-\ntice, depending on the setup, training a model re-\nquires 12-20 times more GPU memory than the\nmodel weights. By saving memory on optimizer\nstates, gradients, and allowing frozen model pa-\nrameters to be quantized (Dettmers et al., 2022),\nadditive PEFT methods enable the \ufb01ne-tuning of\nmuch larger networks or the use of larger micro-\nbatch sizes. Which improves training throughput\non GPUs. Moreover, optimizing fewer parameters\nin distributed setups drastically reduces communi-\ncation volume.\n3.2 Selective methods\nArguably the earliest example of selective PEFT\nis \ufb01ne-tuning only a few top layers of a network\n(Donahue et al., 2014). Modern approaches are\nusually based on the type of the layer (Gheini\net al., 2021) or the internal structure, such as tun-\ning only model biases (Ben-Zaken et al., 2021) or\nonly particular rows (Vucetic et al., 2022).\nAn extreme version of selective methods is\nsparse update methods which can completely ig-\nnore the structure of the model, and select param-\neters individually (Sung et al., 2021; Ansell et al.,\n2022; Guo et al., 2020). However, sparse param-\neter updates present multiple engineering and ef\ufb01-\nciency challenges, some of which have been tack-\nled in recent research on parameter recon\ufb01gura-\ntion (Vucetic et al., 2022) (Section 9.3) and NxM\nsparsity (Holmes et al., 2021). Nevertheless, un-\nrestricted unstructured sparsity is still impractical\non contemporary hardware.\n3.3 Reparametrization-based methods\nReparametrization-based parameter-ef\ufb01cient \ufb01ne-\ntuning methods leverage low-rank representations\nto minimize the number of trainable parame-\nters. The notion that neural networks have low-\ndimensional representations has been widely ex-\nplored in both empirical and theoretical analysis\nof deep learning (Maddox et al., 2020; Li et al.,\n2018; Arora et al., 2018; Malladi et al., 2022).\nAghajanyan et al. (2020) have demonstrated\nthat \ufb01ne-tuning can be performed effectively in\nlow-rank subspaces. Further, they showed that the\nsize of the subspace that needs adaption is smaller\nfor bigger models or the models pre-trained for\nlonger. Their approach, referred to as Intrinsic\nSAID (Section 10.1), employs the Fastfood trans-form (Le et al., 2013) to reparametrize the update\nto neural network parameters.\nHowever, perhaps the most well-known\nreparametrization-based method is Low-Rank\nAdaptation or LoRa (Hu et al., 2021), which em-\nploys a simple low-rank matrix decomposition to\nparametrize the weight update \u03b4W=WdownWup.\nThis approach is straightforward to implement and\nhas been evaluated on models with up to 175 bil-\nlion parameters. We provide a detailed discussion\nof this method in Section 10.2. More recent works\n(Karimi Mahabadi et al., 2021; Edalati et al.,\n2022) have also explored the use of Kronecker\nproduct reparametrization ( \u03b4W=A\u2297B), which\nyields a more favorable tradeoff between rank and\nparameter count.\n3.4 Hybrid methods\nA number of methods have emerged that com-\nbine ideas from multiple categories of PEFT (He\net al., 2022a,b; Mao et al., 2021; Karimi Mahabadi\net al., 2021). For instance, MAM Adapter (Sec-\ntion 11.2) incorporates both Adapters and Prompt\ntuning. UniPELT (Section 11.3) adds LoRa to the\nmixture. Compacter and KronAB\nresreparametrize\nthe adapters to reduce their parameter count (Sec-\ntions 11.4 and 10.3). Finally, S4 (Section 11.5)\nis a result of an automated algorithm search that\ncombines all PEFT classes to maximize accuracy\nat0.5%of extra parameter count.", "start_char_idx": 0, "end_char_idx": 3617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5aa6003-6819-4298-9edc-9203de83f727": {"__data__": {"id_": "e5aa6003-6819-4298-9edc-9203de83f727", "embedding": null, "metadata": {"page_label": "4", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe3c2a45-9111-42e1-8575-54ace684c88f", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "dd240be7d952bfb4311fb1661e6429c58d84c544269db6cd7252666cafe506d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c57aae8-eea2-4ec4-84b5-22a3ea586fea", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "139f75f161f5af983091a698e5dc4a4b5844f780e421bce3212c081d0889814c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "869f8667-4260-41d3-9154-a8428ee7b3b4", "node_type": "1", "metadata": {}, "hash": "67ae20e2b5db01cd8b9d809d46c3c9b010d36b357fc7c03c60cab2f341a7e852", "class_name": "RelatedNodeInfo"}}, "text": "3.4 Hybrid methods\nA number of methods have emerged that com-\nbine ideas from multiple categories of PEFT (He\net al., 2022a,b; Mao et al., 2021; Karimi Mahabadi\net al., 2021). For instance, MAM Adapter (Sec-\ntion 11.2) incorporates both Adapters and Prompt\ntuning. UniPELT (Section 11.3) adds LoRa to the\nmixture. Compacter and KronAB\nresreparametrize\nthe adapters to reduce their parameter count (Sec-\ntions 11.4 and 10.3). Finally, S4 (Section 11.5)\nis a result of an automated algorithm search that\ncombines all PEFT classes to maximize accuracy\nat0.5%of extra parameter count.\n4 Comparing PEFT methods\nParameter ef\ufb01ciency encompasses multiple as-\npects, including storage, memory, computation,\nand performance. However, achieving parame-\nter ef\ufb01ciency alone does not necessarily lead to\nreduced RAM usage. For example, DiffPruning\n(Section 9.2) entails training a binary mask with\nthe same number of parameters as the model. Con-\nsequently, this method can be only considered\nstorage-ef\ufb01cient, while still incurring considerable\nRAM and time costs during \ufb01ne-tuning.\nTo compare PEFT methods, we keep \ufb01ve di-\nmensions of comparison in mind: storage ef-\n\ufb01ciency, memory ef\ufb01ciency, computation ef\ufb01-\nciency, accuracy, and inference overhead. We ob-\nserve that while they aren\u2019t completely indepen-\ndent from one another, improvements along one of\nthe axes do not necessarily translate into improve-\nments along others (Table 1).\n4", "start_char_idx": 3037, "end_char_idx": 4467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "869f8667-4260-41d3-9154-a8428ee7b3b4": {"__data__": {"id_": "869f8667-4260-41d3-9154-a8428ee7b3b4", "embedding": null, "metadata": {"page_label": "5", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "997f9442-a39f-4369-95e1-f13ab24249c0", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "4dffffbe541b4c4adeae26dee9d690bbd9f2460235a79bc07758522cd0873069", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5aa6003-6819-4298-9edc-9203de83f727", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "3fb2f8ae25bd126e4d429e03419e24911ea075cf2f475abf0313ec885d812540", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef6655fd-e555-4977-8f6d-5fff1d3eeed5", "node_type": "1", "metadata": {}, "hash": "35d10a5c3adb5a47b08710c154eb56d77b73992eb3b3cac3d3ed050f055b6af1", "class_name": "RelatedNodeInfo"}}, "text": "Method Type Storage Memory Backprop Inference overhead\nAdapters (Houlsby et al., 2019) A yes yes no Extra FFN\nAdaMix (Wang et al., 2022) A yes yes no Extra FFN\nSparseAdapter (He et al., 2022b) AS yes yes no Extra FFN\nCross-Attn tuning (Gheini et al., 2021) S yes yes no No overhead\nBitFit (Ben-Zaken et al., 2021) S yes yes no No overhead\nDiffPruning (Guo et al., 2020) S yes no no No overhead\nFish-Mask (Sung et al., 2021) S yes maybe5no No overhead\nLT-SFT (Ansell et al., 2022) S yes maybe5no No overhead\nPrompt Tuning (Lester et al., 2021) A yes yes no Extra input\nPre\ufb01x-Tuning (Li and Liang, 2021) A yes yes no Extra input\nSpot (Vu et al., 2021) A yes yes no Extra input\nIPT (Qin et al., 2021) A yes yes no Extra FFN and input\nMAM Adapter (He et al., 2022a) A yes yes no Extra FFN and input\nParallel Adapter (He et al., 2022a) A yes yes no Extra FFN\nIntrinsinc SAID (Aghajanyan et al., 2020) R no no no No overhead\nLoRa (Hu et al., 2021) R yes yes no No overhead\nUniPELT (Mao et al., 2021) AR yes yes no Extra FFN and input\nCompacter (Karimi Mahabadi et al., 2021) AR yes yes no Extra FFN\nPHM Adapter (Karimi Mahabadi et al., 2021) AR yes yes no Extra FFN\nKronA (Edalati et al., 2022) R yes yes no No overhead\nKronAB\nres(Edalati et al., 2022) AR yes yes no Extra linear layer\n(IA)3(Liu et al., 2022) A yes yes no Extra gating\nAttention Fusion (Cao et al., 2022) A yes yes yes Extra decoder\nLeTS (Fu et al., 2021) A yes yes yes Extra FFN\nLadder Side-Tuning (Sung et al., 2022) A yes yes yes Extra decoder\nFAR (Vucetic et al., 2022) S yes maybe6no No overhead\nS4-model (Chen et al., 2023) ARS yes yes no Extra FFN and input\nTable 1: Comparing PEFT methods across storage ef\ufb01ciency, memory ef\ufb01ciency, and computational ef\ufb01ciency in\nterms of reducing backpropagation costs and having inference overhead. Method types: A\u2013 additive, S\u2013 selective,\nR\u2013 reparametrization-based.\n5 Overview of PEFT Approaches\nIn the next sections, we dive into the details of sev-\neral parameter-ef\ufb01cient \ufb01ne-tuning approaches.\nWe will describe the distinctions and tradeoffs be-\ntween them in terms of the axes outlined in Sec-\ntion 4. We bold a one-sentence summary of each\nmethod to simplify skimming through them.\nIn the method description, we also indicate\nwhether it has been applied to models with fewer\nthan 1 billion, fewer than 20 billion, or more than\n20 billion parameters. The model size summary\ncan be found in Table 2. We stick to the param-\neter counts indication where possible because the\nwords \u201csmall\u201d and \u201clarge\u201d change their meaning\ntoo quickly. Finally, we provide a brief pseudo-\nPyTorch implementation of the most important\npart of the algorithm where it\u2019s feasible.\n5Depends on sparse operations hardware support.\n6Depends on the weight pruning method.6 Additive methods: Adapters\nWe start our dive into PEFT methods with one of\nthe largest sub-families of methods that are built\non the idea of adding fully-connected networks be-\ntween model layers, i.e., adapters.\n6.1 Adapters\nThe idea of Adapters was initially developed for\nmulti-domain image classi\ufb01cation (Rebuf\ufb01 et al.,\n2017, 2018) and consisted in adding domain-\nspeci\ufb01c layers between neural network modules.\nHoulsby et al. (2019) adapt this idea to NLP.\nThey propose to add fully-connected networks\nafter attention and FFN layers in Transformer.", "start_char_idx": 0, "end_char_idx": 3308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef6655fd-e555-4977-8f6d-5fff1d3eeed5": {"__data__": {"id_": "ef6655fd-e555-4977-8f6d-5fff1d3eeed5", "embedding": null, "metadata": {"page_label": "5", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "997f9442-a39f-4369-95e1-f13ab24249c0", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "4dffffbe541b4c4adeae26dee9d690bbd9f2460235a79bc07758522cd0873069", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "869f8667-4260-41d3-9154-a8428ee7b3b4", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "b173c37b6ef407e030d77e204600864203539278318abf10fd784fd2166bb48c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2a1dcbe-c6a2-4a78-a389-5e9441efc45c", "node_type": "1", "metadata": {}, "hash": "1ca812f1279ba7714f1fcb53ee87773ee9edb440f5dcbea6742c9762124a3c0d", "class_name": "RelatedNodeInfo"}}, "text": "Finally, we provide a brief pseudo-\nPyTorch implementation of the most important\npart of the algorithm where it\u2019s feasible.\n5Depends on sparse operations hardware support.\n6Depends on the weight pruning method.6 Additive methods: Adapters\nWe start our dive into PEFT methods with one of\nthe largest sub-families of methods that are built\non the idea of adding fully-connected networks be-\ntween model layers, i.e., adapters.\n6.1 Adapters\nThe idea of Adapters was initially developed for\nmulti-domain image classi\ufb01cation (Rebuf\ufb01 et al.,\n2017, 2018) and consisted in adding domain-\nspeci\ufb01c layers between neural network modules.\nHoulsby et al. (2019) adapt this idea to NLP.\nThey propose to add fully-connected networks\nafter attention and FFN layers in Transformer.\nUnlike the transformer FFN block, Adapters usu-\nally have a smaller hidden dimension than the in-\nput. Adapters have demonstrated impressive pa-\nrameter ef\ufb01ciency at the time, showing that it is\npossible to achieve performance competitive to\nfull \ufb01ne-tuning by tuning less than 4% of the to-\ntal model parameters.\n5", "start_char_idx": 2544, "end_char_idx": 3624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2a1dcbe-c6a2-4a78-a389-5e9441efc45c": {"__data__": {"id_": "e2a1dcbe-c6a2-4a78-a389-5e9441efc45c", "embedding": null, "metadata": {"page_label": "6", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a4be734-6624-420b-8a46-bfaa870706ae", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "a2aee72e77e915e5454c71d5ceeeb852a099f0d3da853d588507ddbe9a5d5aeb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef6655fd-e555-4977-8f6d-5fff1d3eeed5", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "cfdac1f6b198b2b3f4ae1ddd61041cf6b18dc0efb3aacf2eed72cbb50dfa6fa5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06fa9fd3-6bb7-491b-b0e7-4d10cc4ece24", "node_type": "1", "metadata": {}, "hash": "2985eebb2dfe090914aab30e113eef74f2ccab4505255844568a067084cc0a60", "class_name": "RelatedNodeInfo"}}, "text": "Method% Trainable\nparameters% Changed\nparametersEvaluated on\n<1B <20B >20B\nAdapters (Houlsby et al., 2019) 0.1 - 6 0.1 - 6 yes yes yes\nAdaMix (Wang et al., 2022) 0.1 - 0.2 0.1 - 0.2 yes no no\nSparseAdapter (He et al., 2022b) 2.0 2.0 yes no no\nBitFit (Ben-Zaken et al., 2021) 0.05 - 0.1 0.05 - 0.1 yes yes yes\nDiffPruning (Guo et al., 2020) 200 0.5 yes no no\nFish-Mask (Sung et al., 2021) 0.01 - 0.5 0.01 - 0.5 yes yes no\nPrompt Tuning (Lester et al., 2021) 0.1 0.1 yes yes yes\nPre\ufb01x-Tuning (Li and Liang, 2021) 0.1 - 4.0 0.1 - 4.0 yes yes yes\nIPT (Qin et al., 2021) 56.0 56.0 yes no no\nMAM Adapter (He et al., 2022a) 0.5 0.5 yes no no\nParallel Adapter (He et al., 2022a) 0.5 0.5 yes no no\nIntrinsinc SAID (Aghajanyan et al., 2020) 0.001 - 0.1\u223c0.1 or 100 yes yes no\nLoRa (Hu et al., 2021) 0.01 - 0.5\u223c0.5 or\u223c30 yes yes yes\nUniPELT (Mao et al., 2021) 1.0 1.0 yes no no\nCompacter (Karimi Mahabadi et al., 2021) 0.05-0.07\u223c0.07 or\u223c0.1 yes yes no\nPHM Adapter (Karimi Mahabadi et al., 2021) 0.2\u223c0.2 or\u223c1.0 yes no no\nKronA (Edalati et al., 2022) 0.07\u223c0.07 or\u223c30.0 yes no no\nKronAB\nres(Edalati et al., 2022) 0.07\u223c0.07 or\u223c1.0 yes no no\n(IA)3(Liu et al., 2022) 0.02 0.02 no yes no\nLadder Side-Tuning(Sung et al., 2022) 7.5 7.5 yes yes no\nFAR (Vucetic et al., 2022) 6.6-26.4 6.6-26.4 yes no no\nS4-model (Chen et al., 2023) 0.5 more than 0.5 yes yes no\nTable 2: What model sizes PEFTmethods have been evaluated on and their typical amount of trainable parameters\nused in the papers. By trainable parameter count we speci\ufb01cally mean the number parameters that are updated by\na gradient optimization algorithm, not the delta between the original and \ufb01nal model which we denote \u201cchanged\nparameters\u201d. For reprametrization-based method we report parameters before and after reparametrization. Esti-\nmating updated parameter count for S4 is complicated as the model uses different methods at different layers. We\nreport the range at which the methods were evaluated in the published literature.\nA schematic adapter implementation:\ndef transformer_block_with_adapter(x):\nresidual = x\nx = SelfAttention(x)\nx = FFN(x) # adapter\nx = LN(x + residual)\nresidual = x\nx = FFN(x) # transformer FFN\nx = FFN(x) # adapter\nx = LN(x + residual)\nreturn x\nPfeiffer et al. (2020a) found that inserting the\nadapter only after the self-attention layer (after\nnormalization) achieves similar performance as\nusing two adapters per transformer block.\n6.2 AdaMix\nAdaMix (Wang et al., 2022) improves the perfor-\nmance of adapters by utilizing multiple adapters\nin a mixture-of-experts (MoE) fashion (Shazeer\net al., 2017).", "start_char_idx": 0, "end_char_idx": 2577, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06fa9fd3-6bb7-491b-b0e7-4d10cc4ece24": {"__data__": {"id_": "06fa9fd3-6bb7-491b-b0e7-4d10cc4ece24", "embedding": null, "metadata": {"page_label": "6", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a4be734-6624-420b-8a46-bfaa870706ae", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "a2aee72e77e915e5454c71d5ceeeb852a099f0d3da853d588507ddbe9a5d5aeb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2a1dcbe-c6a2-4a78-a389-5e9441efc45c", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "011cca552e3aefc2a3d72e287b77864743bc3962c6713b7e75d0383ffb81ff38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c21241b-f464-413d-9942-dfec22189f6c", "node_type": "1", "metadata": {}, "hash": "f1aee3d359e8e8db0fc1dfb502295288fb26d73ebf300ff7bf4284ce15ec9d52", "class_name": "RelatedNodeInfo"}}, "text": "We\nreport the range at which the methods were evaluated in the published literature.\nA schematic adapter implementation:\ndef transformer_block_with_adapter(x):\nresidual = x\nx = SelfAttention(x)\nx = FFN(x) # adapter\nx = LN(x + residual)\nresidual = x\nx = FFN(x) # transformer FFN\nx = FFN(x) # adapter\nx = LN(x + residual)\nreturn x\nPfeiffer et al. (2020a) found that inserting the\nadapter only after the self-attention layer (after\nnormalization) achieves similar performance as\nusing two adapters per transformer block.\n6.2 AdaMix\nAdaMix (Wang et al., 2022) improves the perfor-\nmance of adapters by utilizing multiple adapters\nin a mixture-of-experts (MoE) fashion (Shazeer\net al., 2017). Meaning, that each adapter layer is\na set of layers (experts), and for each forward pass\nonly a small set of experts is activated. In con-\ntrast to a regular MoE, which selects and weightsmultiple experts using a routing network AdaMix\nrandomly selects a single expert for each forward\npass. This minimizes computational costs and, ac-\ncording to Wang et al. (2022), doesn\u2019t degrade\nperformance. Another difference from a regu-\nlar MoE layer is that up and down projections\nof the adapter are selected independently. After\ntraining, the adapter weights are averaged across\nthe experts which makes inference more memory-\nef\ufb01cient. To stabilize training, the authors propose\nconsistency regularization which minimizes sym-\nmetrizedKLbetween two models\u2019 forwards with\ndifferent sets of experts selected.\nAlthough AdaMix achieves better performance\nthan regular adapters with the same inference cost,\nit can use more memory during training. Wang\net al. (2022) show that AdaMix can use much\nsmaller adapter hidden states, which amortizes\ntrainable parameter overhead over the number of\nexperts (\u223c4-8). However, the consistency regular-\nization technique increases computational require-\nments and memory consumption, as it needs to\n6", "start_char_idx": 1890, "end_char_idx": 3806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c21241b-f464-413d-9942-dfec22189f6c": {"__data__": {"id_": "1c21241b-f464-413d-9942-dfec22189f6c", "embedding": null, "metadata": {"page_label": "7", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c42cc958-8536-4c4f-a43b-4562d494ee68", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "d427060196d75259a75819e752ab48c88c52d96c776f2e4e87c5b1ebdaab8b42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06fa9fd3-6bb7-491b-b0e7-4d10cc4ece24", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "d78d703a8d362ff377c947694303fcb79866f246730f3ce206a411b5861a96b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d14c55cc-8bc3-4f55-8b33-71578bfcde0d", "node_type": "1", "metadata": {}, "hash": "1e5a534056e2b68dec18239b7a3a047a70d614f3ee5e7bf58ecbe87686d8edfc", "class_name": "RelatedNodeInfo"}}, "text": "keep two versions of the hidden states and gradi-\nents over two model forward passes with different\nexperts.\ndef transformer_block_with_adamix(x):\nresidual = x\nx = SelfAttention(x)\nx = LN(x + residual)\nresidual = x\nx = FFN(x)\n# adamix starts here\nx = random_choice(experts_up)(x)\nx = nonlinearity(x)\nx = random_choice(experts_down)(x)\nx = LN(x + residual)\nreturn x\ndef consistency_regularization(x):\nlogits1 = transformer_adamix(x)\n# second pass uses different experts\nlogits2 = transformer_adamix(x)\nr = symmetrized_KL(logits1, logits2)\nreturn r\n7 Additive Methods: Soft Prompts\nPrompting language models has demonstrated re-\nmarkable performance in zero- and few-shot sce-\nnarios (Brown et al., 2020; Schick and Sch\u00fctze,\n2021). However, optimizing discrete natural lan-\nguage prompts or using in-context learning is im-\npractical when there are many training examples.\nTo overcome this challenge, the concept of \u201csoft\u201d\nor \u201ccontinuous\u201d prompts was proposed (Li and\nLiang, 2021; Lester et al., 2021; Liu et al., 2021)\nthat converts the discrete optimization problem of\n\ufb01nding the best \"hard\" prompt to a continuous one.\n7.1 Prompt Tuning\nPrompt tuning (Lester et al., 2021) proposes to\nprepend the model input embeddings with a\ntrainable tensor P\u2208Rl\u00d7h. This tensor is com-\nmonly referred to as \u201csoft prompt\u201d and it is opti-\nmized directly through gradient descent.\ndef soft_prompted_model(input_ids):\nx = Embed(input_ids)\nx = concat([soft_prompt, x], dim=seq)\nreturn model(x)\nAblation studies by Su et al. (2021) over prompt\nlength from 1 to 150 tokens and model size from\n10M to 11B parameters reveal that prompt tuning\nis more parameter ef\ufb01cient the larger the model.\nFor instance, prompt tuning of T5-11B achieves\nthe same SuperGLUE (Wang et al., 2019) perfor-\nmance with either 5 or 150 soft prompt length.\nFurthermore, ef\ufb01ciency grows faster than the\nmodel size: T5-large performance saturates at\nprompt length 20 or 20K trainable parame-\nters ( 0.002% ), and T5-XL performance saturatesprompt length 5, also 20K trainable parameters\n(0.0002% ). However, prompt tuning only be-\ncomes comparable with full \ufb01ne-tuning at the 10B\nmodel scale. Also, increasing the length of the\ninput by 20-100 tokens can signi\ufb01cantly increase\ncomputation, given the quadratic complexity of\nthe transformer. Overall, soft prompts are incred-\nibly parameter-ef\ufb01cient at the cost of inference\noverhead and more applicable to larger models.\n7.2 Pre\ufb01x-Tuning\nLi and Liang (2021) independently develop the\nidea of soft prompts with a distinctive \ufb02avor: in-\nstead of adding a soft prompt to the model input,\ntrainable parameters are prepended to the hid-\nden states of all layers . The same pre\ufb01x P\u03b8\u2208\nRl\u00d7his prepended to all of the hidden states.\nThey observe that directly optimizing the soft\nprompt leads to instabilities during training. In-\nstead, soft prompts are parametrized through a\nfeed-forward network P\u03b8= FFN( \u02c6P\u03b8). During\ntraining, \u02c6P\u03b8and the parameters of the FFN are\noptimized. After, only P\u03b8is needed for inference,\nand the FFN can be discarded.\nPseudocode for a single layer:\ndef transformer_block_for_prefix_tuning(x):\nsoft_prompt = FFN(soft_prompt)\nx = concat([soft_prompt, x], dim=seq)\nreturn transformer_block(x)\nNote that the approach is very similar to\nPrompt-tuning (Section 7.1), but the soft prompts\nare added in each layer.\nIn their experiments, Li and Liang (2021) apply\nBART (Lewis et al., 2019) model (<1B) to differ-\nent generation tasks and show a performance close\nto the full \ufb01ne-tuning by training only 0.1%pa-\nrameters. Soft prompt lengths used in the study\nvary from 10 to 200 tokens.", "start_char_idx": 0, "end_char_idx": 3600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d14c55cc-8bc3-4f55-8b33-71578bfcde0d": {"__data__": {"id_": "d14c55cc-8bc3-4f55-8b33-71578bfcde0d", "embedding": null, "metadata": {"page_label": "7", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c42cc958-8536-4c4f-a43b-4562d494ee68", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "d427060196d75259a75819e752ab48c88c52d96c776f2e4e87c5b1ebdaab8b42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c21241b-f464-413d-9942-dfec22189f6c", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "421f8df779b1ee7f824b13b713611e8ae6320972d5545395eb62d0dae92abb59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4189545-9f9a-4ef1-98cc-0a81aec6ea93", "node_type": "1", "metadata": {}, "hash": "f6f30f31cc5fbfde068572bf98613c2f7c6f90de412c9f9c5019edfeefb70de0", "class_name": "RelatedNodeInfo"}}, "text": "During\ntraining, \u02c6P\u03b8and the parameters of the FFN are\noptimized. After, only P\u03b8is needed for inference,\nand the FFN can be discarded.\nPseudocode for a single layer:\ndef transformer_block_for_prefix_tuning(x):\nsoft_prompt = FFN(soft_prompt)\nx = concat([soft_prompt, x], dim=seq)\nreturn transformer_block(x)\nNote that the approach is very similar to\nPrompt-tuning (Section 7.1), but the soft prompts\nare added in each layer.\nIn their experiments, Li and Liang (2021) apply\nBART (Lewis et al., 2019) model (<1B) to differ-\nent generation tasks and show a performance close\nto the full \ufb01ne-tuning by training only 0.1%pa-\nrameters. Soft prompt lengths used in the study\nvary from 10 to 200 tokens.\n7.3 Intrinsic Prompt Tuning (IPT)\nPrompt tuning methods, despite being parameter\nef\ufb01cient, present practical problems such as slow\nconvergence and a need to store all of the task-\nspeci\ufb01c parameters. A few studies (Su et al., 2021;\nVu et al., 2021) proposed to pre-train soft prompts\nto improve performance and convergence speed.\nHowever, these methods do not provide solutions\nto reduce the number of parameters per task.\nQin et al. (2021) hypothesize that the h-\ndimensional space used to de\ufb01ne soft prompt pa-\nrameters contains an \u201cintrinsic task subspace\u201d that\ncan differentiate between various tasks.\n7", "start_char_idx": 2907, "end_char_idx": 4208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4189545-9f9a-4ef1-98cc-0a81aec6ea93": {"__data__": {"id_": "d4189545-9f9a-4ef1-98cc-0a81aec6ea93", "embedding": null, "metadata": {"page_label": "8", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "562f55cb-358a-4448-8f15-73d39df4745d", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "d34d5393906b4f5be0ded366f0e56c2e8c80c20e9493d5afa508c3fd8ae6006a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d14c55cc-8bc3-4f55-8b33-71578bfcde0d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "3925e43fe9b8917be9572bc7fd32fe74156a8d201fcccd3a33f04a80e245bb99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b84764ee-9a70-4440-a2b0-d61a221a5e13", "node_type": "1", "metadata": {}, "hash": "413947e251d44480390fe060937c5138609c5cf8f1ac975cbf7e86b7e14c9d50", "class_name": "RelatedNodeInfo"}}, "text": "IPT method works in three steps. First, given\na set of training tasks, their soft prompts are\nlearned in a regular way (Section 7.1). Then,\nthese prompts are used to train an autoencoder\nthat compresses their dimensionality. After this,\nthe encoder part is discarded, and only the in-\nput to the autoencoder decoder is trained on new\ntasks. In summary, IPT uses an autoencoder to\n(de)compress the soft prompt .\ndef autoencoder(soft_prompt):\nsoft_prompt = soft_prompt.flatten()\nP = FFN_A(soft_prompt) # encoder\nP = FFN_B(P) # decoder\nP = P.reshape(prompt_len, hidden)\nreturn P\ndef soft_prompted_model(x):\nP = FFN_B(intrinsic_subspace_prompt)\nP = P.reshape(prompt_len, hidden)\nx = concat([P, x], dim=seq)\nreturn model(x)\nEven though the IPT framework reduces the\nnumber of parameters for the unseen tasks, this\nreduction comes at the price of training the au-\ntoencoder. The authors conduct experiments with\nthe BART-base model and a prompt length of\n100. The resulting autoencoder, which is imple-\nmented4as a fully-connected network that accepts\na one-dimensional tensor of size 768\u00b7100reaches\napproximately 78 million parameters. I.e., over\n56% of total parameters in the BART-base model.\nHence, signi\ufb01cantly more ef\ufb01cient ways of prompt\nautoencoding are required to make IPT practically\napplicable.\n8 Additive Methods: Other Approaches\nSeveral of the additive PEFT methods do not fol-\nlow the idea of either adapters or soft prompts and\npropose to augment a pre-trained network in an\noriginal way.\n8.1 Ladder-Side Tuning (LST)\nLadder-Side Tuning (Sung et al., 2022) trains a\nsmall transformer network on the side of the\npre-trained network . This side network com-\nbines the hidden states of the pre-trained backbone\nnetwork with its own hidden states.\nThis way, the side network only uses the pre-\ntrained model as a feature extractor, and backprop-\nagation must only be computed through the side\nnetwork saving on both memory and compute dur-\ning training. The authors also use multiple tricks\n4github.com/thunlp/Intrinsic-Prompt-Tuningno improve the performance and parameter ef\ufb01-\nciency of LST. Namely, initializing the side net-\nwork from the pre-trained model parameters using\nstructural pruning and using twice (or 4x) fewer\nlayers in the side network than in the backbone\nnetwork.\nThe pseudocode:\ndef ladder_side_layer(x, h_pt):\nh_pt = h_pt @ W_down # to x.shape\ngate = sigmoid(alpha)\nx = gate *x + (1 - gate) *h_pt\nreturn transformer_block(x)\ndef ladder_side_network(x):\nwith no_grad():\nH_pt = pretrained_network(\nx, return_all_hiddens=True\n)\nfor i in range(layers):\nlayer = ladder_side_layers[i]\nx = layer(x, H_pt[i])\nreturn x\nWhere h_pt is the output of the corresponding\nlayer of the pre-trained network, and alpha is an\ninput-independent trainable scalar gate.\nLST demonstrated a three-fold RAM reduc-\ntion in \ufb01ne-tuning T5-Base compared to full \ufb01ne-\ntuning and a two-fold RAM usage reduction com-\npared to LoRa (Section 10.2) with a small degra-\ndation in accuracy and outperforms these methods\nwhen controlling for RAM usage.\n8.2 (IA)3\nLiu et al. (2022) propose a new parameter-ef\ufb01cient\nmethod to multi-task \ufb01ne-tune T-few. (IA)3learns\nnew parameters lv,lk,lffwhich rescale key,\nvalue, and hidden FFN activations .", "start_char_idx": 0, "end_char_idx": 3231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b84764ee-9a70-4440-a2b0-d61a221a5e13": {"__data__": {"id_": "b84764ee-9a70-4440-a2b0-d61a221a5e13", "embedding": null, "metadata": {"page_label": "8", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "562f55cb-358a-4448-8f15-73d39df4745d", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "d34d5393906b4f5be0ded366f0e56c2e8c80c20e9493d5afa508c3fd8ae6006a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4189545-9f9a-4ef1-98cc-0a81aec6ea93", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "2ac79d75d1c8c76790bfc3877c449bcbaad7a1c001e095fe2af12c1388f2dab8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a0f0850-85f3-439f-84ce-f190377921d1", "node_type": "1", "metadata": {}, "hash": "af92c1606eaf2e9bfb38859d2b98c67e6d008f7fcaef542822fef2948981f7f5", "class_name": "RelatedNodeInfo"}}, "text": "LST demonstrated a three-fold RAM reduc-\ntion in \ufb01ne-tuning T5-Base compared to full \ufb01ne-\ntuning and a two-fold RAM usage reduction com-\npared to LoRa (Section 10.2) with a small degra-\ndation in accuracy and outperforms these methods\nwhen controlling for RAM usage.\n8.2 (IA)3\nLiu et al. (2022) propose a new parameter-ef\ufb01cient\nmethod to multi-task \ufb01ne-tune T-few. (IA)3learns\nnew parameters lv,lk,lffwhich rescale key,\nvalue, and hidden FFN activations . Speci\ufb01cally,\ndef transformer_block_with_ia3(x):\nresidual = x\nx = ia3_self_attention(x)\nx = LN(x + residual)\nresidual = x\nx = x @ W_1 # FFN in\nx = l_ff *gelu(x) # (IA)3 scaling\nx = x @ W_2 # FFN out\nx = LN(x + residual)\nreturn x\ndef ia3_self_attention(x):\nk, q, v = x @ W_k, x @ W_q, x @ W_v\nk = l_k *k\nv = l_v *v\nreturn softmax(q @ k.T) @ V\nTraining only these three vectors lv,lk,lff\nfor each transformer block leads to high param-\neter ef\ufb01ciency. For T0-3B, it only updates about\n0.02% of model parameters and outperforms other\nmethods, including Compacter (Section 11.4) with\nsimilar parameter count and LoRa (Section 10.2)\n8", "start_char_idx": 2776, "end_char_idx": 3860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a0f0850-85f3-439f-84ce-f190377921d1": {"__data__": {"id_": "2a0f0850-85f3-439f-84ce-f190377921d1", "embedding": null, "metadata": {"page_label": "9", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd786b2d-8393-4dd5-b5c7-7e07ab90747b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "797d4d32a4be41fc6ca58a931a1ed3ef0652a2f0e703a385def1abeea03996da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b84764ee-9a70-4440-a2b0-d61a221a5e13", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "3ab3cdeeebd9bbe8b0ac3ea97d94118be65f56bb5ddd93b554973babf683e162", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dbeae1b-299d-4f5f-a1b3-f0d4f879a423", "node_type": "1", "metadata": {}, "hash": "a0eff1702dec08a3430dc1035bbfb64909babf1a8a87dd631cde2df3d8995f10", "class_name": "RelatedNodeInfo"}}, "text": "with 16 times more trainable parameters. Un-\nlike adapters-tuned models, (IA)3-tuned models\nexhibit minimal overhead. Vectors lvandlkcan\nbe integrated into the corresponding linear layers,\nand the only overhead comes from lff.\n9 Selective Methods\nSelective methods \ufb01ne-tune a subset of the exist-\ning parameters of the model. It could be a layer\ndepth-based selection, layer type-based lection, or\neven individual parameter selection.\n9.1 BitFit\nBen-Zaken et al. (2021) propose to only \ufb01ne-tune\nthe biases of the network . That is, for every lin-\near or convolutional layer, the weight matrix Wis\nleft as is, and only the bias vector bis optimized.\nparams = (p for n, p\nin model.named_parameters()\nif \"bias\" in n)\noptimizer = Optimizer(params)\nBitFit only updates about 0.05% of the model\nparameters. The original paper demonstrated\nthat the method achieves similar performance to\nfull \ufb01ne-tuning or better performance in low and\nmedium-data scenarios in BERT models (<1B pa-\nrameters). Further research evaluated the method\non larger networks such as T0-3B (Sanh et al.,\n2022; Liu et al., 2022) or GPT-3 (Hu et al., 2021).\nAt this scale, BitFit signi\ufb01cantly underperforms\nfull \ufb01ne-tuning, and other PEFT approaches.\n9.2 DiffPruning\nDiffPruning (Guo et al., 2020) aims to achieve pa-\nrameter ef\ufb01ciency by learning a sparse update of a\nneural network\u2019s weights. The method introduces\na learnable binary mask on the weights, denoted\nby\u03b4=z\u25e6\u2206W, where\u25e6represents the Hadamard\nproduct. This parameter mask is learned during\nmodel \ufb01ne-tuning as part of the regularization ob-\njective, which is a differentiable approximation to\ntheL0norm of the update vector \u03b4.\nDiffPruning has achieved comparable perfor-\nmance to full \ufb01ne-tuning while modifying only\n0.5% of the model parameters in smaller-scale\n(<1B) scenarios. This makes it a useful method for\nmulti-task deployment for edge (mobile) scenarios\nwhere storage is limited. However, this method re-\nquires more memory than traditional \ufb01ne-tuning,\nas it involves optimizing all parameters during\ntraining in addition to the learnable binary mask.9.3 Freeze and Recon\ufb01gure (FAR)\nFAR (Vucetic et al., 2022) selects columns of\nparameter matrices to prune and recon\ufb01gures\nlinear layers into trainable and frozen . The\nmethod operates in two stages. In the \ufb01rst stage,\nthe most important rows of parameter matrices\nare identi\ufb01ed for updating. This process is simi-\nlar to structured pruning and can use any pruning\nmethod. In their paper, the authors \ufb01ne-tune the\nmodel on a percentage of the data and select the\ntop-rrows based on the L1distance between the\n\ufb01ne-tuned and original models.\nIn the second stage, the network is recon\ufb01gured\nby splitting each parameter W\u2208Rin\u00d7hinto a\ntrainable component Wt\u2208Rin\u00d7h\u2032and a frozen\ncomponentWf\u2208Rin\u00d7(h\u2212h\u2032), whereh\u2032is the de-\nsired number of trainable parameters. The matrix\nmultiplications with WtandWfare computed in-\ndependently, and the results are concatenated. A\nsimilar operation is performed on biases.\nPseudocode implementation is rather simple\ndef far_layer(x):\nh1 = x @ W_t\nh2 = x @ W_f\nreturn concat([h1, h2], dim=-1)\nWhile this approach creates additional compute\noverhead during training, it provides great \ufb02ex-\nibility in terms of parameter selection on mod-\nern hardware using standard frameworks like Py-\nTorch. After training, the parameters can be recon-\n\ufb01gured back removing any inference overhead.\nThe original paper focused on edge scenarios\nand used DistilBERT (66M) for their experiments.\nFAR was only applied to feed-forward layers, as\nthese make up the majority of the DistilBERT pa-\nrameters.", "start_char_idx": 0, "end_char_idx": 3595, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dbeae1b-299d-4f5f-a1b3-f0d4f879a423": {"__data__": {"id_": "3dbeae1b-299d-4f5f-a1b3-f0d4f879a423", "embedding": null, "metadata": {"page_label": "9", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd786b2d-8393-4dd5-b5c7-7e07ab90747b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "797d4d32a4be41fc6ca58a931a1ed3ef0652a2f0e703a385def1abeea03996da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a0f0850-85f3-439f-84ce-f190377921d1", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "01c7c3b7031a73002b4dc728e84757db3998f9a7f81cad589c6349ce1145a858", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b251110-29d9-4bfb-9962-1398169cae0b", "node_type": "1", "metadata": {}, "hash": "55b0a9d8e3e1da409fedbcae879f22642df27c42972ddce28c47b09b501cd44d", "class_name": "RelatedNodeInfo"}}, "text": "The matrix\nmultiplications with WtandWfare computed in-\ndependently, and the results are concatenated. A\nsimilar operation is performed on biases.\nPseudocode implementation is rather simple\ndef far_layer(x):\nh1 = x @ W_t\nh2 = x @ W_f\nreturn concat([h1, h2], dim=-1)\nWhile this approach creates additional compute\noverhead during training, it provides great \ufb02ex-\nibility in terms of parameter selection on mod-\nern hardware using standard frameworks like Py-\nTorch. After training, the parameters can be recon-\n\ufb01gured back removing any inference overhead.\nThe original paper focused on edge scenarios\nand used DistilBERT (66M) for their experiments.\nFAR was only applied to feed-forward layers, as\nthese make up the majority of the DistilBERT pa-\nrameters. The authors showed that FAR achieved\nsimilar performance to \ufb01ne-tuning on \ufb01ve GLUE\ntasks and SQuAD 2.0 (Rajpurkar et al., 2018)\nwhile updating 6% of the parameters.\n9.4 FishMask\nFishMask (Sung et al., 2021) is a sparse \ufb01ne-\ntuning method that selects top-p parameters of\nthe model based on their Fisher information .\nFisher information is estimated in a common way\nthrough a diagonal approximation\n\u02c6F\u03b8=1\nNN\u2211\ni=1Ey\u223cp\u03b8(y|xi)(\u2207\u03b8logp\u03b8(y|xi))2\nThus, the method requires computing gradients\nfor all parameters on (several batches of) the data.\n9", "start_char_idx": 2840, "end_char_idx": 4135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b251110-29d9-4bfb-9962-1398169cae0b": {"__data__": {"id_": "1b251110-29d9-4bfb-9962-1398169cae0b", "embedding": null, "metadata": {"page_label": "10", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba68a9fb-d9f3-4cab-9b39-2107974a3240", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "e38970e9f69c224bd8a9215c4ffc6bcb530b0b05730db4505b2fa951e3e0b112", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dbeae1b-299d-4f5f-a1b3-f0d4f879a423", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "caeb8398650aa21414dbb6411a1bd6b3d31f02c40bf7dba5b7fa5504a9e41f83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e24fd0b9-8aec-4fa5-b5cb-2706b417a90f", "node_type": "1", "metadata": {}, "hash": "a0f1dbf3cf42e0cfa7ad3b86836e42cba1e3ddb68907550d24dabafad4c197e4", "class_name": "RelatedNodeInfo"}}, "text": "However, after the highest-Fisher parameters are\nselected, only they need to be optimized.\nPseudocode to compute the masks:\nsparsity = 0.99\nN = len(data)\nfor x, y in data:\nloss = loss_fn(model(x), y)\nloss.backward()\nfor n, p in model.named_params():\nfisher[n] += p.grad **2 / N\nthreshold = percentile(fisher, sparsity)\nmasks = {n: f > threshold\nfor n, f in fisher.items()}\nThe method generally performs on-par with\nadapters but sub-par to LoRa and (IA)3(Sections\n10.2 and 8.2). It has been evaluated on BERT\n(<1B) and T0-3B models. However, FishMask is\ncomputationally intensive and inef\ufb01cient on con-\ntemporary deep learning hardware due to the lack\nof support for sparse operations.\n10 Reparametrization-based methods\nThese methods use the idea of reparametrizing the\nweights of the network using a low-rank transfor-\nmation. This decreases the trainable parameter\ncount while still allowing the method to work with\nhigh-dimensional matrices, such as the pre-trained\nparameters of the networks.\n10.1 Intrinsic SAID\nIn their work, Aghajanyan et al. (2020) inves-\ntigate the intrinsic dimensionality of \ufb01ne-tuning\nand demonstrate that this process can be per-\nformed in a low-rank subspace. Speci\ufb01cally, they\nuse the Fastfood transform to reparametrize\nthe update to the model weights . Fastfood\nis a compute-ef\ufb01cient dimensionality expansion\ntransformF:Rd\u2192RDthat can be done\ninO(Dlogd)time andO(D)memory.\nTheir results indicate that larger models require\nchanges in a lower-rank subspace compared to\nsmaller models to achieve the same \ufb01ne-tuning\nperformance. This observation motivates both\nscaling large models and parameter-ef\ufb01cient \ufb01ne-\ntuning. It is important to note that, unlike meth-\nods that select a particular subset of parameters\nfor \ufb01ne-tuning, Intrinsic SAID updates all model\nparameters in a low-rank manner, i.e., \u03b8=\u03b80+\nF(\u03b8d), where\u03b80\u2208RDdenotes the pre-trained\nmodel parameters and \u03b8d\u2208Rddenotes the pa-\nrameters to be optimized. Therefore, while the\nnumber of optimizable parameters is low, the\nO(D)memory complexity of Fastfood and theupdate to all of the model\u2019s parameters make In-\ntrinsic SAID impractical for \ufb01ne-tuning large net-\nworks. For more details on Fastfood, we refer the\nreader to the original paper by Le et al. (2013).\n10.2 LoRa\nLoRa (Hu et al., 2021) takes inspiration from In-\ntrinsicSAID and proposes a simpler way to per-\nform low-rank \ufb01ne-tuning. Parameter update for\na weight matrix in LoRa is decomposed into a\nproduct of two low-rank matricies\n\u03b4W=WAWB\nWA\u2208Rin\u00d7r,WB\u2208Rr\u00d7out(1)\nAll pre-trained model parameters are kept\nfrozen, and only WAandWBmatrices are train-\nable. The scaling factor is constant and typically\nequals1\nr. After training, they can be integrated\ninto the original Wby just adding the matrix\nWAWBto the original matrix W.\nPseudocode is very simple:\ndef lora_linear(x):\nh = x @ W # regular linear\nh += x @ W_A @ W_B # low-rank update\nreturn scale *h\nIn Transformers, LoRa is typically used for WK\nandWVprojection matrices in multi-head atten-\ntion modules. The method outperforms BitFit and\nAdapters and has been evaluated on the models up\nto 175B parameters.\n10.3 KronA\nKronA (Edalati et al., 2022) replaces matrix fac-\ntorization\u03b4W=WAWBin LoRa (Section 10.2)\nwith a matrix factorization through a Kro-\nnecker product \u03b4W=WA\u2297WB. This yields\na better rank per parameters tradeoff because the\nKronecker product keeps the rank of the original\nmatrices being multiplied. Or, in other words,\nrank(A\u2297B) = rankA\u00b7rankB. Additionally,\nEdalati et al.", "start_char_idx": 0, "end_char_idx": 3494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e24fd0b9-8aec-4fa5-b5cb-2706b417a90f": {"__data__": {"id_": "e24fd0b9-8aec-4fa5-b5cb-2706b417a90f", "embedding": null, "metadata": {"page_label": "10", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba68a9fb-d9f3-4cab-9b39-2107974a3240", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "e38970e9f69c224bd8a9215c4ffc6bcb530b0b05730db4505b2fa951e3e0b112", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b251110-29d9-4bfb-9962-1398169cae0b", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "274a18597750e796f4527929cc4770fe282fa445cd04907ac6f6be8f7591a99f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "357e8147-4dbe-4fd0-a197-20df732bec23", "node_type": "1", "metadata": {}, "hash": "ee14d9a06790689e0de5d598ebbf1f8863eb94699802465387f9b6815f7d599b", "class_name": "RelatedNodeInfo"}}, "text": "The method outperforms BitFit and\nAdapters and has been evaluated on the models up\nto 175B parameters.\n10.3 KronA\nKronA (Edalati et al., 2022) replaces matrix fac-\ntorization\u03b4W=WAWBin LoRa (Section 10.2)\nwith a matrix factorization through a Kro-\nnecker product \u03b4W=WA\u2297WB. This yields\na better rank per parameters tradeoff because the\nKronecker product keeps the rank of the original\nmatrices being multiplied. Or, in other words,\nrank(A\u2297B) = rankA\u00b7rankB. Additionally,\nEdalati et al. (2022) propose to use ef\ufb01cient Kro-\nnecker product-vector product operation x(A\u2297B)\nwhich avoids representing \u03b4Wexplicitly and leads\nto signi\ufb01cant speedups\nKrona pseudocode:\ndef krona_linear(x):\nx = x @ W # regular linear\nx += kronecker_vector_prod(x, W_A, W_B)\nreturn scale *x\n# same as x @ kronecker_product(A, B)\ndef kronecker_vector_prod(x, A, B):\nx = x.reshape(A.shape[1], B.shape[1])\nx = A.T @ x @ B\nreturn x.reshape(-1)\n10", "start_char_idx": 3011, "end_char_idx": 3923, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "357e8147-4dbe-4fd0-a197-20df732bec23": {"__data__": {"id_": "357e8147-4dbe-4fd0-a197-20df732bec23", "embedding": null, "metadata": {"page_label": "11", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae59d4a4-b6ff-4652-9261-0aba6de6a794", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "1534c8364ee714b0aaa75c977fd6aa2ba93aa375da4c4440850d1e2166159207", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e24fd0b9-8aec-4fa5-b5cb-2706b417a90f", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "9377c1d76d9abf072c80e6577c818e08f8590233d8574cd4ed0d6a8d39904e3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbd78736-83ee-4a0f-8123-4b3066399ee0", "node_type": "1", "metadata": {}, "hash": "7843b2523d9805d3d76786f2743800c448be40d21416a3363f264a2b62259695", "class_name": "RelatedNodeInfo"}}, "text": "KronAB\nres is another method presented in\nEdalati et al. (2022). It is a parallel adapter with\nKronecker product-parametrization of the weights\nand a residual connection.\nOn GLUE, KronA methods perform on-par or\nbetter than adapters (Section 6.1), LoRa (Section\n10.2), and Compacter (Section 11.4) at the same\ntrainable parameter count 0.07% while being sig-\nni\ufb01cantly faster than adapters or Compacter at in-\nference time. The method was evaluated only on\nsmall (<1B) models.\nBackground: What is a Kronecker product?\nKronecker product is a tensor operation de\ufb01ned as\nA\u2297B:Rn\u00d7m\u00d7Rk\u00d7l\u2192Rnk\u00d7ml\nA\u2297B=\uf8ee\n\uf8ef\uf8f0a1,1B\u00b7\u00b7\u00b7a1,nB\n.........\nam,1B\u00b7\u00b7\u00b7am,nB\uf8f9\n\uf8fa\uf8fb(2)\nIt can be easily implemented5in PyTorch using\nthe command torch.einsum\ndef batched_kronecker_product(a, b):\nbs, i, j = a.shape\nbs, k, m = b.shape\nres = einsum(\"bij,bkm->bikjm\", a, b)\nreturn res.view(bs, i *k, j *m)\n11 Hybrid Approaches\nHybrid methods for parameter-ef\ufb01cient \ufb01ne-\ntuning combine different techniques and strate-\ngies to achieve better performance while reduc-\ning the computational costs associated with \ufb01ne-\ntuning large neural networks. These methods can\nbe viewed as a synergistic blend of multiple ap-\nproaches. The resulting hybrid methods can lever-\nage the strengths of each individual technique,\nwhile mitigating their weaknesses, leading to im-\nproved performance and ef\ufb01ciency.\n11.1 SparseAdapter\nHe et al. (2022b) propose Large-Sparse strategy\nto train adapter layers. In this strategy, they use\nalarge hidden dimension for the added module\nand prune around 40% of the values at initial-\nization .Large-Sparse consistently outperforms its\nnon-sparse counterpart with the same trainable pa-\nrameter count. However, training and inference\ncosts can be higher depending on hardware sup-\nport for sparse tensors and operations. It is also\nworth noting that computing the pruning mask for\n5Source: github.com/rabeehk/compacterthis method may require obtaining gradients for all\nnewly added parameters.\n11.2 MAM Adapters\nIn their study, He et al. (2022a) conducted a thor-\nough investigation of adapter placement and soft\nprompts. They concluded that scaled parallel\nadapters outperform sequentially-placed adapters\nand that placing an adapter in parallel to FFN out-\nperforms multi-head attention-parallel adapters.\n6They also notice that soft prompts can ef\ufb01-\nciently modify attentions by only changing 0.1%\nof the parameters and propose to \u201cmix-and-match\u201d\n(MAM) these ideas. Their \ufb01nal model, MAM\nAdapter is a combination of scaled parallel\nadapter for FFN layer and soft prompt .\ndef transformer_block_mam(x):\nx = concat([x, soft_prompt], dim=seq)\nresidual = x\nx = SelfAttention(x)\nx = LN(x + residual)\nx_a = FFN(x) # parallel adapter\nx_a = scale *x_a\nx = LN(x + x_adapter)\nreturn x\nMAM method outperforms BitFit and PromptTun-\ning by a large margin and consistently outperforms\nLoRa (Section 10.2), Adapters (Section 6.1), and\nPre\ufb01x Tuning (Section 7.2) with 200 soft prompt\nlength at 7% extra parameters. The experiments\nwere concluded on <1B models.\nIt\u2019s worth noting that parallel adapters were in-\ndependently studied by Zhu et al. (2021) in the do-\nmain of machine translation.\n11.3 UniPELT\nUniPELT (Mao et al., 2021) is a gated combi-\nnation of LoRa, Pre\ufb01x-tuning, and Adapters .\nMore speci\ufb01cally, LoRa reparametrization is used\nforWQandWVattention matrices, pre\ufb01x-tuning\nis applied to keys and values of each layer, and\nadapters are added after the feed-forward layer of\nthe transformer block.", "start_char_idx": 0, "end_char_idx": 3467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbd78736-83ee-4a0f-8123-4b3066399ee0": {"__data__": {"id_": "fbd78736-83ee-4a0f-8123-4b3066399ee0", "embedding": null, "metadata": {"page_label": "11", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae59d4a4-b6ff-4652-9261-0aba6de6a794", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "1534c8364ee714b0aaa75c977fd6aa2ba93aa375da4c4440850d1e2166159207", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "357e8147-4dbe-4fd0-a197-20df732bec23", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "a7c84bf0bf1fa325a4dc972a365731cb4ed1697d043574e7c735eecb724e516f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bec28a5-2384-4702-b391-0c209de2c43b", "node_type": "1", "metadata": {}, "hash": "b76b9688870586ced886d22702f073aef6138ac018a6431376b332072caa59ae", "class_name": "RelatedNodeInfo"}}, "text": "The experiments\nwere concluded on <1B models.\nIt\u2019s worth noting that parallel adapters were in-\ndependently studied by Zhu et al. (2021) in the do-\nmain of machine translation.\n11.3 UniPELT\nUniPELT (Mao et al., 2021) is a gated combi-\nnation of LoRa, Pre\ufb01x-tuning, and Adapters .\nMore speci\ufb01cally, LoRa reparametrization is used\nforWQandWVattention matrices, pre\ufb01x-tuning\nis applied to keys and values of each layer, and\nadapters are added after the feed-forward layer of\nthe transformer block. For each of the modules,\ngating is implemented as a linear layer projecting\nthe module input into a dimension of size one, sig-\nmoid activation, and averaging the resulting vector\nover the sequence length. Trainable parameters in-\nclude LoRa matrices WA,WB, prompt tuning pa-\nrametersPq,Pk, adapter parameters, and gating\nfunction weights.\n6While at \ufb01rst this might look contradicting the \ufb01nding of\nPfeiffer et al. (2020a), it actually supports it because the FFN-\nparallel adapter modi\ufb01es the outputs of attention, just like the\nMHA-sequential adapter.\n11", "start_char_idx": 2973, "end_char_idx": 4024, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bec28a5-2384-4702-b391-0c209de2c43b": {"__data__": {"id_": "9bec28a5-2384-4702-b391-0c209de2c43b", "embedding": null, "metadata": {"page_label": "12", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd852b53-a1b3-49b8-8721-705d90fc3e96", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "048a83963cbc9b57d0716c8761758d98c6371dfd0444eb7e7e8d355ed6c1edd6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbd78736-83ee-4a0f-8123-4b3066399ee0", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "53a1439417b8a6f7cc86293f9de13d7ef36e83db244f33ccf3846eb7325786a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71c2f4d4-9f8b-404a-9473-aae3dcf0894a", "node_type": "1", "metadata": {}, "hash": "81a6ac0f622a4ccd3d922d02940abbb9473fe18a169d56d24c32bcf3eb4c2feb", "class_name": "RelatedNodeInfo"}}, "text": "Next, we present a schematic implementation\nof UniPELT. We omit multiple attention heads for\nsimplicity.\ndef transformer_block_with_unipelt(x):\nresidual = x\nx = unipelt_self_attention(x)\nx = LN(x + residual)\nresidual = x\nx = FFN(x)\nadapter_gate = gate(x)\nx = adapter_gate *FFN(x)\nx = LN(x + residual)\nreturn x\ndef unipelt_self_attention(x):\nk, q, v = x @ W_k, x @ W_q, x @ W_v\n# lora for queries and values\nlora_gate = gate(x)\nq += lora_gate *W_qA @ W_aB\nv += lora_gate *W_vA @ W_vB\n# prefix tuning\npt_gate = gate(x)\nq_prefix = pt_gate *P_q\nk_prefix = pt_gate *P_k\nreturn softmax(q @ k.T) @ V\ndef gate(x):\nx = Linear(x)\nx = sigmoid(x)\nreturn mean(x, dim=seq)\nUniPELT demonstrates signi\ufb01cant improve-\nments over individual LoRa, Adapters, and Pre-\n\ufb01x Tuning approaches in low-data scenarios with\nonly 100 examples. In higher data scenarios,\nUniPELT performs on par or better than these ap-\nproaches. Mao et al. (2021) reports 1.3% trainable\nmodel parameters using UniPELT on BERT (<1B\nparameters) models.\n11.4 Compacter\nCompacter (Karimi Mahabadi et al., 2021) uti-\nlizes Kronecker product, low-rank matrices,\nand parameter sharing across layers to produce\nadapter weights . Each parameter Win an adapter\nis equal to a sum of Kronecker products\n\u02c6W=n\u2211\ni=0Ai\u2297Bi\n\u02c6W\u2208Rk\u00d7d, Ai\u2208Rn\u00d7n, Bi\u2208Rk\nn\u00d7d\nn.(3)\nA linear layer x\u02c6W+bwith this parametrization\nis called parametrized hypercomplex multiplica-\ntion (PHM) layer (Zhang et al., 2021). Compacter\ntakes this idea futher, parametrizing Bisimilar to\nLoRa (Section 10.2) Bi=Bdown\niBup\niwhere all\nmatricies are of rank at most r. MatricesAiare\nshared across all adapter layers for further param-\neter ef\ufb01ciency. The corresponding layer is namedLow-rank PHM or LPHM. Compacter layer pseu-\ndocode:\ndef compacter(x):\nx = LPHM(x) # Essentially an FFN\nx = gelu(x) # but\nx = LPHM(x) # LPHM replaces linear\nreturn x\ndef lphm_forward(x):\nB = B_d @ B_u\nW = batched_kronecker_product(A, B)\nW = sum(W, dim=0)\nreturn x @ W + b\nNote that all AandBare 3D tensors with the\n\ufb01rst dimension equal to n, the number of Kro-\nnecker products in the PHM layer.\nCompacter comes in two \ufb02avors: two adapters\nper transformer block or a single adapter after\na feedforward layer (Compacter++). With only\n0.05% additional parameters Compacter++ per-\nforms on par or better than adapters with 0.8% ad-\nditional parameters. The model has been evaluated\non T5 Base (<1B) and T0-3B.\n11.5 S4\nChen et al. (2023) conduct an extensive explo-\nration of various combinations of parameter-\nef\ufb01cient \ufb01ne-tuning techniques . Their search\nspace includes dividing consecutive layers into\nfour uneven groups, allocating varying amounts\nof trainable parameters to each layer, determin-\ning which groups to \ufb01ne-tune, and deciding which\nPEFT methods to apply to each group.\nTheir proposed method, S4, divides layers into\nfour groups ( G1,2,3,4) using a \u201cspindle\u201d pattern:\nmore layers are allocated to the middle groups and\nfewer to the top and bottom groups. All groups\nare trainable, with trainable parameters uniformly\nallocated across the layers (not groups). Different\ncombinations of PEFT methods are applied to dif-\nferent groups.", "start_char_idx": 0, "end_char_idx": 3122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71c2f4d4-9f8b-404a-9473-aae3dcf0894a": {"__data__": {"id_": "71c2f4d4-9f8b-404a-9473-aae3dcf0894a", "embedding": null, "metadata": {"page_label": "12", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd852b53-a1b3-49b8-8721-705d90fc3e96", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "048a83963cbc9b57d0716c8761758d98c6371dfd0444eb7e7e8d355ed6c1edd6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bec28a5-2384-4702-b391-0c209de2c43b", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7bca6ac65e4224502cc785b98628ed3e78aadfd54d24b7ba0fcc66a42fe77c57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69a130fa-e299-4c24-8003-83a85da8ed33", "node_type": "1", "metadata": {}, "hash": "ddc49b4744ae29362b86a01994cbbf0417ccb7c1dd6b035df86ab73ea8a5006d", "class_name": "RelatedNodeInfo"}}, "text": "The model has been evaluated\non T5 Base (<1B) and T0-3B.\n11.5 S4\nChen et al. (2023) conduct an extensive explo-\nration of various combinations of parameter-\nef\ufb01cient \ufb01ne-tuning techniques . Their search\nspace includes dividing consecutive layers into\nfour uneven groups, allocating varying amounts\nof trainable parameters to each layer, determin-\ning which groups to \ufb01ne-tune, and deciding which\nPEFT methods to apply to each group.\nTheir proposed method, S4, divides layers into\nfour groups ( G1,2,3,4) using a \u201cspindle\u201d pattern:\nmore layers are allocated to the middle groups and\nfewer to the top and bottom groups. All groups\nare trainable, with trainable parameters uniformly\nallocated across the layers (not groups). Different\ncombinations of PEFT methods are applied to dif-\nferent groups. Speci\ufb01cally,\nG1:A,L G 3:A,P,B\nG2:A,P G 4:P,B,L(4)\nwhere A stands for Adapters (Section 6.1), P for\nPre\ufb01x-Tuning (Section 7.2), B for BitFit (Section\n9.1), and L for LoRa (Section 10.2).\nThe search experiments were conducted on T5-\nbase and the GLUE dataset at 0.5% trainable pa-\nrameters. The S4 method was then applied to T5-\n3B, RoBERTa, and XL-Net, consistently outper-\nforming individual BitFit, Pre\ufb01x Tuning, LoRa,\nand Adapters across different architectures, model\nsizes, and tasks.\n12", "start_char_idx": 2327, "end_char_idx": 3614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69a130fa-e299-4c24-8003-83a85da8ed33": {"__data__": {"id_": "69a130fa-e299-4c24-8003-83a85da8ed33", "embedding": null, "metadata": {"page_label": "13", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "749ba495-d5ca-4d10-a92f-283833d68dd2", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "da8760eaf692a2dd3ad4b7a82c4d7163f96f733be02c94b1d2c8a54e16931885", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71c2f4d4-9f8b-404a-9473-aae3dcf0894a", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "152b31f037a36cb92fda355a98841c7dfaedeeff3ede9d37fb748a2092faba22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68aa091d-c8f4-44fd-ad48-98eec269670e", "node_type": "1", "metadata": {}, "hash": "3a05d09ec2a648fd5f58dc9f17fffc2475a99222c6ba6aa21d81b9928a64fe7a", "class_name": "RelatedNodeInfo"}}, "text": "12 Reporting and comparison issues\nSurvey papers tend to discuss reporting issues, and\nunfortunately, this one is not an exception. We\nidenti\ufb01ed several challenges and inconsistencies\nthat warrant discussion. These challenges make\nit dif\ufb01cult to draw direct comparisons between\nmethods and evaluate their true performance.\nInconsistent Parameter Counts One of the pri-\nmary challenges stems from the difference in the\nway researchers report parameter counts. These\ninconsistencies are not a result of dishonesty but\narise from the inherent complexity of the prob-\nlem. Generally, parameter counts can be catego-\nrized into three types: the number of trainable\nparameters , the number of changed parameters\nbetween the original and \ufb01ne-tuned models, and\ntherank of the difference between the original and\n\ufb01ne-tuned models.\nThese distinctions can have signi\ufb01cant impli-\ncations. For example, IntrinsicSAID (Section\n10.1) learns a low-rank ( \u223c100-1000) transforma-\ntion of model parameters. However, it changes\nall of the model\u2019s parameters. DiffPruning (Sec-\ntion 9.2) learns an update of 0.5%parameters,\nbut it actually trains 200% parameters: \ufb01ne-tuning\nthe model and learning the binary mask. For\nreparameterization-based methods (Sections 10.2,\n10.3, 11.4), memory requirements may vary de-\npending on the implementation design choices.\nOf the three types, the number of trainable pa-\nrameters is the most reliable predictor of mem-\nory ef\ufb01ciency. However, it is still imperfect.\nFor instance, Ladder-side Tuning (Section 8.1)\nuses a separate side-network with more parame-\nters than LoRa or BitFit, but it requires less RAM,\nsince backpropagation is not computed through\nthe main network.\nModel Size Another challenge arises from the\nvariation in model sizes used in the evaluation\nof PEFT methods. Several studies (Aghajanyan\net al., 2020; Hu et al., 2021) have demonstrated\nthat larger models require fewer parameters to\nbe updated during \ufb01ne-tuning, both in terms of\npercentage and when the model is large enough,\nsometimes even in absolute terms (Li and Liang,\n2021). Thus, model size must be considered\nwhen comparing PEFT methods , not just the ra-\ntio of trainable parameters.Lack of Standard Benchmarks and Metrics\nThe absence of standard benchmarks and met-\nrics further complicates comparisons. New meth-\nods are often evaluated on different model/dataset\ncombinations, making it challenging to draw\nmeaningful conclusions.\nWe would like to highlight the papers that re-\nport a variety of metrics on standard datasets sim-\nplifying comparison to other methods. For exam-\nple, KronA (Edalati et al., 2022) evaluated T5-\nbase on the GLUE benchmark and reported ac-\ncuracy, training time, and inference time while\nmaintaining the same number of trainable param-\neters. UniPELT (Mao et al., 2021) assessed BERT\non the GLUE benchmark and reported accuracy,\ntraining time, and inference latency, although it\nused different parameter counts for various meth-\nods. LST (Sung et al., 2022) evaluated different\nT5 sizes on the GLUE benchmark, reporting met-\nrics such as accuracy, training time, the number\nof updated parameters, and memory usage. MAM\n(He et al., 2022a) applied multiple models to the\nXSUM benchmark and reported accuracy across\na range of trainable parameters, although memory\ncomparisons were not provided.\nHowever, even these papers lack full compa-\nrability due to differences in their evaluation set-\ntings, such as varying parameter counts or the\nabsence of certain metrics like memory compar-\nisons. These inconsistencies highlight the need for\na standardized benchmark and uni\ufb01ed metrics to\nfacilitate more accurate comparisons and evalua-\ntions of PEFT methods.\nIssues with Published Implementations An-\nother issue encountered is the state of published\nimplementations. Many codebases are simply\ncopies of the Transformers library (Wolf et al.,\n2020) or other repositories with only minor modi-\n\ufb01cations. These copies often do not use git forks,\nmaking it dif\ufb01cult to identify the differences un-\nless they are highlighted in the README \ufb01le.\nFurthermore, even when differences are easy to\n\ufb01nd, the code is frequently not reusable.", "start_char_idx": 0, "end_char_idx": 4160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68aa091d-c8f4-44fd-ad48-98eec269670e": {"__data__": {"id_": "68aa091d-c8f4-44fd-ad48-98eec269670e", "embedding": null, "metadata": {"page_label": "13", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "749ba495-d5ca-4d10-a92f-283833d68dd2", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "da8760eaf692a2dd3ad4b7a82c4d7163f96f733be02c94b1d2c8a54e16931885", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69a130fa-e299-4c24-8003-83a85da8ed33", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "bdc3d3559b7546e634912cb286d52d4e7e5b6b9d2b746c131920f4d098f3b5bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "445afb20-7c2e-4c7d-93ee-72a2e75b38cb", "node_type": "1", "metadata": {}, "hash": "89ef929a406f0cbf794b79edaafed2ab2ede75472b6d927ce1c74817efad2834", "class_name": "RelatedNodeInfo"}}, "text": "However, even these papers lack full compa-\nrability due to differences in their evaluation set-\ntings, such as varying parameter counts or the\nabsence of certain metrics like memory compar-\nisons. These inconsistencies highlight the need for\na standardized benchmark and uni\ufb01ed metrics to\nfacilitate more accurate comparisons and evalua-\ntions of PEFT methods.\nIssues with Published Implementations An-\nother issue encountered is the state of published\nimplementations. Many codebases are simply\ncopies of the Transformers library (Wolf et al.,\n2020) or other repositories with only minor modi-\n\ufb01cations. These copies often do not use git forks,\nmaking it dif\ufb01cult to identify the differences un-\nless they are highlighted in the README \ufb01le.\nFurthermore, even when differences are easy to\n\ufb01nd, the code is frequently not reusable. Users\nare often required to install a modi\ufb01ed version of\nthe Transformers library, which con\ufb02icts with the\nmost recent version and lacks documentation or\nany examples of how to reuse the method outside\nof the existing codebase.\nDespite these challenges, there are some\nmethods with reusable implementations worth\n13", "start_char_idx": 3329, "end_char_idx": 4476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "445afb20-7c2e-4c7d-93ee-72a2e75b38cb": {"__data__": {"id_": "445afb20-7c2e-4c7d-93ee-72a2e75b38cb", "embedding": null, "metadata": {"page_label": "14", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b124b9d-5509-4177-9e6c-d4e7c2cea021", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "ab3d48ee4555b2b424472e53745a2f392a4b2fc422fb5e0f565d3f836fdfe705", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68aa091d-c8f4-44fd-ad48-98eec269670e", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "416f593e17bc49713a85a73253a178b8f63b9d60eca56f1764b2edfa04b85c39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14f4c3aa-a3d8-4f30-a22e-9d18f9bbc32b", "node_type": "1", "metadata": {}, "hash": "45bee86fae94617d864dac3b55d3b3335fc3d29016e7cac82e30f2a056ff999a", "class_name": "RelatedNodeInfo"}}, "text": "highlighting, such as LoRa7and Compacter8.\nThese implementations stand out for their user-\nfriendliness and adaptability, providing a solid\nfoundation for further research and development.\n13 Best Practices\nTo address different issues identi\ufb01ed in the\nparameter-ef\ufb01cient \ufb01ne-tuning literature, we put\nforward the following best practices for future re-\nsearch:\nExplicit reporting of parameter count type:\nWe encourage authors to clearly specify the pa-\nrameter count being reported in their papers or,\nideally, report all three types of parameter count:\ntrainable, changed, and rank. This will improve\nunderstanding and allow for more accurate com-\nparisons between methods.\nEvaluate with different model sizes: It is im-\nportant to assess their methods using different\nmodel sizes, as this can provide a more compre-\nhensive understanding of each method\u2019s strengths\nand limitations. This is particularly important con-\nsidering that even recent papers often focus solely\non BERT.\nComparisons to similar methods: In addition\nto comparing their methods with popular ap-\nproaches (e.g., LoRa, BitFit, Adapters) we should\nalso analyze their methods alongside other tech-\nniques that share conceptual and architectural re-\nsemblances. In our review, we often came across\nmethods that were based on very similar ideas and\ndesigns but were never directly compared. Under-\ntaking such comparisons will offer a more com-\nprehensive understanding of a method\u2019s perfor-\nmance and its relative strengths in relation to ex-\nisting techniques.\nStandardized PEFT benchmarks and competi-\ntions: We propose the development of standard-\nized PEFT benchmarks and competitions, which\nwould require participants to compete under the\nsame conditions and facilitate direct comparisons\nof results. These benchmarks should provide stan-\ndardized data and models at different scales to\nevaluate training ef\ufb01ciency.\nTo assess training time and memory ef\ufb01ciency,\ncompetitions can offer a centralized server or spec-\nify a comprehensive server con\ufb01guration that out-\n7github.com/microsoft/LoRA\n8github.com/rabeehk/compacterlines the CPU type, amount of memory, and GPU\ntype and quantity. Ideally, this could take the form\nof an instance template to one of the major cloud\nproviders. GPU memory consumption should be\nevaluated in a standardized way.\nEmphasize code clarity and minimal imple-\nmentations: As a community, we need to pri-\noritize code that is easy to understand and fea-\ntures simple, reusable implementations. In some\ncases, such implementations provide additional in-\nsight to the paper and could be written in a concise\nmanner. This proposal is in the interest of indi-\nvidual researchers as well, as easy-to-reuse meth-\nods may become more popular and, consequently,\nmore cited.\n14 Discussion\nThe growing accessibility of large language mod-\nels (Zhang et al., 2022; Zeng et al., 2022;\nKhrushchev et al., 2022; Touvron et al., 2023)\nand the democratization of their inference through\nlow-bit quantization (Dettmers et al., 2022;\nDettmers and Zettlemoyer, 2022) has enabled the\nresearch community to study, experiment, and\ntackle new tasks with relatively modest compute\nbudgets. Parameter-ef\ufb01cient \ufb01ne-tuning is the next\nstep that will allow us not just to inference, but to\nmodify these models.\nAmong the developed methods, some have\ndemonstrated their practicality at scale (Table 2),\nsuch as Adapters (Section 6.1), Prompt Tuning\n(Section 7.1), LoRa (Section 10.2), and (IA)3\n(Section 8.2). However, in practice, matching the\nperformance of full \ufb01ne-tuning remains a chal-\nlenge. One of the reasons is high sensitivity to\nhyperparameters , with optimal hyperparameters\noften signi\ufb01cantly deviating from those used in\nfull \ufb01ne-tuning due to the varying number of train-\nable parameters. For instance, the optimal learn-\ning rate for parameter-ef\ufb01cient \ufb01ne-tuning is gen-\nerally much higher than that for full \ufb01ne-tuning.\nThe research community should promote in-depth\ninvestigations into the impact of hyperparameters\non these methods and \ufb01nding reasonable defaults,\nas parameter-ef\ufb01cient \ufb01ne-tuning or large models\ncan be noticeably costly at 20-100B scale.", "start_char_idx": 0, "end_char_idx": 4149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14f4c3aa-a3d8-4f30-a22e-9d18f9bbc32b": {"__data__": {"id_": "14f4c3aa-a3d8-4f30-a22e-9d18f9bbc32b", "embedding": null, "metadata": {"page_label": "14", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b124b9d-5509-4177-9e6c-d4e7c2cea021", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "ab3d48ee4555b2b424472e53745a2f392a4b2fc422fb5e0f565d3f836fdfe705", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "445afb20-7c2e-4c7d-93ee-72a2e75b38cb", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "fe1e5c4713f52a484c7c012a443927d58229937d5c4d2fc7f7fcba599005ecb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a792c8ef-7a81-48a7-bc0d-6797331df8b8", "node_type": "1", "metadata": {}, "hash": "27d73accda11dc852cbae6adb782e333572274519d3b1d03f40dfe80d5e0d6f3", "class_name": "RelatedNodeInfo"}}, "text": "However, in practice, matching the\nperformance of full \ufb01ne-tuning remains a chal-\nlenge. One of the reasons is high sensitivity to\nhyperparameters , with optimal hyperparameters\noften signi\ufb01cantly deviating from those used in\nfull \ufb01ne-tuning due to the varying number of train-\nable parameters. For instance, the optimal learn-\ning rate for parameter-ef\ufb01cient \ufb01ne-tuning is gen-\nerally much higher than that for full \ufb01ne-tuning.\nThe research community should promote in-depth\ninvestigations into the impact of hyperparameters\non these methods and \ufb01nding reasonable defaults,\nas parameter-ef\ufb01cient \ufb01ne-tuning or large models\ncan be noticeably costly at 20-100B scale. Addi-\ntionally, efforts should be directed towards devel-\noping methods that minimize hyperparameter sen-\nsitivity, such as pre-training new parameters (Vu\net al., 2021; Su et al., 2021).\nExamining the taxonomy of methods and the\n14", "start_char_idx": 3483, "end_char_idx": 4382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a792c8ef-7a81-48a7-bc0d-6797331df8b8": {"__data__": {"id_": "a792c8ef-7a81-48a7-bc0d-6797331df8b8", "embedding": null, "metadata": {"page_label": "15", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30d5b6e7-7500-4405-b0bf-10a1615bff47", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "2a732a8c178282accfaaf9f75f2e1ad0cbe2f07b6ddb55b02fdaf8f549cabb6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14f4c3aa-a3d8-4f30-a22e-9d18f9bbc32b", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "4d63d2c3d3e68cbd83c7c909195840aa9b50ca0d9485d6574a9fea1add732dc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19f1515b-1bd1-4641-99a5-93e14df1457d", "node_type": "1", "metadata": {}, "hash": "5565a5b8af1257dd153799a76d636cbeafc638065c49ca9a42752e534588a744", "class_name": "RelatedNodeInfo"}}, "text": "progress made thus far, it is evident that low-rank\nreparameterization has been remarkably success-\nful in enhancing parameter ef\ufb01ciency. LoRa-style\n(Section 10.2) and Kronecker-product (Sections\n11.4 and 10.3) reparameterizations both decrease\nthe number of trainable parameters while requir-\ning minimal extra computation. A possible future\ndirection of \ufb01nding new PEFT models is explor-\ning different reparametrization techniques with\nfavorable trainable parameter count vs. rank ratio.\nAnother possible direction of improvement is\nutilizing what we know about how transformer\nmodels process texts (Rogers et al., 2020). Most\nof the PEFT methods work uniformly for the\nmodel, while we know that models process in-\nput differently at different layers. Utilizing this\nknowledge or building systems that have an adap-\ntive number of parameters per layer could further\nimprove parameter ef\ufb01ciency and accuracy.\nIn many respects, our current situation resem-\nbles the challenges from edge machine learning :\nwe consistently face constraints in memory, com-\nputation, and even energy consumption. Tech-\nniques like quantization and pruning (Gupta et al.,\n2015; LeCun et al., 1989) widely used in edge ma-\nchine learning, now bene\ufb01t large language mod-\nels. As we move forward, it is not only plausible\nbut also likely that more ideas could be exchanged\nbetween these two areas. Cross-disciplinary col-\nlaboration could further exchange ideas, accelerat-\ning innovation and progress in parameter-ef\ufb01cient\n\ufb01ne-tuning.\nReferences\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal\nGupta. 2020. Intrinsic dimensionality explains\nthe effectiveness of language model \ufb01ne-tuning.\nInAnnual Meeting of the Association for Com-\nputational Linguistics .\nAlan Ansell, Edoardo Ponti, Anna Korhonen, and\nIvan Vuli \u00b4c. 2022. Composable sparse \ufb01ne-\ntuning for cross-lingual transfer. In Proceed-\nings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1:\nLong Papers) , pages 1778\u20131796, Dublin, Ire-\nland. Association for Computational Linguis-\ntics.\nSanjeev Arora, Rong Ge, Behnam Neyshabur,\nand Yi Zhang. 2018. Stronger generalizationbounds for deep nets via a compression ap-\nproach. In International Conference on Ma-\nchine Learning .\nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E.\nHinton. 2016. Layer normalization. ArXiv ,\nabs/1607.06450.\nElad Ben-Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021. Bit\ufb01t: Simple parameter-ef\ufb01cient\n\ufb01ne-tuning for transformer-based masked\nlanguage-models. ArXiv , abs/2106.10199.\nTom Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-V oss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter,\nChris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot\nlearners. In Advances in Neural Information\nProcessing Systems , volume 33, pages 1877\u2013\n1901. Curran Associates, Inc.\nJin Cao, Chandan Prakash, and Wael Hamza.\n2022. Attention fusion: a light yet ef\ufb01cient late\nfusion mechanism for task adaptation in nlu. In\nNAACL-HLT .\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li,\nAlexander J. Smola, and Diyi Yang. 2023.\nParameter-ef\ufb01cient \ufb01ne-tuning design spaces.\nArXiv , abs/2301.01821.\nDavid Chiang, Alexander M. Rush, and Boaz\nBarak. 2021. Named tensor notation.", "start_char_idx": 0, "end_char_idx": 3545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19f1515b-1bd1-4641-99a5-93e14df1457d": {"__data__": {"id_": "19f1515b-1bd1-4641-99a5-93e14df1457d", "embedding": null, "metadata": {"page_label": "15", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30d5b6e7-7500-4405-b0bf-10a1615bff47", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "2a732a8c178282accfaaf9f75f2e1ad0cbe2f07b6ddb55b02fdaf8f549cabb6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a792c8ef-7a81-48a7-bc0d-6797331df8b8", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "92504ba55a5c2281fbfa482c167862ce4f16fd01741dbfc01497453b8155b07a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec383b36-8ddd-45f5-9703-287d48500c3e", "node_type": "1", "metadata": {}, "hash": "3a598bd884f142c8e513101334c9ceb262b116f0e8baa6f1050016878d8214e9", "class_name": "RelatedNodeInfo"}}, "text": "2020. Language models are few-shot\nlearners. In Advances in Neural Information\nProcessing Systems , volume 33, pages 1877\u2013\n1901. Curran Associates, Inc.\nJin Cao, Chandan Prakash, and Wael Hamza.\n2022. Attention fusion: a light yet ef\ufb01cient late\nfusion mechanism for task adaptation in nlu. In\nNAACL-HLT .\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li,\nAlexander J. Smola, and Diyi Yang. 2023.\nParameter-ef\ufb01cient \ufb01ne-tuning design spaces.\nArXiv , abs/2301.01821.\nDavid Chiang, Alexander M. Rush, and Boaz\nBarak. 2021. Named tensor notation. ArXiv ,\nabs/2102.13196.\nAakanksha Chowdhery, Sharan Narang, Jacob De-\nvlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko,\nJoshua Maynez, Abhishek Rao, Parker Barnes,\nYi Tay, Noam M. Shazeer, Vinodkumar Prab-\nhakaran, Emily Reif, Nan Du, Benton C.\nHutchinson, Reiner Pope, James Bradbury,\nJacob Austin, Michael Isard, Guy Gur-Ari,\n15", "start_char_idx": 3007, "end_char_idx": 3989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec383b36-8ddd-45f5-9703-287d48500c3e": {"__data__": {"id_": "ec383b36-8ddd-45f5-9703-287d48500c3e", "embedding": null, "metadata": {"page_label": "16", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "128f4dbe-1291-4aa9-84f8-2b07369f8f9d", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "479a0a34c6ca9643c34791a8974142640f1eba1d51637410c353ab5eb514665d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19f1515b-1bd1-4641-99a5-93e14df1457d", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "614fe9c1604f7668991f2a489a86073e1825ae0a6e32bfed2030424ca181ce40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61c49036-b174-42f1-94d5-e42ac913de81", "node_type": "1", "metadata": {}, "hash": "13a80d1ac648dc912218317cf69c986e089f16e55a4470f491a6a47af6b1652d", "class_name": "RelatedNodeInfo"}}, "text": "Pengcheng Yin, Toju Duke, Anselm Lev-\nskaya, Sanjay Ghemawat, Sunipa Dev, Hen-\nryk Michalewski, Xavier Garc\u00eda, Vedant Misra,\nKevin Robinson, Liam Fedus, Denny Zhou,\nDaphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal,\nMark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pel-\nlat, Aitor Lewkowycz, Erica Moreira, Re-\nwon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark D\u00edaz, Orhan Firat, Michele Catasta, Ja-\nson Wei, Kathleen S. Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel.\n2022. Palm: Scaling language modeling with\npathways. ArXiv , abs/2204.02311.\nTim Dettmers, Mike Lewis, Younes Belkada, and\nLuke Zettlemoyer. 2022. Llm. int8 (): 8-bit\nmatrix multiplication for transformers at scale.\narXiv preprint arXiv:2208.07339 .\nTim Dettmers and Luke Zettlemoyer. 2022. The\ncase for 4-bit precision: k-bit inference scaling\nlaws. ArXiv , abs/2212.09720.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019\nConference of the North American Chapter\nof the Association for Computational Linguis-\ntics: Human Language Technologies, Volume\n1 (Long and Short Papers) , pages 4171\u20134186,\nMinneapolis, Minnesota. Association for Com-\nputational Linguistics.\nNing Ding, Yujia Qin, Guang Yang, Fu Wei,\nZonghan Yang, Yusheng Su, Shengding Hu,\nYulin Chen, Chi-Min Chan, Weize Chen, Jing\nYi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu,\nHaitao Zheng, Jianfei Chen, Yang Liu, Jie Tang,\nJuan Li, and Maosong Sun. 2022. Delta tun-\ning: A comprehensive study of parameter ef\ufb01-\ncient methods for pre-trained language models.\nArXiv , abs/2203.06904.\nJeff Donahue, Yangqing Jia, Oriol Vinyals, Judy\nHoffman, Ning Zhang, Eric Tzeng, and Trevor\nDarrell. 2014. Decaf: A deep convolutional ac-\ntivation feature for generic visual recognition.\nInProceedings of the 31st International Confer-ence on Machine Learning , volume 32 of Pro-\nceedings of Machine Learning Research , pages\n647\u2013655, Bejing, China. PMLR.\nAli Edalati, Marzieh S. Tahaei, Ivan Kobyzev,\nV . Nia, James J. Clark, and Mehdi Reza-\ngholizadeh. 2022. Krona: Parameter ef\ufb01-\ncient tuning with kronecker adapter. ArXiv ,\nabs/2212.10650.\nWilliam Fedus, Barret Zoph, and Noam M.\nShazeer. 2021. Switch transformers: Scaling to\ntrillion parameter models with simple and ef\ufb01-\ncient sparsity. ArXiv , abs/2101.03961.\nCheng Fu, Hanxian Huang, Xinyun Chen, Yuan-\ndong Tian, and Jishen Zhao. 2021. Learn-\nto-share: A hardware-friendly transfer learning\nframework exploiting computation and param-\neter sharing. In Proceedings of the 38th Inter-\nnational Conference on Machine Learning , vol-\nume 139 of Proceedings of Machine Learning\nResearch , pages 3469\u20133479. PMLR.\nMozhdeh Gheini, Xiang Ren, and Jonathan May.\n2021. Cross-attention is all you need: Adapting\npretrained transformers for machine translation.\nInConference on Empirical Methods in Natural\nLanguage Processing .\nDemi Guo, Alexander M. Rush, and Yoon Kim.\n2020. Parameter-ef\ufb01cient transfer learning with\ndiff pruning.", "start_char_idx": 0, "end_char_idx": 3188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61c49036-b174-42f1-94d5-e42ac913de81": {"__data__": {"id_": "61c49036-b174-42f1-94d5-e42ac913de81", "embedding": null, "metadata": {"page_label": "16", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "128f4dbe-1291-4aa9-84f8-2b07369f8f9d", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "479a0a34c6ca9643c34791a8974142640f1eba1d51637410c353ab5eb514665d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec383b36-8ddd-45f5-9703-287d48500c3e", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "0536ff345a8b09e9142cf781fc2d8eb97f061c0da4238c90f8b150d4c04406cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4e28ff8-a124-47ca-857f-e84a5517d915", "node_type": "1", "metadata": {}, "hash": "97951ebf429fcf3b84b52e70e148a69fa7e239e10afe5df020abb64e6aa322ed", "class_name": "RelatedNodeInfo"}}, "text": "ArXiv , abs/2101.03961.\nCheng Fu, Hanxian Huang, Xinyun Chen, Yuan-\ndong Tian, and Jishen Zhao. 2021. Learn-\nto-share: A hardware-friendly transfer learning\nframework exploiting computation and param-\neter sharing. In Proceedings of the 38th Inter-\nnational Conference on Machine Learning , vol-\nume 139 of Proceedings of Machine Learning\nResearch , pages 3469\u20133479. PMLR.\nMozhdeh Gheini, Xiang Ren, and Jonathan May.\n2021. Cross-attention is all you need: Adapting\npretrained transformers for machine translation.\nInConference on Empirical Methods in Natural\nLanguage Processing .\nDemi Guo, Alexander M. Rush, and Yoon Kim.\n2020. Parameter-ef\ufb01cient transfer learning with\ndiff pruning. In Annual Meeting of the Associa-\ntion for Computational Linguistics .\nSuyog Gupta, Ankur Agrawal, Kailash Gopalakr-\nishnan, and Pritish Narayanan. 2015. Deep\nlearning with limited numerical precision. In\nProceedings of the 32nd International Confer-\nence on Machine Learning , volume 37 of Pro-\nceedings of Machine Learning Research , pages\n1737\u20131746, Lille, France. PMLR.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. Warp: Word-level adver-\nsarial reprogramming. In Annual Meeting of the\nAssociation for Computational Linguistics .\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor\nBerg-Kirkpatrick, and Graham Neubig. 2022a.\nTowards a uni\ufb01ed view of parameter-ef\ufb01cient\ntransfer learning. In International Conference\non Learning Representations .\n16", "start_char_idx": 2502, "end_char_idx": 3954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4e28ff8-a124-47ca-857f-e84a5517d915": {"__data__": {"id_": "f4e28ff8-a124-47ca-857f-e84a5517d915", "embedding": null, "metadata": {"page_label": "17", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3c1b640-f9e6-4019-b9e3-bc78d5bdb498", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "5a59ebf2fb73646f8acdeb5a3d79425d03463ecca7def42f73312e969cb1c5c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61c49036-b174-42f1-94d5-e42ac913de81", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "f9ee58076f21e554f1964e63632bd516d57a60a8fa31a974fe4e8e7df978974b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a9c6235-b1f6-41fd-8c2f-f0cea3d0c1d7", "node_type": "1", "metadata": {}, "hash": "38bc0557a116ce1296b1643df8e8ee9310a92314634a8c3d1e9b67354d458072", "class_name": "RelatedNodeInfo"}}, "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. 2016. Deep residual learning for\nimage recognition. 2016 IEEE Conference\non Computer Vision and Pattern Recognition\n(CVPR) .\nShwai He, Liang Ding, Daize Dong,\nJeremy Zhang, and Dacheng Tao. 2022b.\nSparseAdapter: An easy approach for improv-\ning the parameter-ef\ufb01ciency of adapters. In\nFindings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 2184\u20132190,\nAbu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nConnor Holmes, Minjia Zhang, Yuxiong He,\nand Bo Wu. 2021. Nxmtransformer: Semi-\nstructured sparsi\ufb01cation for natural language\nunderstanding via admm. In Advances in Neu-\nral Information Processing Systems , volume 34,\npages 1818\u20131830. Curran Associates, Inc.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzeb-\nski, Bruna Morrone, Quentin de Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Syl-\nvain Gelly. 2019. Parameter-ef\ufb01cient transfer\nlearning for nlp. In International Conference\non Machine Learning .\nEdward J. Hu, Yelong Shen, Phillip Wallis,\nZeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nand Weizhu Chen. 2021. Lora: Low-rank\nadaptation of large language models. ArXiv ,\nabs/2106.09685.\nRabeeh Karimi Mahabadi, James Henderson, and\nSebastian Ruder. 2021. Compacter: Ef\ufb01cient\nlow-rank hypercomplex adapter layers. In Ad-\nvances in Neural Information Processing Sys-\ntems, volume 34, pages 1022\u20131035. Curran As-\nsociates, Inc.\nMikhail Khrushchev, Ruslan Vasilev, Alexey\nPetrov, and Nikolay Zinov. 2022. YaLM 100B.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. In 3rd In-\nternational Conference on Learning Represen-\ntations, ICLR 2015, San Diego, CA, USA, May\n7-9, 2015, Conference Track Proceedings .\nQuoc V . Le, Tam\u00e1s Sarl\u00f3s, and Alex Smola. 2013.\nFastfood: Approximate kernel expansions in\nloglinear time. ArXiv , abs/1408.3060.Yann LeCun, John Denker, and Sara Solla. 1989.\nOptimal brain damage. In Advances in Neu-\nral Information Processing Systems , volume 2.\nMorgan-Kaufmann.\nBrian Lester, Rami Al-Rfou, and Noah Con-\nstant. 2021. The power of scale for\nparameter-ef\ufb01cient prompt tuning. ArXiv ,\nabs/2104.08691.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer.\n2019. Bart: Denoising sequence-to-sequence\npre-training for natural language generation,\ntranslation, and comprehension. arXiv preprint\narXiv:1910.13461 .\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and\nJason Yosinski. 2018. Measuring the intrinsic\ndimension of objective landscapes. In Interna-\ntional Conference on Learning Representations .\nXiang Lisa Li and Percy Liang. 2021. Pre\ufb01x-\ntuning: Optimizing continuous prompts for\ngeneration. In Proceedings of the 59th An-\nnual Meeting of the Association for Compu-\ntational Linguistics and the 11th International\nJoint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 4582\u2013\n4597, Online. Association for Computational\nLinguistics.\nHaokun Liu, Derek Tam, Mohammed Muqeeth,\nJay Mohta, Tenghao Huang, Mohit Bansal,\nand Colin Raffel. 2022. Few-shot parameter-\nef\ufb01cient \ufb01ne-tuning is better and cheaper than\nin-context learning.", "start_char_idx": 0, "end_char_idx": 3194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a9c6235-b1f6-41fd-8c2f-f0cea3d0c1d7": {"__data__": {"id_": "6a9c6235-b1f6-41fd-8c2f-f0cea3d0c1d7", "embedding": null, "metadata": {"page_label": "17", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3c1b640-f9e6-4019-b9e3-bc78d5bdb498", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "5a59ebf2fb73646f8acdeb5a3d79425d03463ecca7def42f73312e969cb1c5c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4e28ff8-a124-47ca-857f-e84a5517d915", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "fb58d4e9ca88df1db8475025df8fb6b85ecee92661cff4315339c1f9c6f46ed2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e281bd8a-7299-4f17-87fd-31d2eb25073e", "node_type": "1", "metadata": {}, "hash": "475c3f0e66932626fa793dc941978355e610ba012a4beaef61cf06778a98c48c", "class_name": "RelatedNodeInfo"}}, "text": "2018. Measuring the intrinsic\ndimension of objective landscapes. In Interna-\ntional Conference on Learning Representations .\nXiang Lisa Li and Percy Liang. 2021. Pre\ufb01x-\ntuning: Optimizing continuous prompts for\ngeneration. In Proceedings of the 59th An-\nnual Meeting of the Association for Compu-\ntational Linguistics and the 11th International\nJoint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 4582\u2013\n4597, Online. Association for Computational\nLinguistics.\nHaokun Liu, Derek Tam, Mohammed Muqeeth,\nJay Mohta, Tenghao Huang, Mohit Bansal,\nand Colin Raffel. 2022. Few-shot parameter-\nef\ufb01cient \ufb01ne-tuning is better and cheaper than\nin-context learning. ArXiv , abs/2205.05638.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming\nDing, Yujie Qian, Zhilin Yang, and Jie\nTang. 2021. Gpt understands, too. ArXiv ,\nabs/2103.10385.\nWesley J. Maddox, Gregory Benton, and An-\ndrew Gordon Wilson. 2020. Rethinking pa-\nrameter counting: Effective dimensionality re-\nvisted. arXiv preprint arXiv:2003.02139 .\nSadhika Malladi, Alexander Wettig, Dingli Yu,\nDanqi Chen, and Sanjeev Arora. 2022. A\nkernel-based view of language model \ufb01ne-\ntuning. arXiv preprint arXiv:2210.05643 .\n17", "start_char_idx": 2512, "end_char_idx": 3700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e281bd8a-7299-4f17-87fd-31d2eb25073e": {"__data__": {"id_": "e281bd8a-7299-4f17-87fd-31d2eb25073e", "embedding": null, "metadata": {"page_label": "18", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e020124-a827-4495-82dc-522c8f050497", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "2d3a61727376197d69f4ae3eaa12e31219f5d6c2b677ef013314bd751dcf5b81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a9c6235-b1f6-41fd-8c2f-f0cea3d0c1d7", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "3648474cc77c62b161f7309260af24e9d8516fcd67ed6ff5fc3b0c1b4c75c8c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd9a1243-7e48-4401-a299-71334c199a08", "node_type": "1", "metadata": {}, "hash": "16e3943ec7d4116747dcb53ad51a30ee83b393306b09dbb5ede8c656b59917fc", "class_name": "RelatedNodeInfo"}}, "text": "Yuning Mao, Lambert Mathias, Rui Hou, Amjad\nAlmahairi, Hao Ma, Jiawei Han, Wen tau Yih,\nand Madian Khabsa. 2021. Unipelt: A uni-\n\ufb01ed framework for parameter-ef\ufb01cient language\nmodel tuning. ArXiv , abs/2110.07577.\nJonas Pfeiffer, Aishwarya Kamath, Andreas\nR\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych.\n2020a. Adapterfusion: Non-destructive task\ncomposition for transfer learning. ArXiv ,\nabs/2005.00247.\nJonas Pfeiffer, Sebastian Ruder, Ivan Vulic, and\nE. Ponti. 2023. Modular deep learning. ArXiv ,\nabs/2302.11529.\nJonas Pfeiffer, Andreas R\u00fcckl\u00e9, Clifton Poth,\nAishwarya Kamath, Ivan Vuli \u00b4c, Sebastian\nRuder, Kyunghyun Cho, and Iryna Gurevych.\n2020b. Adapterhub: A framework for adapt-\ning transformers. Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations .\nYujia Qin, Xiaozhi Wang, Yusheng Su, Yankai\nLin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan\nLiu, Juanzi Li, Lei Hou, Peng Li, Maosong Sun,\nand Jie Zhou. 2021. Exploring universal intrin-\nsic task subspace via prompt tuning.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learn-\ners.\nPranav Rajpurkar, Robin Jia, and Percy Liang.\n2018. Know what you don\u2019t know: Unan-\nswerable questions for squad. Proceedings of\nthe 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short\nPapers) .\nSylvestre-Alvise Rebuf\ufb01, Hakan Bilen, and An-\ndrea Vedaldi. 2017. Learning multiple visual\ndomains with residual adapters. In Advances\nin Neural Information Processing Systems , vol-\nume 30. Curran Associates, Inc.\nSylvestre-Alvise Rebuf\ufb01, Hakan Bilen, and An-\ndrea Vedaldi. 2018. Ef\ufb01cient parametrization\nof multi-domain deep neural networks. 2018\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 8119\u20138127.Anna Rogers, Olga Kovaleva, and Anna\nRumshisky. 2020. A primer in BERTol-\nogy: What we know about how BERT works.\nTransactions of the Association for Computa-\ntional Linguistics , 8:842\u2013866.\nVictor Sanh, Albert Webson, Colin Raffel,\nStephen Bach, Lintang Sutawika, Zaid\nAlyafeai, Antoine Chaf\ufb01n, Arnaud Stiegler,\nArun Raja, Manan Dey, M Saiful Bari, Canwen\nXu, Urmish Thakker, Shanya Sharma Sharma,\nEliza Szczechla, Taewoon Kim, Gunjan Chh-\nablani, Nihal Nayak, Debajyoti Datta, Jonathan\nChang, Mike Tian-Jian Jiang, Han Wang,\nMatteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht\nSharma, Andrea Santilli, Thibault Fevry,\nJason Alan Fries, Ryan Teehan, Teven Le Scao,\nStella Biderman, Leo Gao, Thomas Wolf, and\nAlexander M Rush. 2022. Multitask prompted\ntraining enables zero-shot task generalization.\nInInternational Conference on Learning\nRepresentations .", "start_char_idx": 0, "end_char_idx": 2771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd9a1243-7e48-4401-a299-71334c199a08": {"__data__": {"id_": "dd9a1243-7e48-4401-a299-71334c199a08", "embedding": null, "metadata": {"page_label": "18", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e020124-a827-4495-82dc-522c8f050497", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "2d3a61727376197d69f4ae3eaa12e31219f5d6c2b677ef013314bd751dcf5b81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e281bd8a-7299-4f17-87fd-31d2eb25073e", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "fb2fb7fb6a8bdc22d88cd9af17bdf30892d691fa939c12b64b95c916ef545f6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0321cb7-d60d-4168-a93d-ef0b89ee413a", "node_type": "1", "metadata": {}, "hash": "11a0343814d9b9b1cb1de212ec08d676dbca1e9ffc63f132f2836f610555546a", "class_name": "RelatedNodeInfo"}}, "text": "2022. Multitask prompted\ntraining enables zero-shot task generalization.\nInInternational Conference on Learning\nRepresentations .\nTeven Le Scao, Angela Fan, Christopher Akiki,\nElizabeth-Jane Pavlick, Suzana Ili\u2019c, Daniel\nHesslow, Roman Castagn\u2019e, Alexandra Sasha\nLuccioni, Franccois Yvon, Matthias Gall\u00e9,\nJonathan Tow, Alexander M. Rush, Stella Rose\nBiderman, Albert Webson, Pawan Sasanka Am-\nmanamanchi, Thomas Wang, Beno\u00eet Sagot,\nNiklas Muennighoff, Albert Villanova del\nMoral, Olatunji Ruwase, Rachel Bawden, Stas\nBekman, Angelina McMillan-Major, Iz Belt-\nagy, Huu Nguyen, Lucile Saulnier, Sam-\nson Tan, Pedro Ortiz Suarez, Victor Sanh,\nHugo Laurenccon, Yacine Jernite, Julien Lau-\nnay, Margaret Mitchell, Colin Raffel, Aaron\nGokaslan, Adi Simhi, Aitor Soroa Etxabe, Al-\nham Fikri Aji, Amit Alfassy, Anna Rogers,\nAriel Kreisberg Nitzav, Canwen Xu, Chenghao\nMou, Chris C. Emezue, Christopher Klamm,\nColin Leong, Daniel Alexander van Strien,\nDavid Ifeoluwa Adelani, Dragomir R. Radev,\nEduardo G. Ponferrada, Efrat Levkovizh, Ethan\nKim, Eyal Bar Natan, Francesco De Toni,\nG\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada\nPistilli, Hady ElSahar, Hamza Benyamina,\nHieu Tran, Ian Yu, Idris Abdulmumin, Isaac\nJohnson, Itziar Gonzalez-Dios, Javier de la\nRosa, Jenny Chim, Jesse Dodge, Jian Zhu,\n18", "start_char_idx": 2642, "end_char_idx": 3925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0321cb7-d60d-4168-a93d-ef0b89ee413a": {"__data__": {"id_": "b0321cb7-d60d-4168-a93d-ef0b89ee413a", "embedding": null, "metadata": {"page_label": "19", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2fcfd476-bafc-40c5-8ba0-af0499e67607", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7e4d2e575c86c69cca9d27261a5e7866c5e30b4e18843b2261211094a9c91a61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd9a1243-7e48-4401-a299-71334c199a08", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "d297c6a32697c043031f0c7cf4c7e296ef5863eb010a4e135d8d58e9689df5ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "227521f7-0347-4971-a446-96cdfc8fa441", "node_type": "1", "metadata": {}, "hash": "dc6666f58401d4c75c20cc1f4af1a8766b9edd487be5668d7c2605cc62cb8b54", "class_name": "RelatedNodeInfo"}}, "text": "Jonathan Chang, Jorg Frohberg, Josephine L.\nTobing, Joydeep Bhattacharjee, Khalid Al-\nmubarak, Kimbo Chen, Kyle Lo, Lean-\ndro von Werra, Leon Weber, Long Phan,\nLoubna Ben Allal, Ludovic Tanguy, Manan\nDey, Manuel Romero Mu\u00f1oz, Maraim Ma-\nsoud, Mar\u2019ia Grandury, Mario vSavsko, Max\nHuang, Maximin Coavoux, Mayank Singh,\nMike Tian-Jian Jiang, Minh Chien Vu, Mo-\nhammad Ali Jauhar, Mustafa Ghaleb, Nishant\nSubramani, Nora Kassner, Nurulaqilla Khamis,\nOlivier Nguyen, Omar Espejel, Ona de Gib-\nert, Paulo Villegas, Peter Henderson, Pierre\nColombo, Priscilla Amuok, Quentin Lhoest,\nRheza Harliman, Rishi Bommasani, Roberto\nL\u2019opez, Rui Ribeiro, Salomey Osei, Sampo\nPyysalo, Sebastian Nagel, Shamik Bose, Sham-\nsuddeen Hassan Muhammad, Shanya Sharma,\nS. Longpre, Somaieh Nikpoor, Stanislav Silber-\nberg, Suhas Pai, Sydney Zink, Tiago Timponi\nTorrent, Timo Schick, Tristan Thrush, Valentin\nDanchev, Vassilina Nikoulina, Veronika Laip-\npala, Violette Lepercq, Vrinda Prabhu, Zaid\nAlyafeai, Zeerak Talat, Arun Raja, Benjamin\nHeinzerling, Chenglei Si, Elizabeth Salesky,\nSabrina J. Mielke, Wilson Y . Lee, Abheesht\nSharma, Andrea Santilli, Antoine Chaf\ufb01n, Ar-\nnaud Stiegler, Debajyoti Datta, Eliza Szczechla,\nGunjan Chhablani, Han Wang, Harshit Pandey,\nHendrik Strobelt, Jason Alan Fries, Jos Rozen,\nLeo Gao, Lintang Sutawika, M Saiful Bari,\nMaged S. Al-shaibani, Matteo Manica, Ni-\nhal V .", "start_char_idx": 0, "end_char_idx": 1377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "227521f7-0347-4971-a446-96cdfc8fa441": {"__data__": {"id_": "227521f7-0347-4971-a446-96cdfc8fa441", "embedding": null, "metadata": {"page_label": "19", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2fcfd476-bafc-40c5-8ba0-af0499e67607", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7e4d2e575c86c69cca9d27261a5e7866c5e30b4e18843b2261211094a9c91a61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0321cb7-d60d-4168-a93d-ef0b89ee413a", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "0f1754808629dec57be3afbbc0f6089cdecac56d82fcdee4353dc1269a2d36e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "410a1ea1-ee6a-4bad-91d8-faebdf087155", "node_type": "1", "metadata": {}, "hash": "4e0dadc5151bc82de1801181bda0a401ea72b34fe2cdf9a4fdab990980ff95b1", "class_name": "RelatedNodeInfo"}}, "text": "Lee, Abheesht\nSharma, Andrea Santilli, Antoine Chaf\ufb01n, Ar-\nnaud Stiegler, Debajyoti Datta, Eliza Szczechla,\nGunjan Chhablani, Han Wang, Harshit Pandey,\nHendrik Strobelt, Jason Alan Fries, Jos Rozen,\nLeo Gao, Lintang Sutawika, M Saiful Bari,\nMaged S. Al-shaibani, Matteo Manica, Ni-\nhal V . Nayak, Ryan Teehan, Samuel Albanie,\nSheng Shen, Srulik Ben-David, Stephen H.\nBach, Taewoon Kim, Tali Bers, Thibault F\u00e9vry,\nTrishala Neeraj, Urmish Thakker, Vikas Rau-\nnak, Xiang Tang, Zheng Xin Yong, Zhiqing\nSun, Shaked Brody, Y Uri, Hadar Tojarieh,\nAdam Roberts, Hyung Won Chung, Jaesung\nTae, Jason Phang, O\ufb01r Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared\nCasper, Jeff Rasley, Max Ryabinin, Mayank\nMishra, Minjia Zhang, Mohammad Shoeybi,\nMyriam Peyrounette, Nicolas Patry, Nouamane\nTazi, Omar Sanseviero, Patrick von Platen,\nPierre Cornette, Pierre Franccois Lavall\u2019ee,\nR\u00e9mi Lacroix, Samyam Rajbhandari, San-\nchit Gandhi, Shaden Smith, St\u00e9phane Re-\nquena, Suraj Patil, Tim Dettmers, Ahmed\nBaruwa, Amanpreet Singh, Anastasia Chevel-\neva, Anne-Laure Ligozat, Arjun Subramo-nian, Aur\u2019elie N\u2019ev\u2019eol, Charles Lovering,\nDaniel H Garrette, Deepak R. Tunuguntla,\nEhud Reiter, Ekaterina Taktasheva, Ekaterina\nV oloshina, Eli Bogdanov, Genta Indra Winata,\nHailey Schoelkopf, Jan-Christoph Kalo, Jeka-\nterina Novikova, Jessica Zosa Forde, Jordan\nClive, Jungo Kasai, Ken Kawamura, Liam\nHazan, Marine Carpuat, Miruna Clinciu, Na-\njoung Kim, Newton Cheng, Oleg Serikov,\nOmer Antverg, Oskar van der Wal, Rui Zhang,\nRuochen Zhang, Sebastian Gehrmann, S. Os-\nher Pais, Tatiana Shavrina, Thomas Scialom,\nTian Yun, Tomasz Limisiewicz, Verena Rieser,\nVitaly Protasov, Vladislav Mikhailov, Yada\nPruksachatkun, Yonatan Belinkov, Zachary\nBamberger, Zdenvek Kasner, Alice Rueda,\nAmanda Pestana, Amir Feizpour, Ammar\nKhan, Amy Faranak, Ananda Santa Rosa San-\ntos, Anthony Hevia, Antigona Unldreaj, Arash\nAghagol, Arezoo Abdollahi, Aycha Tammour,\nAzadeh HajiHosseini, Bahareh Behroozi, Ben-\njamin Olusola Ajibade, Bharat Kumar Sax-\nena, Carlos Mu\u00f1oz Ferrandis, Danish Contrac-\ntor, David M. Lansky, Davis David, Douwe\nKiela, Duong Anh Nguyen, Edward Tan,\nEmily Baylor, Ezinwanne Ozoani, Fatim T\nMirza, Frankline Ononiwu, Habib Rezanejad,\nH.A.", "start_char_idx": 1088, "end_char_idx": 3310, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "410a1ea1-ee6a-4bad-91d8-faebdf087155": {"__data__": {"id_": "410a1ea1-ee6a-4bad-91d8-faebdf087155", "embedding": null, "metadata": {"page_label": "19", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2fcfd476-bafc-40c5-8ba0-af0499e67607", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7e4d2e575c86c69cca9d27261a5e7866c5e30b4e18843b2261211094a9c91a61", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "227521f7-0347-4971-a446-96cdfc8fa441", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "78312500e7182c0a4cf11858cf9fad773572857269b780a77cab4d0e34023c8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c4d8ebd-2d0b-484d-80ae-3623139f0a64", "node_type": "1", "metadata": {}, "hash": "08eb7819c61fbf85f2922ab989066fdd1ef6eef84f93ad1cb1f2ed5c782de0e9", "class_name": "RelatedNodeInfo"}}, "text": "Jones, Indrani Bhattacharya, Irene So-\nlaiman, Irina Sedenko, Isar Nejadgholi, Jan\nPassmore, Joshua Seltzer, Julio Bonis Sanz,\nKaren Fort, L\u00edvia Macedo Dutra, Mairon\nSamagaio, Maraim Elbadri, Margot Mieskes,\nMarissa Gerchick, Martha Akinlolu, Michael\nMcKenna, Mike Qiu, M. K. K. Ghauri, Mykola\nBurynok, Na\ufb01s Abrar, Nazneen Rajani, Nour\nElkott, Nourhan Fahmy, Olanrewaju Modupe\nSamuel, Ran An, R. P. Kromann, Ryan Hao,\nSamira Alizadeh, Sarmad Shubber, Silas L.\nWang, Sourav Roy, Sylvain Viguier, Thanh-\nCong Le, Tobi Oyebade, Trieu Nguyen Hai\nLe, Yoyo Yang, Zachary Kyle Nguyen, Abhi-\nnav Ramesh Kashyap, Alfredo Palasciano, Ali-\nson Callahan, Anima Shukla, Antonio Miranda-\nEscalada, Ayush Kumar Singh, Benjamin\nBeilharz, Bo Wang, Caio Matheus Fonseca\nde Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu,\nCl\u00e9mentine Fourrier, Daniel Le\u2019on Perin\u2019an,\nDaniel Molano, Dian Yu, Enrique Manjavacas,\nFabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddin Bayrak, Gully A. Burns, Helena U.\nVrabec, Iman I.B. Bello, Isha Dash, Ji Soo\n19", "start_char_idx": 3311, "end_char_idx": 4335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c4d8ebd-2d0b-484d-80ae-3623139f0a64": {"__data__": {"id_": "2c4d8ebd-2d0b-484d-80ae-3623139f0a64", "embedding": null, "metadata": {"page_label": "20", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da36e7b2-1ce3-4399-90a8-4f3aa340ed78", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "3728fa3a34517e51d11b7986878f87f180340f587dd3d6ec775af99798b38d85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "410a1ea1-ee6a-4bad-91d8-faebdf087155", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "de39a80dc3e028d2fa3bcf171b5aac1444372f92f9288435663aa816c4ea1c9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b22e1b1c-268c-40b9-aa57-08a90f20bb23", "node_type": "1", "metadata": {}, "hash": "38f450a234d5293991aad0fef5f0c4434e65e190b91c061e9511d83dd5350a04", "class_name": "RelatedNodeInfo"}}, "text": "Kang, John Giorgi, Jonas Golde, Jose David\nPosada, Karthi Sivaraman, Lokesh Bulchan-\ndani, Lu Liu, Luisa Shinzato, Madeleine Hahn\nde Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies,\nMar\u00eda Andrea Castillo, Marianna Nezhurina,\nMario Sanger, Matthias Samwald, Michael Cul-\nlan, Michael Weinberg, M Wolf, Mina Mi-\nhaljcic, Minna Liu, Moritz Freidank, Myung-\nsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muell-\nner, Pascale Fung, Patricia Haller, R. Chan-\ndrasekhar, R. Eisenberg, Robert Martin, Ro-\ndrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel\nCahyawijaya, Samuele Garda, Shlok S Desh-\nmukh, Shubhanshu Mishra, Sid Kiblawi, Si-\nmon Ott, Sinee Sang-aroonsiri, Srishti Kumar,\nStefan Schweter, Sushil Pratap Bharati, T. A.\nLaud, Th\u2019eo Gigant, Tomoya Kainuma, Wo-\njciech Kusa, Yanis Labrak, Yashasvi Bajaj,\nY . Venkatraman, Yifan Xu, Ying Xu, Yun chao\nXu, Zhee Xao Tan, Zhongli Xie, Zifan Ye,\nMathilde Bras, Younes Belkada, and Thomas\nWolf. 2022. Bloom: A 176b-parameter open-\naccess multilingual language model. ArXiv ,\nabs/2211.05100v2.\nTimo Schick and Hinrich Sch\u00fctze. 2021. It\u2019s not\njust size that matters: Small language mod-\nels are also few-shot learners. In Proceedings\nof the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computa-\ntional Linguistics: Human Language Technolo-\ngies, pages 2339\u20132352, Online. Association for\nComputational Linguistics.\nNoam Shazeer, *Azalia Mirhoseini, *Krzysztof\nMaziarz, Andy Davis, Quoc Le, Geoffrey Hin-\nton, and Jeff Dean. 2017. Outrageously large\nneural networks: The sparsely-gated mixture-\nof-experts layer. In International Conference\non Learning Representations .\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan\nCatanzaro. 2019. Megatron-lm: Training multi-\nbillion parameter language models using model\nparallelism. ArXiv , abs/1909.08053.\nYusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min\nChan, Yankai Lin, Zhiyuan Liu, Peng Li, Juan-\nZi Li, Lei Hou, Maosong Sun, and Jie Zhou.\n2021. On transferability of prompt tuningfor natural language understanding. ArXiv ,\nabs/2111.06719.\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal.\n2022. Lst: Ladder side-tuning for parameter\nand memory ef\ufb01cient transfer learning. ArXiv ,\nabs/2206.06522.\nYi-Lin Sung, Varun Nair, and Colin A Raffel.\n2021. Training neural networks with \ufb01xed\nsparse masks. In Advances in Neural Infor-\nmation Processing Systems , volume 34, pages\n24193\u201324205. Curran Associates, Inc.\nHugo Touvron, Thibaut Lavril, Gautier Izacard,\nXavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal,\nEric Hambro, Faisal Azhar, et al. 2023. Llama:\nOpen and ef\ufb01cient foundation language models.\narXiv preprint arXiv:2302.13971 .\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need.", "start_char_idx": 0, "end_char_idx": 2890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b22e1b1c-268c-40b9-aa57-08a90f20bb23": {"__data__": {"id_": "b22e1b1c-268c-40b9-aa57-08a90f20bb23", "embedding": null, "metadata": {"page_label": "20", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da36e7b2-1ce3-4399-90a8-4f3aa340ed78", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "3728fa3a34517e51d11b7986878f87f180340f587dd3d6ec775af99798b38d85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c4d8ebd-2d0b-484d-80ae-3623139f0a64", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "a0ef51550e4c76dfb152a776b8246a37ed7c446e12f82bcd5b506d3ae801914a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d26bd8c-28b6-48e0-b848-15bbbcf909be", "node_type": "1", "metadata": {}, "hash": "8f751fc2646374bd3f2218473cf4884f392a219e9323d998ca0319f384da24fc", "class_name": "RelatedNodeInfo"}}, "text": "Training neural networks with \ufb01xed\nsparse masks. In Advances in Neural Infor-\nmation Processing Systems , volume 34, pages\n24193\u201324205. Curran Associates, Inc.\nHugo Touvron, Thibaut Lavril, Gautier Izacard,\nXavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal,\nEric Hambro, Faisal Azhar, et al. 2023. Llama:\nOpen and ef\ufb01cient foundation language models.\narXiv preprint arXiv:2302.13971 .\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In Advances in neural\ninformation processing systems , pages 5998\u2013\n6008.\nTu Vu, Brian Lester, Noah Constant, Rami Al-\nRfou, and Daniel Matthew Cer. 2021. Spot:\nBetter frozen model adaptation through soft\nprompt transfer. In Annual Meeting of the As-\nsociation for Computational Linguistics .\nDanilo Vucetic, Mohammadreza Tayaranian,\nMaryam Ziaeefard, James J. Clark, Brett H.\nMeyer, and Warren J. Gross. 2022. Ef\ufb01cient\n\ufb01ne-tuning of bert models on the edge. 2022\nIEEE International Symposium on Circuits and\nSystems (ISCAS) , pages 1838\u20131842.\nAlex Wang, Yada Pruksachatkun, Nikita Nan-\ngia, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel R Bowman.\n2019. Superglue: A stickier benchmark for\ngeneral-purpose language understanding sys-\ntems. arXiv preprint arXiv:1905.00537 .\nYaqing Wang, Subhabrata Mukherjee, Xiaodong\nLiu, Jing Gao, Ahmed Hassan Awadallah, and\nJianfeng Gao. 2022. Adamix: Mixture-of-\nadapter for parameter-ef\ufb01cient tuning of large\nlanguage models. ArXiv , abs/2205.12410.\n20", "start_char_idx": 2309, "end_char_idx": 3904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d26bd8c-28b6-48e0-b848-15bbbcf909be": {"__data__": {"id_": "1d26bd8c-28b6-48e0-b848-15bbbcf909be", "embedding": null, "metadata": {"page_label": "21", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "721837dc-eeac-49df-ad56-d17a2ad79445", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "751d972ec164bdb49c1688a831c5a4ad1ac08d867c75f441f8d4582bb5c75873", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b22e1b1c-268c-40b9-aa57-08a90f20bb23", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "bc996dc23f9adc1d325af54af3397cb7469a376ee8b73220c944611a31277a14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45c4175e-d0b4-46d1-ab3e-e248d3a3d02e", "node_type": "1", "metadata": {}, "hash": "ebf23fd3be4eb4e20be50a92bcc076f63e0a8e7f3ee63c422f22238adbd509ba", "class_name": "RelatedNodeInfo"}}, "text": "Thomas Wolf, Lysandre Debut, Victor Sanh,\nJulien Chaumond, Clement Delangue, Anthony\nMoi, Perric Cistac, Clara Ma, Yacine Jernite,\nJulien Plu, Canwen Xu, Teven Le Scao, Syl-\nvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander M. Rush. 2020. Transformers:\nState-of-the-Art Natural Language Processing.\npages 38\u201345. Association for Computational\nLinguistics.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan\nWang, Hanyu Lai, Ming Ding, Zhuoyi Yang,\nYifan Xu, Wendi Zheng, Xiao Xia, Weng Lam\nTam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wen-\nguang Chen, Peng Zhang, Yuxiao Dong, and Jie\nTang. 2022. Glm-130b: An open bilingual pre-\ntrained model.\nAston Zhang, Yi Tay, Shuai Zhang, Alvin Chan,\nAnh Tuan Luu, Siu Cheung Hui, and Jie Fu.\n2021. Beyond fully-connected layers with\nquaternions: Parameterization of hypercomplex\nmultiplications with 1/nparameters. In Inter-\nnational Conference on Learning Representa-\ntions .\nSusan Zhang, Stephen Roller, Naman Goyal,\nMikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li,\nXi Victoria Lin, Todor Mihaylov, Myle Ott,\nSam Shleifer, Kurt Shuster, Daniel Simig,\nPunit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open\npre-trained transformer language models.\nYaoming Zhu, Jiangtao Feng, Chengqi Zhao,\nMingxuan Wang, and Lei Li. 2021. Counter-\ninterference adapter for multilingual machine\ntranslation. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021 ,\npages 2812\u20132823, Punta Cana, Dominican Re-\npublic. Association for Computational Linguis-\ntics.\nA Acknowledgements\nWe would like to thank Vladimir Kluenkov and\nVictoria Maltseva for their help with Figure 2.\n21", "start_char_idx": 0, "end_char_idx": 1672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45c4175e-d0b4-46d1-ab3e-e248d3a3d02e": {"__data__": {"id_": "45c4175e-d0b4-46d1-ab3e-e248d3a3d02e", "embedding": null, "metadata": {"page_label": "1", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "805869df-8afb-4fe8-aa9c-f78b9495293d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "9f372273b76f6503de13b10aa3c6e94277055cd3d4590bfa8b50a5d70d49c325", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d26bd8c-28b6-48e0-b848-15bbbcf909be", "node_type": "1", "metadata": {"page_label": "21", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "751d972ec164bdb49c1688a831c5a4ad1ac08d867c75f441f8d4582bb5c75873", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09815e6c-69c5-430a-a7a3-1ad3d7e60cd5", "node_type": "1", "metadata": {}, "hash": "d3dae2c773d51450d6bb2891c16f78a4d5d7a0cb7efdcb04ee4034cba61a7cbc", "class_name": "RelatedNodeInfo"}}, "text": "BloombergGPT: A Large Language Model for Finance\nShijie Wu1,\u2217, Ozan \u02d9Irsoy1,\u2217, Steven Lu1,\u2217, Vadim Dabravolski1, Mark Dredze1,3,\nSebastian Gehrmann1, Prabhanjan Kambadur1, David Rosenberg2, Gideon Mann1\n1Bloomberg, New York, NY USA\n2Bloomberg, Toronto, ON Canada\n3Computer Science, Johns Hopkins University, Baltimore, MD USA\nAbstract\nThe use of NLP in the realm of \ufb01nancial technology is broad and complex, with applications\nranging from sentiment analysis and named entity recognition to question answering. Large\nLanguage Models (LLMs) have been shown to be e\ufb00ective on a variety of tasks; however, no\nLLM specialized for the \ufb01nancial domain has been reported in literature. In this work, we\npresent BloombergGPT , a 50 billion parameter language model that is trained on a wide\nrange of \ufb01nancial data. We construct a 363 billion token dataset based on Bloomberg\u2019s\nextensive data sources, perhaps the largest domain-speci\ufb01c dataset yet, augmented with\n345 billion tokens from general purpose datasets. We validate BloombergGPT on stan-\ndard LLM benchmarks, open \ufb01nancial benchmarks, and a suite of internal benchmarks\nthat most accurately re\ufb02ect our intended usage. Our mixed dataset training leads to a\nmodel that outperforms existing models on \ufb01nancial tasks by signi\ufb01cant margins without\nsacri\ufb01cing performance on general LLM benchmarks. Additionally, we explain our model-\ning choices, training process, and evaluation methodology. We release Training Chronicles\n(Appendix C) detailing our experience in training BloombergGPT .\nContents\n1 Introduction 3\n1.1BloombergGPT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Broader Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2 Dataset 5\n2.1 Financial Datasets (363B tokens \u2013 51.27% of training) . . . . . . . . . . . . 7\n2.1.1 Web (298B tokens \u2013 42.01% of training) . . . . . . . . . . . . . . . . 7\n2.1.2 News (38B tokens \u2013 5.31% of training) . . . . . . . . . . . . . . . . . 7\n2.1.3 Filings (14B tokens \u2013 2.04% of training) . . . . . . . . . . . . . . . . 7\n2.1.4 Press (9B tokens \u2013 1.21% of training) . . . . . . . . . . . . . . . . . 8\n2.1.5 Bloomberg (5B tokens \u2013 0.70% of training) . . . . . . . . . . . . . . 8\n2.2 Public Datasets (345B tokens \u2013 48.73% of training) . . . . . . . . . . . . . . 9\n2.2.1 The Pile (184B tokens \u2013 25.9% of training) . . . . . . . . . . . . . . 9\n2.2.2 C4 (138B tokens \u2013 19.48% of training) . . . . . . . . . . . . . . . . . 9\n2.2.3 Wikipedia (24B tokens \u2013 3.35% of training) . . . . . . . . . . . . . . 9\n2.3 Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\u2217. Co-\ufb01rst authors. Corresponding author email: gmann16@bloomberg.net\n1arXiv:2303.17564v2  [cs.LG]  9 May 2023", "start_char_idx": 0, "end_char_idx": 2747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09815e6c-69c5-430a-a7a3-1ad3d7e60cd5": {"__data__": {"id_": "09815e6c-69c5-430a-a7a3-1ad3d7e60cd5", "embedding": null, "metadata": {"page_label": "2", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "861a8f99-6af7-480e-92bc-93dc949fe274", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c58a595a33685a0874ade751338168ef8e500ddca9f9d15ba5135b13938567f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45c4175e-d0b4-46d1-ab3e-e248d3a3d02e", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "9f372273b76f6503de13b10aa3c6e94277055cd3d4590bfa8b50a5d70d49c325", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38deda85-8093-417b-85de-eb894a6bc0c3", "node_type": "1", "metadata": {}, "hash": "184d06b86931c3007d2fe6bc1fed8206022d7058a0fff124d0f99c630e086169", "class_name": "RelatedNodeInfo"}}, "text": "3 Model 11\n3.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.2 Model Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.3 Training Con\ufb01guration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.4 Large-scale Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4 Training Run 15\n5 Evaluation 16\n5.1 Few-shot Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.2 Heldout Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.3 Financial Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.3.1 External Financial Tasks . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.3.2 Internal Task: Sentiment Analysis . . . . . . . . . . . . . . . . . . . 22\n5.3.3 Exploratory Task: NER . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.4 BIG-bench Hard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n5.5 Knowledge Assessments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n5.6 Reading Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n5.7 Linguistic Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n5.8 Summary . . . . . . . .", "start_char_idx": 0, "end_char_idx": 1301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38deda85-8093-417b-85de-eb894a6bc0c3": {"__data__": {"id_": "38deda85-8093-417b-85de-eb894a6bc0c3", "embedding": null, "metadata": {"page_label": "2", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "861a8f99-6af7-480e-92bc-93dc949fe274", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c58a595a33685a0874ade751338168ef8e500ddca9f9d15ba5135b13938567f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09815e6c-69c5-430a-a7a3-1ad3d7e60cd5", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "faef56a98d2ddb56f01ab051f67e5899adc5fab76cc0aa6455a1b52e586512c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cec8bdf5-d5c6-403c-8476-332ca241c1c4", "node_type": "1", "metadata": {}, "hash": "c30cedb1129d25ca90305aa2ce1ed183ddf9f558981779d20ba721ae79b71648", "class_name": "RelatedNodeInfo"}}, "text": ". . . . . . . . . . . . . . . . . 26\n5.6 Reading Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n5.7 Linguistic Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n5.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n6 Qualitative Samples 31\n7 Related Work 32\n8 Ethics, Limitations, and Implications 37\n8.1 Ethical Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n8.2 Openness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n9 Conclusion 38\nA Architecture 61\nA.0 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\nA.1 Full Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\nA.2 SelfAttention with ALiBi (SA) . . . . . . . . . . . . . . . . . . . . . . . . . 62\nA.3 LayerNorm (LN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nA.4 FeedForwardNetwork (FFN) . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nA.5 List of All Trainable Parameters . . . . . . . . . . . . . . . . . . . . . . . . 64\nB Details on external \ufb01nancial tasks 65\nC Training Chronicles 67\nC.0 Still\n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\nC.1 Elbow\n . . . . . . . . . . . .", "start_char_idx": 1058, "end_char_idx": 2385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cec8bdf5-d5c6-403c-8476-332ca241c1c4": {"__data__": {"id_": "cec8bdf5-d5c6-403c-8476-332ca241c1c4", "embedding": null, "metadata": {"page_label": "2", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "861a8f99-6af7-480e-92bc-93dc949fe274", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c58a595a33685a0874ade751338168ef8e500ddca9f9d15ba5135b13938567f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38deda85-8093-417b-85de-eb894a6bc0c3", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "f789f68ab13c907ecd2a5ccc9a52c45e905be8f95e141fa48df49e7d604606c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3376e3b-0ace-4fed-a31a-542fa5161a23", "node_type": "1", "metadata": {}, "hash": "080b6a6f15e344cfb7ad24b856016bdf172981724ad2af621b7cf143f177e49b", "class_name": "RelatedNodeInfo"}}, "text": ". . . . 63\nA.5 List of All Trainable Parameters . . . . . . . . . . . . . . . . . . . . . . . . 64\nB Details on external \ufb01nancial tasks 65\nC Training Chronicles 67\nC.0 Still\n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\nC.1 Elbow\n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\nC.2 Slide\n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\nC.3 Suspense\n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n2", "start_char_idx": 2097, "end_char_idx": 2617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3376e3b-0ace-4fed-a31a-542fa5161a23": {"__data__": {"id_": "f3376e3b-0ace-4fed-a31a-542fa5161a23", "embedding": null, "metadata": {"page_label": "3", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9cf3eec0-f1d0-4c09-8697-d5e9209aa401", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "9d6ca17d1712cb06898b2163d64074a7fb30d587ff0ea544cf14c6d6633513af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cec8bdf5-d5c6-403c-8476-332ca241c1c4", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "df69d08cae14f355ef7968273e695bdf339a0a9c0bf0e977a06d52b79ad14946", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02a3d26f-6994-4f95-96af-10da10a657e6", "node_type": "1", "metadata": {}, "hash": "afd6a14199904c3dbbd206d9fa1dba6705ac25e84894c1ca30e3af6650a362ef", "class_name": "RelatedNodeInfo"}}, "text": "1 Introduction\nThe release of GPT-3 in 2020 (Brown et al., 2020) demonstrated the powerful bene\ufb01ts\nof training very large auto-regressive language models (LLMs). GPT-3 had 175 billion\nparameters, a hundredfold increase over the previous GPT-2 model, and did remarkably\nwell across a wide range of now popular LLM tasks, including reading comprehension,\nopen-ended question answering, and code generation. This performance has been replicated\nacross several other models (Chowdhery et al., 2022; Scao et al., 2022; Zhang et al., 2022a).\nFurthermore, evidence suggests that large models exhibit emergent behaviors; growth allows\nthem to acquire abilities not present in smaller models (Wei et al., 2022a). A notable\nexample of emergent behavior is the ability to perform tasks via few-shot prompting, where a\nmodel can learn a task from just a few examples. This ability improves well-above random as\nwe increase the size of language models. Broadly speaking, few-shot prompting dramatically\nexpands the range of tasks supported by models and lowers the barrier to entry for users\nseeking automation for new language tasks.\nAfter GPT-3, models grew in size to 280 billion (Gopher, Rae et al., 2021), 540 bil-\nlion (PaLM, Chowdhery et al., 2022), and 1 trillion parameters (Megatron, Korthikanti\net al., 2022). Work also explored other important aspects of achieving a high-performing\nLLM, such as di\ufb00erent training objectives (Tay et al., 2022b), multilingual models (Scao\net al., 2022), more e\ufb03cient and smaller models (Black et al., 2022), and \ufb01nding data and\nparameter-e\ufb03cient training sizes (Ho\ufb00mann et al., 2022).\nThese e\ufb00orts have almost exclusively focused on general LLMs, trained on datasets\nthat cover a broad range of topics and domains. While these have included some datasets\nfor specialized domains (e.g., code (Chen et al., 2021a) or biomedical articles (Gao et al.,\n2021)) the focus has been on building LLMs with broad capabilities. Recent e\ufb00orts training\nmodels using only domain-speci\ufb01c data have yielded models that, while much smaller, beat\ngeneral purpose LLMs on tasks within those domains, such as science (Taylor et al., 2022)\nand medicine (Bolton et al., 2023; Luo et al., 2022; Lehman et al., 2023). These \ufb01ndings\nmotivate further development of models focused on speci\ufb01c domains.\nFinancial Technology (FinTech) is a large and growing area with NLP technologies\nhaving an increasingly important role (Xing et al., 2018; Fisher et al., 2016; Dredze et al.,\n2016). Financial NLP tasks (Shah et al., 2022) include sentiment analysis (Araci, 2019),\nnamed entity recognition (Salinas Alvarado et al., 2015), news classi\ufb01cation (Sinha and\nKhandait, 2020), and question answering (Chen et al., 2021b, 2022). While the range of\ntasks is similar to those found in general NLP benchmarks, the complexity and terminology\nof the \ufb01nancial domain warrant a domain-speci\ufb01c system. For all of the reasons generative\nLLMs are attractive in general \u2013 few-shot learning, text generation, conversational systems,\netc. \u2013 it would be valuable to have a LLM focused on the \ufb01nancial domain. While there are\nmasked language models tuned for the \ufb01nancial domain (Araci, 2019), no LLM has been\ntuned for or evaluated on tasks for this domain.\n1.1 BloombergGPT\nWe train BloombergGPT , a 50 billion parameter language model that supports a wide\nrange of tasks within the \ufb01nancial industry. Rather than building a general-purpose LLM,\nor a small LLM exclusively on domain-speci\ufb01c data, we take a mixed approach. General\n3", "start_char_idx": 0, "end_char_idx": 3516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02a3d26f-6994-4f95-96af-10da10a657e6": {"__data__": {"id_": "02a3d26f-6994-4f95-96af-10da10a657e6", "embedding": null, "metadata": {"page_label": "4", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f881f6b9-c2ef-4b8b-8e9c-f288d43d1bea", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bd619f83a0fdc61610dd778f349329dd9efcc39aaf95330a2a859b6ed6cc74df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3376e3b-0ace-4fed-a31a-542fa5161a23", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "9d6ca17d1712cb06898b2163d64074a7fb30d587ff0ea544cf14c6d6633513af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac3055d4-bb2f-4a87-ab39-ffc64121cede", "node_type": "1", "metadata": {}, "hash": "b7abad33b9d44566b2c48bc00bea94860b13f771b31b0e525222a4dd89752fcd", "class_name": "RelatedNodeInfo"}}, "text": "models cover many domains, are able to perform at a high level across a wide variety of tasks,\nand obviate the need for specialization during training time. However, results from existing\ndomain-speci\ufb01c models show that general models cannot replace them. At Bloomberg, we\nsupport a very large and diverse set of tasks, well served by a general model, but the vast\nmajority of our applications are within the \ufb01nancial domain, better served by a speci\ufb01c\nmodel. For that reason, we set out to build a model that achieves best-in-class results on\n\ufb01nancial benchmarks, while also maintaining competitive performance on general-purpose\nLLM benchmarks.\nWe achieve this goal by constructing the largest domain-speci\ufb01c dataset yet, drawing on\nexisting data creation, collection, and curation resources at Bloomberg. As Bloomberg is\nprimarily a \ufb01nancial data company, our data analysts have collected and curated \ufb01nancial\nlanguage documents over the span of forty years. We have extensive archives of \ufb01nancial\ndata that cover a range of topics, with careful tracking of data sources and usage rights. We\nadd this data to public datasets to create a large training corpus with over 700 billion tokens.\nUsing a portion of this training corpus, we train a BLOOM-style, 50 billion parameter\nmodel designed based on guidelines from Ho\ufb00mann et al. (2022) and Le Scao et al. (2022).\nWe validate the model on standard LLM benchmarks, open \ufb01nancial benchmarks, and a\nsuite of Bloomberg-internal benchmarks that most accurately re\ufb02ect our intended use cases.\nOur results demonstrate that our mixed training approach leads to a model that vastly\noutperforms existing models on in-domain \ufb01nancial tasks while being on par or better on\ngeneral NLP benchmarks.\n1.2 Broader Contributions\nBeyond the construction of a LLM for \ufb01nancial data, our goal is to contribute to the\nbroader research community. Speci\ufb01cally, our experience documented in this paper provides\nevidence that further develops the community\u2019s understanding of several open questions in\nthe literature.\nDomain-speci\ufb01c LLMs. The few existing domain-speci\ufb01c LLMs are trained exclusively\non domain-speci\ufb01c data sources (Luo et al., 2022; Bolton et al., 2023; Taylor et al., 2022),\nor adapt a very large general purpose model to domain-speci\ufb01c tasks (Singhal et al., 2022;\nLewkowycz et al., 2022). Our alternative approach \u2013 training an LLM on both domain-\nspeci\ufb01c and general data sources \u2013 has not been studied so far. The resulting model does very\nwell on domain-speci\ufb01c tasks, but also maintains strong performance on general-purpose\nbenchmarks.\nTraining data. Nearly all language models rely in large part on web-scraped data, such\nas C4 (Ra\ufb00el et al., 2020) and The Pile (Gao et al., 2021) (which includes OpenWebText2).\nThis data may be cleaned or subsetted in various ways before use (Touvron et al., 2023; Rae\net al., 2020; Scao et al., 2022; Jernite et al., 2022), but issues of data duplication (Carlini\net al., 2020) and toxic language remain (Welbl et al., 2021). Our training data is unusual\nfor LLM training in that it includes a signi\ufb01cant amount of curated and prepared data from\nreliable sources.\nEvaluation. LLM evaluation remains a challenging and evolving problem (Gehrmann\net al., 2022; Goyal et al., 2022), with new benchmarks trying to standardize evaluation\n4", "start_char_idx": 0, "end_char_idx": 3322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac3055d4-bb2f-4a87-ab39-ffc64121cede": {"__data__": {"id_": "ac3055d4-bb2f-4a87-ab39-ffc64121cede", "embedding": null, "metadata": {"page_label": "5", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2a03fce-b57d-422c-8109-c9debb279d07", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "7fba2417fe74db88fc2e2499807aad1c3fc6598d83932110577e5cb2bb6b07d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02a3d26f-6994-4f95-96af-10da10a657e6", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bd619f83a0fdc61610dd778f349329dd9efcc39aaf95330a2a859b6ed6cc74df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35c8874c-aa7d-4995-a294-1f53a09420d9", "node_type": "1", "metadata": {}, "hash": "ac2178f6f6c153e43e78c1fc9071a2146dadafd53cfcb174596e5a768cf2fd36", "class_name": "RelatedNodeInfo"}}, "text": "across models (Liang et al., 2022; Srivastava et al., 2022). However, for domain-speci\ufb01c\ntasks, there remains a mismatch between evaluation and actual use cases. Evaluations are\nbuilt on available datasets and not necessarily on how the model will be used in practice.\nWe provide results on both public \ufb01nancial NLP benchmarks (Shah et al., 2022; Chen et al.,\n2021b) as well as a selection of internal Bloomberg tasks, which are better aligned with our\nintended use cases and directly evaluate our model\u2019s ability to perform tasks of interest.\nModel Size. Early LLMs made a single training pass over a corpus of 200-400 billion to-\nkens (Brown et al., 2020) and Ho\ufb00mann et al. (2022) posited that models were undertrained,\ninstead focusing on training smaller models with more data, a strategy most recently em-\nployed by Touvron et al. (2023). We select a model size motivated by Ho\ufb00mann et al. (2022)\nand train a 50 billion parameter model on 569 billion tokens from our corpus of over 700\nbillion tokens to produce a model that is competitive with larger models.\nTokenizer. After assembling training data, the critical step of tokenization transforms\nthe text into a format suitable for the language model. The importance of this step is\noften overlooked (Mielke et al., 2021), and many older LLMs use the same tokenizer and\nvocabulary, meaning that we have little evidence to support other tokenizers. We take\na di\ufb00erent approach and use a Unigram model instead of greedy merge-based sub-word\ntokenizers since it saves probabilities allowing for smarter tokenization at inference time\n(Kudo, 2018).\nModel Building Challenges. GPT-3 and subsequent models were the work of large\nteams and required an enormous amount of computation. Initial work to reproduce these\nresults, such as OPT (Zhang et al., 2022a), did not match the performance of the original\nmodel. With the release of each subsequent model, the community\u2019s understanding, ex-\nperience, and software tools increase. In developing BloombergGPT , we bene\ufb01ted from\nexisting code developed as part of the BLOOM e\ufb00ort (Scao et al., 2022), showing that a\nmoderately sized team can produce a competitive model on domain-speci\ufb01c data. We de-\nscribe our experiences training BloombergGPT in detail to support future training e\ufb00orts\nand address each of the above topics.\n2 Dataset\nTo train BloombergGPT , we construct \u201c FinPile \u201d, a comprehensive dataset consisting of\na range of English \ufb01nancial documents including news, \ufb01lings, press releases, web-scraped \ufb01-\nnancial documents, and social media drawn from the Bloomberg archives. These documents\nhave been acquired through our business process over the past two decades. We augment\nFinPile with public data widely used to train LLMs. The result is a training corpus that\nis roughly half domain-speci\ufb01c text and half general-purpose text. For a breakdown of the\nfull training set, see Table 1. To improve data quality, we de-duplicate each dataset (The\nPile, C4, Wikipedia, FinPile ) according to Lee et al. (2022a); as a side-e\ufb00ect, the statistics\nreported in Table 1 might be di\ufb00erent from those reported in other papers.\n5", "start_char_idx": 0, "end_char_idx": 3132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35c8874c-aa7d-4995-a294-1f53a09420d9": {"__data__": {"id_": "35c8874c-aa7d-4995-a294-1f53a09420d9", "embedding": null, "metadata": {"page_label": "6", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68d67c24-2ade-4700-9f1b-0ae06f0c7b48", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "78fd41ffe148644bf31cb0783d22f4d805566d84289aa3dd4ab08d230162aa36", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac3055d4-bb2f-4a87-ab39-ffc64121cede", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "7fba2417fe74db88fc2e2499807aad1c3fc6598d83932110577e5cb2bb6b07d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8100895b-27cf-48c5-83e2-92b03cad7595", "node_type": "1", "metadata": {}, "hash": "f567c3cb5e3c5f90cdf8579ff894314a24f0ccbee0380d7ce8fb185619ab341f", "class_name": "RelatedNodeInfo"}}, "text": "DatasetDocs\n1e4C/DChars\n1e8C/TToks\n1e8T%\nFinPile 175,886 1,017 17,883 4.92 3,635 51.27%\nWeb 158,250 933 14,768 4.96 2,978 42.01%\nNews 10,040 1,665 1,672 4.44 376 5.31%\nFilings 3,335 2,340 780 5.39 145 2.04%\nPress 1,265 3,443 435 5.06 86 1.21%\nBloomberg 2,996 758 227 4.60 49 0.70%\nPUBLIC 50,744 3,314 16,818 4.87 3,454 48.73%\nC4 34,832 2,206 7,683 5.56 1,381 19.48%\nPile-CC 5,255 4,401 2,312 5.42 427 6.02%\nGitHub 1,428 5,364 766 3.38 227 3.20%\nBooks3 19 552,398 1,064 4.97 214 3.02%\nPubMed Central 294 32,181 947 4.51 210 2.96%\nArXiv 124 47,819 591 3.56 166 2.35%\nOpenWebText2 1,684 3,850 648 5.07 128 1.80%\nFreeLaw 349 15,381 537 4.99 108 1.52%\nStackExchange 1,538 2,201 339 4.17 81 1.15%\nDM Mathematics 100 8,193 82 1.92 43 0.60%\nWikipedia (en) 590 2,988 176 4.65 38 0.53%\nUSPTO Backgrounds 517 4,339 224 6.18 36 0.51%\nPubMed Abstracts 1,527 1,333 204 5.77 35 0.50%\nOpenSubtitles 38 31,055 119 4.90 24 0.34%\nGutenberg (PG-19) 3 399,351 112 4.89 23 0.32%\nUbuntu IRC 1 539,222 56 3.16 18 0.25%\nEuroParl 7 65,053 45 2.93 15 0.21%\nYouTubeSubtitles 17 19,831 33 2.54 13 0.19%\nBookCorpus2 2 370,384 65 5.36 12 0.17%\nHackerNews 82 5,009 41 4.87 8 0.12%\nPhilPapers 3 74,827 23 4.21 6 0.08%\nNIH ExPorter 92 2,165 20 6.65 3 0.04%\nEnron Emails 24 1,882 5 3.90 1 0.02%\nWikipedia (7/1/22) 2,218 3,271 726 3.06 237 3.35%\nTOTAL 226,631 1,531 34,701 4.89 7,089 100.00%\nTable 1: Breakdown of the full training set used to train BloombergGPT . The statistics\nprovided are the average number of characters per document (\u201cC/D\u201d), the average\nnumber of characters per token (\u201cC/T\u201d), and the percentage of the overall tokens\n(\u201cT%\u201d). Units for each column are denoted in the header.\n6", "start_char_idx": 0, "end_char_idx": 1663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8100895b-27cf-48c5-83e2-92b03cad7595": {"__data__": {"id_": "8100895b-27cf-48c5-83e2-92b03cad7595", "embedding": null, "metadata": {"page_label": "7", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b429f3d3-930f-4efe-8130-a0b91c905db2", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "68d972f40963acaca1f0dacdd980aeed5a4965be79297f7bfa2c31e420369d65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35c8874c-aa7d-4995-a294-1f53a09420d9", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "78fd41ffe148644bf31cb0783d22f4d805566d84289aa3dd4ab08d230162aa36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "014e6ef1-9fdd-4594-8824-dbbacda770b9", "node_type": "1", "metadata": {}, "hash": "ab1e2a0c82b989635bedab97e221f19150a0b2c2d5f3db6c04c9754ea7ba26cf", "class_name": "RelatedNodeInfo"}}, "text": "2.1 Financial Datasets (363B tokens \u2013 51.27% of training)\nThe Bloomberg Terminal has provided access to a comprehensive set of diverse structured\nand unstructured \ufb01nancial data and analytics for the past four decades. In serving this\nmission, Bloomberg analysts have curated a set of \ufb01nancial documents that were either\ncreated internally or acquired from external sources. We utilize this extensive collection of\ncurated and maintained documents to create FinPile , which consists of company \ufb01lings,\n\ufb01nancial news, and other data relevant to the \ufb01nancial markets.\nSome documents included in the FinPile , such as company \ufb01lings, are available to\nthe general public, although collecting these documents and pre-processing them for LLM\ntraining is a non-trivial task. Other documents, such as (a subset of) Bloomberg news, must\nbe purchased. The rest of the documents are private and available, among other sources,\nthrough the Bloomberg Terminal. Finally, we clean this data to strip o\ufb00 markup, special\nformatting, and templates.\nNote that each document in FinPile is time-stamped, with dates ranging from 2007-\n03-01 to 2022-07-31; the quality and quantity of documents increase over this time range.\nWhile we do not utilize date information in this work, we plan to use it in the future,\nsuch as for evaluation of what the model learns about di\ufb00erent time periods. While we\ncannot release FinPile , our experience training on a large, carefully curated, and clean\ndomain-speci\ufb01c dataset may provide helpful insights to the community on the advantages\nand challenges of building a \ufb01nancial LLM in particular, and a domain-speci\ufb01c model in\ngeneral. We provide a breakdown and analysis of FinPile in Table 2 and a brief description\nof the types of data included below.\n2.1.1 Web (298B tokens \u2013 42.01% of training)\nBloomberg collects web content by identifying sites that contain \ufb01nancially relevant infor-\nmation. While this category makes up the majority of FinPile , its classi\ufb01cations are rough,\nwith content classi\ufb01ed mainly by the location of the web domain. Within these location-\nspeci\ufb01c sources, e.g. \u201cUS\u201d (15.95% of total), \u201cAsia-Pac\u201d (4.72% of total), and \u201cUK\u201d (1.98%\nof total), document types are highly varied as would be expected from a web crawl. While\nweb sources are common in existing public LLM training datasets, Bloomberg\u2019s web crawl\nis focused on high-quality websites that have \ufb01nancially relevant information, as opposed\nto a general-purpose crawl of the web.\n2.1.2 News (38B tokens \u2013 5.31% of training)\nThe News category includes all news sources excluding news articles written by Bloomberg\njournalists. Overall, there are hundreds of English news sources in FinPile including\n\u201cBloomberg Transcripts\u201d (0.41% of total), which are transcripts of Bloomberg TV news.\nGenerally, the content in this dataset comes from reputable sources of news that are relevant\nto the \ufb01nancial community so as to maintain factuality and reduce bias.\n2.1.3 Filings (14B tokens \u2013 2.04% of training)\nCompany Filings are \ufb01nancial statements prepared by (public) companies and made avail-\nable to the general public. In some countries, like the US, public companies are mandated\n7", "start_char_idx": 0, "end_char_idx": 3178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "014e6ef1-9fdd-4594-8824-dbbacda770b9": {"__data__": {"id_": "014e6ef1-9fdd-4594-8824-dbbacda770b9", "embedding": null, "metadata": {"page_label": "8", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5bf04aab-e0bb-4ea3-a49b-bfab1f38af92", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "a967165eac7f2af22312099695e63feb475711b574ce19291abf45371db69ed0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8100895b-27cf-48c5-83e2-92b03cad7595", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "68d972f40963acaca1f0dacdd980aeed5a4965be79297f7bfa2c31e420369d65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14981466-f049-4a81-868c-9eaf9a0cb562", "node_type": "1", "metadata": {}, "hash": "b5eaa0aed23935462a9bbe1a21a1591feddfcfa0bffc0c41eb07ff66a96a3d1f", "class_name": "RelatedNodeInfo"}}, "text": "Date Bloomberg Filings News Press Web Total\n2007 [03-] 276 73 892 523 2,667 4,431\n2008 351 91 1,621 628 9,003 11,695\n2009 293 93 1,791 528 9,179 11,883\n2010 292 111 1,917 527 11,388 14,236\n2011 335 117 2,264 548 13,643 16,907\n2012 403 105 2,502 529 15,015 18,554\n2013 415 87 2,437 441 17,230 20,610\n2014 396 251 2,458 437 18,510 22,052\n2015 358 1,639 2,371 427 20,782 25,576\n2016 324 1,891 2,509 418 24,337 29,478\n2017 294 2,294 2,567 398 25,283 30,837\n2018 275 1,791 2,702 420 26,027 31,214\n2019 263 1,662 3,102 504 27,195 32,726\n2020 277 1,632 2,794 805 30,928 36,435\n2021 247 1,767 3,515 938 29,749 36,215\n2022 [-07] 140 882 2,206 531 16,872 20,631\n4,939 14,486 37,647 8,602 297,807 363,482\nTable 2: The number of tokens (in millions) contained within documents in FinPile , or-\nganized by year (rows) and type (column). Units are millions of tokens.\nto prepare and submit their \ufb01nancial statements on a regular cadence; e.g., 10-K annual\nreports and 10-Q quarterly reports. In our dataset, a majority of the \ufb01lings come from\nEDGAR, which is the SEC\u2019s online database (1.90% of total). Filings are typically long\nPDF documents with tables and charts that are dense in \ufb01nancial information, which are\nprocessed and normalized in Bloomberg. Filings are substantially di\ufb00erent from the types\nof documents typically used to train LLMs, but contain critically important information for\n\ufb01nancial decision-making.\n2.1.4 Press (9B tokens \u2013 1.21% of training)\nThe Press category contains press releases typically issued by companies that are \ufb01nancially\nrelevant. Taken together with \ufb01lings, press releases represent most of the public communi-\ncations of a company. However, unlike \ufb01lings, press releases are similar to news stories in\nterms of content and style.\n2.1.5 Bloomberg (5B tokens \u2013 0.70% of training)\nThis category comprises Bloomberg authored news and other documents such as opinions\nand analyses. The largest sources are \u201cBloomberg News\u201d (0.44% of total) and \u201cBloomberg\nFirst Word\u201d (0.13% of total), the Bloomberg-authored wire of real-time news. While\nBloomberg News covers a wide range of topics, it typically focuses on content relevant\nto the \ufb01nancial community. This dataset contains documents of varying lengths.\n8", "start_char_idx": 0, "end_char_idx": 2228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14981466-f049-4a81-868c-9eaf9a0cb562": {"__data__": {"id_": "14981466-f049-4a81-868c-9eaf9a0cb562", "embedding": null, "metadata": {"page_label": "9", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c252b906-0c44-4de1-a2e7-4fb5fa849b07", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "be50953d8bbc649d56bfcd189645a187c778bbeeb2bf9b28b42f9d238dc781eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "014e6ef1-9fdd-4594-8824-dbbacda770b9", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "a967165eac7f2af22312099695e63feb475711b574ce19291abf45371db69ed0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4344cb65-d9e8-42ab-9d34-1b7227c48170", "node_type": "1", "metadata": {}, "hash": "6800f43f63417a9cfedb21a28e6d876ff623ba3b1c7a2fda8fcd693ecfd498d8", "class_name": "RelatedNodeInfo"}}, "text": "2.2 Public Datasets (345B tokens \u2013 48.73% of training)\nWe use three widely known and available public datasets in our training corpus.\n2.2.1 The Pile (184B tokens \u2013 25.9% of training)\nThe Pile (Gao et al., 2021) is the dataset used in GPT-Neo (Black et al., 2021), GPT-\nJ (Wang and Komatsuzaki, 2021), and GPT-NeoX (20B) (Black et al., 2022). We include\nThe Pile in our training data for the following reasons. First, it has been used to successfully\ntrain an LLM. Second, it has undergone signi\ufb01cant data cleaning and pre-processing. Third,\nit includes multiple domains and we believe such diverse data will aid generalization to new\ndomains and may even support training on \ufb01nancial data. For example, domains such as\nFreeLaw and GitHub are useful to teams at Bloomberg that work on legal documents and\nsoftware development, respectively. Creators of The Pile have deliberately chosen to include\nduplicate content, with the duplication factor being proportional to the perceived quality\nof the content. However, as we deduplicate each of our datasets, the size of The Pile is\nsigni\ufb01cantly reduced. Additionally, note that our tokenizer ( \u00a72.3) is trained on The Pile.\n2.2.2 C4 (138B tokens \u2013 19.48% of training)\nThe Colossal Clean Crawled Corpus (C4) is a common dataset used to train LLMs, and was\nintroduced to support training T5 (Ra\ufb00el et al., 2020). Although it overlaps with Pile-CC,\nC4 is cleaned and processed di\ufb00erently; hence, we feel that including C4 in addition to\nThe Pile can add value more than duplicated documents would. We \ufb01nd that C4 contains\nhigh-quality natural language documents due to the layers of cleaning, though others have\nnoted that the distribution across web domains is unusual, with a high fraction of data\nstemming from patents (Dodge et al., 2021).\n2.2.3 Wikipedia (24B tokens \u2013 3.35% of training)\nBoth The Pile and C4 include out-of-date copies of Wikipedia, so it could be bene\ufb01cial for\nthe factuality of the model to have up-to-date Wikipedia pages included. Therefore, we\ninclude a dump of English Wikipedia from July 1, 2022. This dataset is tokenized quite\nine\ufb03ciently (3.06 characters per token), indicating an above-average amount of markup,\nwhich suggests that further cleaning might bene\ufb01t future model training.\n2.3 Tokenization\nWe choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based sub-word\ntokenizer, such as Byte Pair Encoding (BPE) (Sennrich et al., 2016) or Wordpiece (Schuster\nand Nakajima, 2012; Wu et al., 2016), based on promising results in Kudo and Richardson\n(2018) and Bostrom and Durrett (2020). Following GPT-2 (Radford et al., 2019), we treat\nour data as a sequence of bytes rather than Unicode characters, and we include each of the\n256 bytes as tokens. In a pretokenization step, the input byte sequence is broken into chunks\nby greedily matching the following regular expression: [ A-Za-z]+|[0-9]|[^A-Za-z0-9]+ .\nThis follows GPT-2 in preventing multiple character classes from appearing in a single token.\nHowever, we include spaces in the alphabetic chunks, which allows multi-word tokens to be\nlearned, increasing information density and reducing context lengths. The pretokenization\n9", "start_char_idx": 0, "end_char_idx": 3179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4344cb65-d9e8-42ab-9d34-1b7227c48170": {"__data__": {"id_": "4344cb65-d9e8-42ab-9d34-1b7227c48170", "embedding": null, "metadata": {"page_label": "10", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1352167c-6c9a-4aa0-a0b5-9e86ded06817", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "af0fa96651385389625bfbfc9680beb83db1f07ab066c64ba4a117686d0abe21", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14981466-f049-4a81-868c-9eaf9a0cb562", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "be50953d8bbc649d56bfcd189645a187c778bbeeb2bf9b28b42f9d238dc781eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ad3be36-b736-499a-a6f0-2d1312201665", "node_type": "1", "metadata": {}, "hash": "e0a6790a364853f3659f5f8f65f0d8dcdc40f24829070fe76a5b7c1357daa34d", "class_name": "RelatedNodeInfo"}}, "text": "BLOOM /ours NeoX /ours OPT /ours BloombergGPT\nFinPile (old) 451 110% 460 112% 456 111% 412\nC4 166 121% 170 123% 170 123% 138\nThe Pile 203 110% 214 116% 239 130% 184\nWikipedia 21 88% 23 99% 24 103% 24\nTotal 390 113% 408 118% 434 126% 345\nTable 3: Number of tokens in each training dataset with BLOOM, NeoX, OPT (GPT2),\nandBloombergGPT tokenizers. All token counts are in billions (B). Note that\nan older version of FinPile was used for this count, so token numbers will not\nmatch earlier tables.\nfollows the approach of PaLM (Chowdhery et al., 2022) in placing each digit in its own\nchunk, with the hope that this will lead to better handling of numbers. We train our\ntokenizer on The Pile (Gao et al., 2021) as it draws from diverse domains, including code\nand academic papers, in proportions that suit our use case.\nParallel Tokenizer Training. The Unigram tokenizer implementation is too ine\ufb03cient\nto process the entire Pile dataset at once, so we use a split and merge approach. We split\neach of the 22 domains in the Pile into 256 chunks of roughly equal size. We then train\na Unigram tokenizer with a vocabulary size of 65,536 (216) on each of the 22 \u00d7256 (total\n= 5,632) chunks. We hierarchically merge the individual tokenizers by \ufb01rst merging the\n256 tokenizers from each domain, and then combining the 22 resulting tokenizers to get the\n\ufb01nal tokenizer.\nUnigram tokenizers amount to probability distributions over tokens (i.e. unigram lan-\nguage models), and we merge tokenizers by taking a weighted average of the probabilities\nof corresponding tokens, with the weights determined by the relative sizes (in bytes) of\nthe data used to train the tokenizers. The result is a tokenizer with 7 million tokens. To\nreduce the size of the vocabulary to 217tokens, we drop the tokens with the smallest prob-\nabilities and renormalize. To ensure we do not need an out-of-vocabulary token, we also\nadd as tokens the 36 (of 256 possible) bytes that do not occur in The Pile, along with an\n<|endoftext|> token.\nThere are various considerations in choosing the vocabulary size. One advantage of a\nlarge vocabulary for LLMs is that more information can \ufb01t into the context window. On\nthe other hand, there is overhead with a larger vocabulary: a larger proportion of model\nparameters are required for token embedding. We select our vocabulary size of 217tokens\nbased on experiments with vocabulary ranging from 25,000 to 550,000. For each vocabulary\nsize, we tokenize the C4 dataset and compute the total size (in bytes) for the dataset, where\neach token is represented using log2(vocabulary size) bits. Our heuristic is to choose the\nvocabulary size that leads to the smallest encoded representation of C4. This gives us a\nvocabulary size of 125,000, which we then round up to the nearest power of 2 (217, or 131,072\ntokens). Our tokenizer is large, relative to the standard vocabulary size of approximately\n50,000 tokens. For an analysis of tokenization e\ufb03ciency, see Table 3.\n10", "start_char_idx": 0, "end_char_idx": 2975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ad3be36-b736-499a-a6f0-2d1312201665": {"__data__": {"id_": "7ad3be36-b736-499a-a6f0-2d1312201665", "embedding": null, "metadata": {"page_label": "11", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14dd126d-88c1-4a36-ae5c-481eb211bde1", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "91842c00afcc23052ab54a4cc446848f1b81d2805380675214c6f1fe757b1cef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4344cb65-d9e8-42ab-9d34-1b7227c48170", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "af0fa96651385389625bfbfc9680beb83db1f07ab066c64ba4a117686d0abe21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad880d7e-4eea-4819-a6fb-174b059e8d8b", "node_type": "1", "metadata": {}, "hash": "64361f73ea4148104b1a7691ef00f5a7e61f6f9aac5f03dfc8fa4e20e07a88cb", "class_name": "RelatedNodeInfo"}}, "text": "Shape\nNumber of Layers 70\nNumber of Heads 40\nVocabulary Size 131,072\nHidden Dimension 7,680\nTotal Parameters 50.6B\nHyperparameters\nMax Learning Rate 6e-5\nFinal Learning Rate 6e-6\nLearning Rate schedule cosine decay\nGradient Clipping 0.3\nTraining\nTokens 569B\nHardware 64 \u00d78 A100 40GB\nThroughput 32.5 sec/step\navg. TFLOPs 102\ntotal FLOPS 2.36e23\nTable 4: A summary of the hyper-parameters and their values for BloombergGPT .\n3 Model\n3.1 Architecture\nOur model is a decoder-only causal language model based on BLOOM (Scao et al., 2022).\nWe present an overview of the architecture, with full details in Appendix A.\nThe model contains 70 layers of transformer decoder blocks de\ufb01ned as follows:\n\u00afh\u2113=h\u2113\u22121+ SA(LN(h\u2113\u22121))\nh\u2113=\u00afh\u2113+ FFN(LN( \u00afh\u2113))\nwhere SA is multi-head self-attention, LN is layer-normalization, and FFN is a feed-forward\nnetwork with 1-hidden layer. Inside FFN, the non-linear function is GELU (Hendrycks and\nGimpel, 2016). ALiBi positional encoding is applied through additive biases at the self-\nattention component of the transformer network (Le Scao et al., 2022). The input token\nembeddings are tied to the linear mapping before the \ufb01nal softmax. Following Le Scao\net al. (2022) and \ufb01rst used in Dettmers et al. (2022), the model has an additional layer\nnormalization after token embeddings, formally:\n\u00afh1= LNem(h0) + SA(LN(LNem(h0))),\nwhereh0is the initial token embedding and LNemis the new component of embedding layer-\nnormalization. Notice that the second term includes two consecutive layer-normalizations.\n11", "start_char_idx": 0, "end_char_idx": 1525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad880d7e-4eea-4819-a6fb-174b059e8d8b": {"__data__": {"id_": "ad880d7e-4eea-4819-a6fb-174b059e8d8b", "embedding": null, "metadata": {"page_label": "12", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8132d26d-422e-4746-921e-709ef0ebef6b", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d7eb559bb006b2cfed0aa1876470fc452809557541b93b15e6ebcb4fba76fe8b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ad3be36-b736-499a-a6f0-2d1312201665", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "91842c00afcc23052ab54a4cc446848f1b81d2805380675214c6f1fe757b1cef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceb1b3b9-7688-4146-a10b-f77092b54bcc", "node_type": "1", "metadata": {}, "hash": "5907801a9c4d7cd30d5f704972bb09e427eea9107b94e2e17c3dee3d72d29fb9", "class_name": "RelatedNodeInfo"}}, "text": "1e22 3.2e22 1e23 3.2e23 1e24 3.2e24\nFLOPs10205010020050010002000Parameters (B)\nNeoX LaMDA GPT-3/Jurassic/OPT \nOPT  GopherMT-NLG \n BLOOM\nPaLM PaLM \n Chinchilla LLaMA\nLLaMA \nLLaMA BloombergGPT Optimal # Parameters w.r.t. FLOPs\nChinchilla-1\nChinchilla-2\nChinchilla-3\nKaplan\n1e22 3.2e22 1e23 3.2e23 1e24 3.2e24\nFLOPs100200500100020005000Tokens (B) NeoX\n LaMDA GPT-3/Jurassic/OPT  OPT  Gopher MT-NLG BLOOM PaLM  PaLM ChinchillaLLaMA  LLaMA  \n LLaMA\n BloombergGPTOptimal # Tokens w.r.t. FLOPs\nChinchilla-1\nChinchilla-2\nChinchilla-3\nKaplanFigure 1: Kaplan et al. (2020) and Chinchilla scaling laws with prior large language model\nandBloombergGPT parameter and data sizes. We adopt the style from Ho\ufb00-\nmann et al. (2022).\n3.2 Model Scaling\nSize. The size of our model is based on Chinchilla scaling laws (Ho\ufb00mann et al., 2022), in\nparticular their Approach 1 and Approach 2. We start with a total compute budget of 1.3M\nGPU hours on 40GB A100 GPUs. Since we adopt activation checkpointing to reduce our\nmemory footprint, this costs us an additional 0.33x TFLOPs per iteration due to repeated\nforward passes. To account for this additional cost, we plug in 0.75 \u00d71.3M into Chinchilla\nequations instead of the full amount.\nFrom Ho\ufb00mann et al. (2022), we use the data reported in Table 3 for Approach 1 and\nTable A3 for Approach 2, and \ufb01t regression lines to their log-scaled versions. This gives us:\nApproach 1 Parameters = exp10(log10(FLOPs )\u00b70.498\u22121.004) = 52.993B\nTokens = exp10(log10(FLOPs )\u00b70.502 + 0.229) = 1111 .112B\nApproach 2 Parameters = exp10(log10(FLOPs )\u00b70.490\u22120.839) = 49.753B\nTokens = exp10(log10(FLOPs )\u00b70.510 + 0.062) = 1175 .766B\nThese calculations imply that our dataset of /tildelow700B tokens is too small for a \u201cChinchilla\noptimal\u201d con\ufb01guration given our compute budget (assuming just one pass through the\ndata).1While we can increase the amount of general-purpose training data, we are limited\nin the amount of domain-speci\ufb01c training data at our disposal. FinPile is already among\nthe largest domain-speci\ufb01c training sets, and we do not want it to represent less than half\nof our total training.\n1. The scaling law derived by Chinchilla is tokenizer-speci\ufb01c. Our tokenizer can encode the same document\nmore compactly due to the support of multi-word expressions and the larger vocabulary size. It\u2019s still an\nopen question how well these scaling laws transfer across tokenizers, and how vocabulary size impacts\ntoken and parameter trade-o\ufb00s assuming \ufb01xed compute. We leave this exploration to future work.\n12", "start_char_idx": 0, "end_char_idx": 2521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ceb1b3b9-7688-4146-a10b-f77092b54bcc": {"__data__": {"id_": "ceb1b3b9-7688-4146-a10b-f77092b54bcc", "embedding": null, "metadata": {"page_label": "13", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c376a09f-b21c-48dc-bbd6-16ce3eda517d", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "6745d6dd75e369d4377105caeba932ed49f9374e0406ed52836402ed96c591e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad880d7e-4eea-4819-a6fb-174b059e8d8b", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d7eb559bb006b2cfed0aa1876470fc452809557541b93b15e6ebcb4fba76fe8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee04aa4e-58e9-4da7-99c2-8eabd3fe3b70", "node_type": "1", "metadata": {}, "hash": "52a28aefcad90cced4dc5ba05d0dbf04e69dad2475aee9c6b59789add1d6032e", "class_name": "RelatedNodeInfo"}}, "text": "Since we are data limited, we choose the largest model that we can, while ensuring that\nwe can train on all our tokens and still leave /tildelow30% of the total compute budget as a bu\ufb00er\nfor unforeseen failures, retries, and restarts. This leads us to a 50B parameter model, which\nis also roughly the Chinchilla optimal size for our compute budget. Figure 1 provides a\nsummary of the scaling laws and how BloombergGPT compares to other models.\nShape. To determine how to allocate the 50B parameters to di\ufb00erent model components\n(i.e., the \u201cshape\u201d of our model), we follow Levine et al. (2020), who propose that for a total\nnumber of self-attention layers L, the optimal hidden dimension Dis obtained by:\nD= exp(5.039) exp(0.0555\u00b7L)\nWe sweepLover a range of integer values and pick the ( L,D) combination that yields\na total of /tildelow50B parameters. This leads to the choice of L= 70 andD= 7510 as our\ntarget shape parameters. However, we also want to follow the tradition that the hidden\ndimension is evenly divisible by the number of attention heads, with the quotient giving\nthe attention head dimension. Furthermore, we want the dimensions to be multiples of 8\nto achieve higher performance in Tensor Core operations (NVIDIA, 2023). We settle on 40\nheads, each having a dimension of 192, resulting in a total hidden dimension of D= 7680\nand a total of 50.6B parameters. Table 4 provides a summary of the hyper-parameters used\ninBloombergGPT .\n3.3 Training Con\ufb01guration\nTraining. BloombergGPT is a PyTorch model trained with a standard left-to-right\ncausal language modeling objective. Following Brown et al. (2020), we want all our train-\ning sequences to be exactly the same length, in our case 2,048 tokens, to maximize GPU\nutilization. To achieve this, we concatenate all our tokenized training documents with an\n<|endoftext|> token as a document separator. We then break this token sequence into\nchunks of 2,048 tokens. Note that with this approach, each training sequence may contain\nmultiple documents from di\ufb00erent domains. Also note that, because we\u2019re using ALiBi\npositional encoding, BloombergGPT can be applied to sequences longer than 2,048 at\ninference time. For optimization e\ufb03ciency, training sequences are grouped together into\nbatches, as described in more detail below.\nOptimization. We use the AdamW optimizer (Loshchilov and Hutter, 2019). We set\n\u03b21to 0.9,\u03b22to 0.95, and weight decay to 0.1. Following Brown et al. (2020), we set the\nmaximum learning rate to 6e-5 and use the cosine decay learning rate scheduler with linear\nwarmup. We warm up the learning rate in the \ufb01rst 1800 steps. Following Ho\ufb00mann et al.\n(2022), the \ufb01nal learning rate is 0.1x the max learning rate, i.e. 6e-6. We also employ batch\nsize warmup (Brown et al., 2020): in the \ufb01rst 7,200 steps, we use a batch size of 1,024 (2.1M\ntokens), then switch to a batch size of 2,048 (4.2M tokens) for the remainder of training.\nWe set dropout to 0.0 in all layers in our initial run, although we add dropout later as\nexplained in \u00a74. The model parameters are randomly initialized to samples from a normal\ndistribution with zero mean and standard deviation\u221a\n1/(3D) = 0.006588 (Smith et al.,\n2022). Following Megatron-LM (Shoeybi et al., 2019), we rescale the standard deviation\nof the second layer in the MLP and the output layer of the attention by 1 /\u221a\n2L. We use\n13", "start_char_idx": 0, "end_char_idx": 3355, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee04aa4e-58e9-4da7-99c2-8eabd3fe3b70": {"__data__": {"id_": "ee04aa4e-58e9-4da7-99c2-8eabd3fe3b70", "embedding": null, "metadata": {"page_label": "14", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3233c7e-bafc-46f7-a025-194635e51725", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "436832760f39bd44fe456c7aad4a08668b00743dc6cd39e4b8b79894882d0a30", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ceb1b3b9-7688-4146-a10b-f77092b54bcc", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "6745d6dd75e369d4377105caeba932ed49f9374e0406ed52836402ed96c591e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23154560-63b4-464a-bb4d-9ba8508019a2", "node_type": "1", "metadata": {}, "hash": "a6378097a87d41f409685cf48fd61ccbc170ba7f0316e879791fc3cc67452a5b", "class_name": "RelatedNodeInfo"}}, "text": "the technique of query keylayer scaling (Shoeybi et al., 2019), which was proposed to\nimprove numerical stability for FP16 mixed-precision training but may also help in BF16.\nTraining Instability. LLMs optimization requires running convex optimization algo-\nrithms over incredibly complex non-convex loss surfaces. Previous work has reported vari-\nous instabilities while training LLMs. For example, Chowdhery et al. (2022) found that the\nloss spiked roughly 20 times while training PaLM, despite the fact that gradient clipping\nwas enabled. They mitigated these issues by re-starting training from a checkpoint roughly\n100 steps before the spike started, and then skip 200\u2013500 data batches. They hypothesized\nthat spikes occur due to the combination of speci\ufb01c data batches with a particular model\nparameter state. Similarly, during OPT training, Zhang et al. (2022a) noticed spikes in the\ngradient and activation norms, or divergences in the training perplexity. After these behav-\niors, they lowered their learning rate, which stabilized these norms and allowed training to\ncontinue. Interestingly, Scao et al. (2022) report only a single loss spike, from which the\nmodel recovered on its own.\nHardware Stack. We use the Amazon SageMaker service provided by AWS to train and\nevaluate BloombergGPT . We use the latest version available at the time of training and\ntrain on a total of 64 p4d.24xlarge instances. Each p4d.24xlarge instance has 8 NVIDIA\n40GB A100 GPUs with NVIDIA NVSwitch intra-node connections (600 GB/s) and NVIDIA\nGPUDirect using AWS Elastic Fabric Adapter (EFA) inter-node connections (400 Gb/s).\nThis yields a total of 512 40GB A100 GPUs. For quick data access, we use Amazon FSX for\nLustre, which supports up to 1000 MB/s read and write throughput per TiB storage unit.\n3.4 Large-scale Optimization\nTo train BloombergGPT , which has a larger memory footprint than available GPU mem-\nory on cloud instances, we rely on stage 3 of ZeRO optimization (Rajbhandari et al., 2020).\nWe utilize the proprietary SageMaker Model Parallelism (SMP) library from AWS, which\nenables the automatic distribution of large models across multiple GPU devices and in-\nstances (Karakus et al., 2021). After experimenting with various techniques, we achieve\n102 TFLOPs on average and each training step takes 32.5 seconds. We \ufb01nd the following\nsetup to be the best performing in our training.\nZeRO Optimization (stage 3). ZeRO shards the training state (model parameters,\ngradients, and optimizer state) across a group of GPUs. We shard a model across 128\nGPUs, and we have 4 copies of the model during training.\nMiCS. Zhang et al. (2022b) decrease training communication overhead and memory re-\nquirements for cloud training clusters. MiCS includes such features as hierarchical commu-\nnication, 2-hop gradient update, scale-aware model partitioning.\nActivation Checkpointing. Chen et al. (2016) minimizes training memory consumption\nby removing activations at the expense of additional computation during backward passes.\nWhen a layer has activation checkpointing enabled, only the layer input and outputs are\nkept in memory following a forward pass, while any intermediate tensors are discarded from\nmemory. During the backward pass, these intermediate tensors may be recomputed. We\napply activation checkpointing to each transformer layer.\n14", "start_char_idx": 0, "end_char_idx": 3346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23154560-63b4-464a-bb4d-9ba8508019a2": {"__data__": {"id_": "23154560-63b4-464a-bb4d-9ba8508019a2", "embedding": null, "metadata": {"page_label": "15", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3493162d-89d8-494d-819b-28148f912471", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d8a721c07b52e9ab05a4d79ad253b10507161fa0d23fdd6a564b1043957bd023", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee04aa4e-58e9-4da7-99c2-8eabd3fe3b70", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "436832760f39bd44fe456c7aad4a08668b00743dc6cd39e4b8b79894882d0a30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "914f70f3-ee6c-4573-8342-236884201070", "node_type": "1", "metadata": {}, "hash": "978ca0b286f2c14e9ce2ddd43fb670bcc89ef39de247eb90b62dd36aa37339ff", "class_name": "RelatedNodeInfo"}}, "text": "0 20000 40000 60000 80000 100000 120000 140000\nSteps2.002.252.502.753.003.253.503.754.00LossLearning curve\nconfig\nlr 6e-5 + bs 1024\nlr 6e-5\nlr 4e-5\nlr 2e-5 + dropout\nlr 1e-5 + dropout\nmetric\nsmooth train loss\nval loss\n 2.102.152.202.252.30 config\nlr 6e-5 + bs 1024\nlr 6e-5\nlr 4e-5\nlr 2e-5 + dropout\nlr 1e-5 + dropout\nmetric\nsmooth train loss\nval lossFigure 2: (Smoothed) training and validation losses for BloombergGPT . Inner plot is a\nzoomed-in version of the area within dashed rectangle in the outer plot (with\nshared x-axis). Colors denote di\ufb00erent hyperparameter con\ufb01gurations. Styles\ndenote training vs validation loss.\nMixed Precision Training. To reduce the memory requirements, forward and backward\npasses are done in BF16, while parameters are stored and updated in full precision (FP32).\nThe ALiBi matrices are computed in full precision and stored in BF16. We also use FP32\nto calculate fused softmax in the Attention block and store its results in BF16. Finally, the\nsoftmax calculations in the loss function are computed in FP32.\nFused Kernels. Another possibility for optimization is combining composition of several\noperations into a single GPU operation. This can both reduce peak memory usage by\navoiding storage of intermediate results in the computation graph, as well as help improve\nspeed. Similar to Megatron-LM (Shoeybi et al., 2019), we use a masked-causal-softmax\nfused kernel in SMP in the self-attention module. In practice, we observe 4-5 TFLOPs\nimprovement for speed, and avoid out-of-memory errors given the rest of the con\ufb01guration.\n4 Training Run\nThe process of training BloombergGPT involved decisions along the way based on the\nprogress of model training. We share some highlights of this process. A detailed presentation\nappears in the Training Chronicles (Appendix C). Figure 2 shows the learning curves for\nboth training and validation sets. The solid lines show (smoothed) training loss and the\ndotted lines show loss on the held-out validation set. Changes in the color of the lines\n15", "start_char_idx": 0, "end_char_idx": 2026, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "914f70f3-ee6c-4573-8342-236884201070": {"__data__": {"id_": "914f70f3-ee6c-4573-8342-236884201070", "embedding": null, "metadata": {"page_label": "16", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7894177e-62ee-4b0e-a2dd-612a30e939f8", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "0daeb67434eae29001ca98d153e195cb9d093481483f4658980ec05cdae73050", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23154560-63b4-464a-bb4d-9ba8508019a2", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d8a721c07b52e9ab05a4d79ad253b10507161fa0d23fdd6a564b1043957bd023", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd738e10-6b5f-4d17-872d-b41a1b9555f4", "node_type": "1", "metadata": {}, "hash": "f54ed34635aa4521a21f02057f4d87a82f9a5f4a058707c2eff2ddb0a8d4367c", "class_name": "RelatedNodeInfo"}}, "text": "indicate changes to the optimization hyperparameter con\ufb01gurations, either as scheduled, or\nin response to increasing or stagnating validation loss. This plot shows the path taken by\nthe successful model training run. To present a clear plot, the Figure does not show other\nattempts with di\ufb00erent model con\ufb01gurations, overwritten partial runs after a rollback, or\nother training strategies not utilized in the \ufb01nal model.\nWe measured training loss every \ufb01ve steps on the current batch. The raw values vary\nwildly, causing large jitter when plotted. The plot smoothes the training loss by showing\na running average yt=\u2211t\ni=0xi\u00b7(1\u2212\u03b1)(t\u2212i)\n\u2211t\ni=0(1\u2212\u03b1)(t\u2212i)where\u03b1= 0.001. Smoothing is not needed for the\nvalidation loss since it is measured on the entire validation set every 300 steps.\nWe trained for a total of 139,200 steps ( /tildelow53 days) and ended model training after\ncompleting /tildelow80% of one epoch through our training data (569B tokens out of the 709B\ntokens available). We ended training early because the loss on our held-out development\nset was no longer improving, although it\u2019s possible that substantially longer training may\nhave yielded further improvements.\nWe began the run with a warm-up batch size of 1,024 for 7,200 steps, after which we\nswitched to the regular batch size of 2,048 (color changes from black to blue). Change in\nbatch size manifests as a visible curvature change in the validation loss at step 7,200. Most\nof the remainder of the training performed stably with decreasing training and validation\nlosses. Intervention was required at later stages, after step 115,500, when we observed\n\ufb02at or increasing validation loss. We then applied the following corrective modi\ufb01cations in\nsequence:\n\u2022Step 115,500 (blue to orange): Shrink learning rate to two-thirds\n\u2022Step 129,900 (orange to green): Halve learning rate, and add dropout (with 0.1 prob-\nability)\n\u2022Step 137,100 (green to red): Halve learning rate again\nWe ended the run at step 146,000 based on the lack of observable progress on the validation\nloss. We selected the checkpoint at step 139,200 as the \ufb01nal model based on validation loss\nand downstream evaluations.\n5 Evaluation\nWe evaluated the performance of BloombergGPT on two broad categories of tasks:\n\ufb01nance-speci\ufb01c and general purpose. The \ufb01nance-speci\ufb01c tasks help us test our hypoth-\nesis that training on high-quality \ufb01nance-speci\ufb01c data will yield better results on \ufb01nancial\ntasks. The general purpose tasks investigate whether the performance of our model is\ndirectly comparable to previously published results. For \ufb01nancial tasks, we assembled pub-\nlicly available \ufb01nancial datasets that include a range of NLP tasks. Then, to directly test\nBloombergGPT \u2019s ability on Bloomberg tasks of interest, we also included tasks drawn\nfrom Bloomberg-internal high-quality evaluation sets for sentiment analysis and named en-\ntity recognition. For general-purpose tasks, we draw from multiple existing benchmarks\nand group results into the following categories: BIG-bench Hard, Knowledge Assessments,\nReading Comprehension, and Linguistic Tasks. The number of tasks per type and the\nde\ufb01nitions of the groups are presented in Table 5.\n16", "start_char_idx": 0, "end_char_idx": 3181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd738e10-6b5f-4d17-872d-b41a1b9555f4": {"__data__": {"id_": "dd738e10-6b5f-4d17-872d-b41a1b9555f4", "embedding": null, "metadata": {"page_label": "17", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8cadc1a7-4345-4ce4-8dae-4fe134bef877", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "5e3a98febec24fe1453dac1f9ca6373812a190521ed4c92ea749a66442da642f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "914f70f3-ee6c-4573-8342-236884201070", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "0daeb67434eae29001ca98d153e195cb9d093481483f4658980ec05cdae73050", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47d9d1a7-d392-4ae7-8d30-0f396c41c545", "node_type": "1", "metadata": {}, "hash": "ef95805b0ade5f83c21c476ee328f22b19c691f4ac10ce4fc584cfff3cf346df", "class_name": "RelatedNodeInfo"}}, "text": "Suite Tasks What does it measure?\nPublic Financial Tasks 5 Public datasets in the \ufb01nancial domain\nBloomberg Financial Tasks 12 NER and sentiment analysis tasks\nBig-bench Hard (Suzgun et al., 2022) 23 Reasoning and general NLP tasks\nKnowledge Assessments 5 Testing closed-book information recall\nReading Comprehension 5 Testing open-book tasks\nLinguistic Tasks 9 Not directly user-facing NLP tasks\nTable 5: Evaluation Benchmarks. We evaluate BloombergGPT on a high-coverage set\nof standard benchmarks that assess downstream performance, taken from HELM,\nSuperGLUE, MMLU, and the GPT-3 suite. Since these have signi\ufb01cant overlap\nand/or include each other, we restructure them into the categories presented here.\nWe only evaluate on one setup per dataset. We further assess BloombergGPT\non a suite of internal and public \ufb01nancial tasks.\nName # Tokens (B) # Params. (B) Compute\nBloombergGPT 569 50.6 1.00 \u00d7\nGPT-NeoX 472 20 0.33 \u00d7\nOPT 300 66 0.69 \u00d7\nBLOOM 366 176 2.24 \u00d7\nGPT-3 300 175 1.82 \u00d7\nTable 6: Evaluation model cohort. OPT and BLOOM each have multiple sizes available and\nwe report those we evaluated. We note that compute numbers are only partially\ncomparable between models: For example, BLOOMs training data is only 1/3\nEnglish, and OPT repeated some of its training data. We report GPT-3 results\nwhenever available but did not run it ourselves due to lack of availability.\nWe compare BloombergGPT to the three closest models described in \u00a77 based on\nmodel size, type of training data, overall performance, and most importantly, access. An\noverview of the model sizes and compute is provided in Table 6.\n1. GPT-NeoX (Black et al., 2022): According to Liang et al. (2022), this model is the\nbest performing available model under 50B parameters.\n2. OPT 66B(Zhang et al., 2022a): We chose to compare to OPT 66Bsince our model size\nand structure roughly match, though our model is smaller.\n3. BLOOM 176B(Scao et al., 2022): While this model is substantially larger than BloombergGPT ,\nwe use the same model architecture and software stack. We note that BLOOM 176Bis\nmultilingual, so while it is much larger, it also is trained on data from more languages.\n17", "start_char_idx": 0, "end_char_idx": 2158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47d9d1a7-d392-4ae7-8d30-0f396c41c545": {"__data__": {"id_": "47d9d1a7-d392-4ae7-8d30-0f396c41c545", "embedding": null, "metadata": {"page_label": "18", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f51ddd1-f17e-44c6-b54b-de27c1552691", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "ec2020a1e00f2f4d6a1a9cb35f3931983e5e46bed8538047a78c22b8a9103165", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd738e10-6b5f-4d17-872d-b41a1b9555f4", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "5e3a98febec24fe1453dac1f9ca6373812a190521ed4c92ea749a66442da642f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bd61933-8656-42a3-bf4e-182fd5fb7830", "node_type": "1", "metadata": {}, "hash": "a6b82197b80a2a6d234b3f45161a6098ad2c451d08337e7f83effba47da9156b", "class_name": "RelatedNodeInfo"}}, "text": "All three models use some of the same general-purpose datasets we use in our training cor-\npus. We additionally report results from the original GPT-3 (Brown et al., 2020) whenever\nexternally available.2\nWe prefer running models ourselves to ensure identical evaluation setups, and we place\nany results that have been reported elsewhere and were not run by us into a separated\ngroup. To fairly compare the models, we avoid any tuning of prompts and other techniques\nthat could lead to improved results for some, but not all, models. For that reason, every\ntask is tested via \u201cstandard\u201d prompting (shown in Table 7), i.e., without any parameter\nchanges to the underlying model, without task descriptions, and without Chain-of-Thought\nprompting (Wei et al., 2022b). The number of few-shot examples presented to the model\ndepends on the task, and we include these details in the respective sections. For each group\nof results, we further present a win rate similar to Liang et al. (2022) that represents the\nfraction of \u201cwins\u201d in side-by-side comparisons over individual tasks between all model pairs\nfor which we have run the evaluation ourselves.\n5.1 Few-shot Methodology\nFor tasks where a set of candidates are given, we perform likelihood-based classi\ufb01cation,\nfollowing Brown et al. (2020). We consider three methods for classi\ufb01cation: regular, cali-\nbration, and normalization. Formally,\n\u2022Regular: arg max \u03b1p(\u03b1|s)\n\u2022Calibration: arg max \u03b1p(\u03b1|s)/p(\u03b1|\u201cAnswer:\u201d)\n\u2022Normalization: arg max \u03b1p(\u03b1|s)/len(\u03b1)\nwhere\u03b1is a candidate, sis the context, and len measures the number of sub-word tokens.\nWe report the performance of the best method for each model and task. For other tasks,\nwe perform generation via greedy decoding.\nWe use the o\ufb03cial split and report performance on the test set whenever possible. If\nthe test labels are not publicly available, we report performance on the dev set instead. If\nan o\ufb03cial split for a dataset does not exist, we create train and test splits by selecting 20%\nof examples to be the test and the rest as train. All few-shot context examples are sampled\nfrom the training set. To reduce the variance of few-shot evaluation, we sample di\ufb00erent\nshots for each test example, unless otherwise speci\ufb01ed. For the sake of consistency, for each\ntest example, all models have identical surface form as input in our evaluation.\n5.2 Heldout Loss\nWe begin by testing how well BloombergGPT models the language distribution of the in-\ndistribution \ufb01nance data. We evaluate the bits per byte of the di\ufb00erent models on a heldout\ndataset that contains examples from all sections of FinPile (described in \u00a72). To limit\ndata leakage and better simulate real-world usage of LLMs, we select a temporally heldout\n2. Another related general-purpose model at a comparable size (LLaMA, Touvron et al., 2023), was released\nduring the preparation of this manuscript, but third-party evaluation results were not available and we\nhaven\u2019t received access to the model weights.\n18", "start_char_idx": 0, "end_char_idx": 2977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bd61933-8656-42a3-bf4e-182fd5fb7830": {"__data__": {"id_": "1bd61933-8656-42a3-bf4e-182fd5fb7830", "embedding": null, "metadata": {"page_label": "19", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b58f2bcc-546a-4b45-b4de-7e264d2d5ce8", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e67e4b6cff6ceaadce6ff573c96d3d2a9c195e35f212bf31debdd3f6dac18f23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47d9d1a7-d392-4ae7-8d30-0f396c41c545", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "ec2020a1e00f2f4d6a1a9cb35f3931983e5e46bed8538047a78c22b8a9103165", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76afe11e-2b48-4eb5-bcf0-4324308aafe7", "node_type": "1", "metadata": {}, "hash": "6315dddec42ebe6555f7ed78f376e2dc521fbf068015a0d84fb7ad8fd5cf6d3f", "class_name": "RelatedNodeInfo"}}, "text": "Overall Bloomberg Filings Newswires Press Web0.00.20.40.60.8bits per byteBloombergGPT\nGPT-Neo-X\nOPT66B\nBLOOM176BFigure 3: Bits per byte on a heldout test set of each data type in our FinPile (lower\nis better). The set of documents is held out in time and deduplicated with\nthe training set, such that all of it is completely unseen by BloombergGPT .\nRegardless, we observe a large gap between the models. The improvement is\nlargest for specialized in-domain documents like Filings.\ndataset that is strictly further in the future than the training set, and perform deduplication\nbetween the training and heldout set. During evaluation, for documents that are longer than\n2,048 tokens, we use a sliding window approach with half window size as context. That\nmeans that any token beyond the \ufb01rst 2,048 has at least 1,024 tokens as context during\nprediction. We report the loss breakdown by the type of document in FinPile .\nFigure 3 shows that BloombergGPT consistently outperforms other models. While\nthis is expected and mainly serves as a sanity check, it also provides valuable insight into the\ngeneralization capabilities of the other models. For example, the gap to BloombergGPT\nis most signi\ufb01cant in the Filings category, likely because these documents, while public, are\ntypically in PDF format and thus not included in any existing datasets.\n5.3 Financial Tasks\nThe NLP tasks most often considered in \ufb01nance are also common in the broader NLP liter-\nature; but, these tasks take on di\ufb00erent characteristics and challenges when performed on\n\ufb01nancial data. Take the example of sentiment analysis, where a headline such as \u201cCOM-\nPANY to cut 10,000 jobs\u201d portrays negative sentiment in the general sense but can at times\nbe considered positive for \ufb01nancial sentiment towards COMPANY, as it might result in the\nstock price or investor con\ufb01dence increasing. We use a combination of public and internal\nbenchmarks to assess the performance of BloombergGPT , BLOOM 176B, GPT-NeoX, and\nOPT 66B. All task types considered and their corresponding prompt templates are shown\nin Table 7.\n19", "start_char_idx": 0, "end_char_idx": 2083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76afe11e-2b48-4eb5-bcf0-4324308aafe7": {"__data__": {"id_": "76afe11e-2b48-4eb5-bcf0-4324308aafe7", "embedding": null, "metadata": {"page_label": "20", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "01bd8987-9835-4bd2-840c-9ec42b2baa8a", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "f2a6599fa8a6046591887ea8b451f25c242df76a7e44d724673f26e8ba237846", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bd61933-8656-42a3-bf4e-182fd5fb7830", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e67e4b6cff6ceaadce6ff573c96d3d2a9c195e35f212bf31debdd3f6dac18f23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2622add1-04c3-465a-af28-ab8ed217f13f", "node_type": "1", "metadata": {}, "hash": "7cdff8ebfe6d158022fa4816ec524502c4d465f2a879cee5c9fa506f58651167", "class_name": "RelatedNodeInfo"}}, "text": "Task Template/Example\nDiscriminative\nSentiment Analysis {sentence}\nQuestion: what is the sentiment?\nAnswer:{negative/neutral/positive }\nAspect Sentiment Analysis {sentence}\nQuestion: what is the sentiment on {target}?\nAnswer:{negative/neutral/positive }\nBinary Classi\ufb01cation {sentence}\nQuestion:{question}?\nAnswer:{Yes/No}\nGenerative\nNER Steve Jobs is the CEO of Apple\nExtract named entity: Steve Jobs (person), Apple (organization)\nNER+NED AAPL stopped using Intel Chips\nExtract ticker: AAPL, INTC\nQA {context}\nQuestion:{question}?\nAnswer:{answer}\nTable 7: Template for the di\ufb00erent tasks we evaluate in the \ufb01nancial domain.\n5.3.1 External Financial Tasks\nOur public \ufb01nancial benchmarks include four tasks from the FLUE benchmark (Shah et al.,\n2022) and the ConvFinQA dataset (Chen et al., 2022). As LLM performance on most of\nthese \ufb01nancial tasks have not been broadly reported, there is no standard testing frame-\nwork. Thus, we adapt them to a few-shot setting (see Section \u00a75.1). Our guiding principle\nin designing the experiments was to select the number of shots such that the average per-\nformance across all the models was best. While non-LLM numbers of custom models for\nthese tasks are available, we omit reporting them here due to di\ufb00erences in the evaluation\nsetup. As a result, our claims are restricted to comparisons of LLMs. We evaluate on the\nfollowing tasks (more details provided in Appendix B):\n\u2022FPB (Malo et al., 2014): The Financial Phrasebank Dataset includes a sentiment\nclassi\ufb01cation task on sentences from \ufb01nancial news. Any news that could bene\ufb01t/hurt\nan investor is considered positive/negative and neutral otherwise. We create our own\nsplits and report F1 score weighted by support in a 5-shot setup.\n\u2022FiQA SA (Maia et al., 2018): The second sentiment analysis task is to predict the\naspect-speci\ufb01c sentiment in English \ufb01nancial news and microblog headlines, which\nwere published as a part of the 2018 challenge on \ufb01nancial question answering and\nopinion mining. While the original dataset is annotated on a continuous scale, we\ndiscretize the data into a classi\ufb01cation setup with negative, neutral, and positive\nclasses. Like with FPB, we create our own splits including microblogs and news, and\nuse a 5-shot setup, reporting weighted F1.\n20", "start_char_idx": 0, "end_char_idx": 2272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2622add1-04c3-465a-af28-ab8ed217f13f": {"__data__": {"id_": "2622add1-04c3-465a-af28-ab8ed217f13f", "embedding": null, "metadata": {"page_label": "21", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c4505c2-d38c-4c13-8c88-937414d7421f", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "5f25fa9fe11ee360ae6a06645cab54c110320118e871b28141a2a26319918d02", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76afe11e-2b48-4eb5-bcf0-4324308aafe7", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "f2a6599fa8a6046591887ea8b451f25c242df76a7e44d724673f26e8ba237846", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59db3101-fe89-4f49-af6c-524894cd5e25", "node_type": "1", "metadata": {}, "hash": "f30a908f7d9372e926b5a751a6a4d8a1e5b7ec3cf24c6dce6bf1ea454c68389a", "class_name": "RelatedNodeInfo"}}, "text": "BloombergGPT GPT-NeoX OPT 66B BLOOM 176B\nConvFinQA 43.41 30.06 27.88 36.31\nFiQA SA 75.07 50.59 51.60 53.12\nFPB 51.07 44.64 48.67 50.25\nHeadline 82.20 73.22 79.41 76.51\nNER 60.82 60.98 57.49 55.56\nAll Tasks (avg) 62.51 51.90 53.01 54.35\nAll Tasks (WR) 0.93 0.27 0.33 0.47\nTable 8: Results on \ufb01nancial domain tasks.\n\u2022Headline (Sinha and Khandait, 2020): This is a binary classi\ufb01cation task of whether\na news headline in the gold commodity domain includes certain information. This\nhuman-annotated dataset consists of English news headlines about \u201cgold\u201d. Each news\narticle carries a subset of the following tags: \u201cprice or not\u201d, \u201cprice up\u201d, \u201cprice down\u201d,\n\u201cprice stable\u201d, \u201cpast price\u201d, \u201cfuture price\u201d, \u201cpast general\u201d, \u201cfuture general\u201d, \u201casset\ncomparison\u201d. We verbalize each tag into a question using the o\ufb03cial documentation,\nuse 5 shots, and report the average weighted F1 score across all categories.\n\u2022NER (Salinas Alvarado et al., 2015): This is a named entity recognition task on \ufb01nan-\ncial data gathered for credit risk assessment from \ufb01nancial agreements \ufb01led with the\nSEC. The annotated entity types follow the standard CoNLL format (Tjong Kim Sang\nand De Meulder, 2003) and are annotated with PER, LOC, ORG, and MISC. As it\nis nontrivial to learn to predict empty outputs in few-shot setups, we drop sentences\nthat do not contain any entity. We further drop MISC tags due to their ambiguous\nde\ufb01nition. All the models required more shots to perform well and we thus selected\n20 shots and report the entity-level F1 score.\n\u2022ConvFinQA (Chen et al., 2022): Given input from S&P 500 earnings reports that\nincludes text and at least one table with \ufb01nancial data, the task is to answer conver-\nsational questions that require numerical reasoning over the input. This task requires\nnumerical reasoning, an understanding of structured data and \ufb01nancial concepts, and\na model needs to relate follow-up questions to the dialog turns.\nFor ConvFinQA, we use an entire gold conversation and its context is used as input\nto the models. As each \u201cturn\u201d of the conversation concludes, the \u201cturn\u201d along with\nthe answer for that turn is appended as context for future turns. We report the exact\nmatch accuracy on the public development set.\nBloombergGPT performs best of all models for four of the \ufb01ve tasks (ConvFinQA,\nFiQA SA, FPB, and Headline) and comes in second in NER (Table 8). Consequently,\nBloombergGPT also has the highest win rate among all the models that we tested. The\ngap to equally-sized models is especially pronounced for ConvFinQA which is challenging\ndue to the requirement to use conversational input to reason over tables and generate an\nanswer.\n21", "start_char_idx": 0, "end_char_idx": 2655, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59db3101-fe89-4f49-af6c-524894cd5e25": {"__data__": {"id_": "59db3101-fe89-4f49-af6c-524894cd5e25", "embedding": null, "metadata": {"page_label": "22", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "368d7731-a37c-4558-8253-ca751f363a3f", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "fe99f70ae0bdeba6a4d443cb4690152b9cddbaac63bad2f6dca0c789a54b7a32", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2622add1-04c3-465a-af28-ab8ed217f13f", "node_type": "1", "metadata": {"page_label": "21", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "5f25fa9fe11ee360ae6a06645cab54c110320118e871b28141a2a26319918d02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0aad11f5-66e6-4f96-8e3f-a12fdc42feb2", "node_type": "1", "metadata": {}, "hash": "74842172ee318172068466acf9f9bf05e348c2887be3adff0dc6ae3b0b90f8c1", "class_name": "RelatedNodeInfo"}}, "text": "Name Time Tokens Test Size % Pos % Neu % Neg\nEquity News 2018\u20132019 150-200 1,000 7 87 6\nEquity Social Media 2015\u20132020 15-20 1,000 10 83 7\nEquity Transcript 2008\u20132020 70-80 800 19 75 6\nES News 2016\u20132019 100-120 1,000 32 53 15\nCountry News 2009\u20132021 50-1,000 1,000 18 60 22\nTable 9: An overview of the Bloomberg-internal sentiment analysis tasks. Input token and\nlabel distribution numbers are computed on the test set.\n5.3.2 Internal Task: Sentiment Analysis\nFor the Bloomberg-internal tasks, we consider aspect-speci\ufb01c sentiment analysis, which is\nprevalent in \ufb01nancial literature. All of the datasets we use are in English.\nOur annotation process consists of a discovery phase during which we establish the an-\nnotation and sampling procedures, understand how many annotators are typically required\nper example, and determine the level of training that is needed for the annotators (Tseng\net al., 2020). Depending on the complexity of the task, our annotators are a dedicated team\nof \ufb01nancial experts at Bloomberg, consultant workers, or a combination of both. In each\ncase, ties are resolved by adjudication from additional annotators and ambiguous examples\nare excluded. All the datasets in this section were annotated by 2 annotators with a third\nannotator breaking any ties.\nWe measure the performance of LLMs for the internal datasets using a \ufb01ve-shot evalu-\nation, similar to the external datasets. As the datasets are large, we randomly sample at\nmost 1k test examples. We report F1 weighted by the support of each label. Note that,\nsimilar to the external datasets, it is likely that the unlabeled versions of the data used in\nour internal datasets occur in FinPile and are therefore seen by BloombergGPT during\ntraining. However, since some of FinPile is also available on the web, other LLMs we\ncompare against may have also been trained on unlabeled versions of this data. Dataset\nstatistics are provided in Table 9.\n\u2022Equity News Sentiment : This task is to predict the aspect-speci\ufb01c sentiment ex-\npressed in the news story toward a company. The dataset consists of English news\nstories from Bloomberg, premium, and web content. Annotations of \u201cpositive\u201d, \u201cneg-\native\u201d, or \u201cneutral\u201d indicate that the news story is likely to increase, decrease, or not\nchange the long-term investor con\ufb01dence in the company.\n\u2022Equity Social Media Sentiment : The task is similar to \u201cEquity News Sentiment\u201d\nbut instead of news, we use \ufb01nancially-relevant English social media content.\n\u2022Equity Transcript Sentiment : This task is also similar to \u201cEquity News Senti-\nment\u201d but instead of news, we use transcripts from company press conferences. The\ntranscripts are made available through the use of speech recognition and at times,\nhuman edits. Long transcripts are processed in chunks, and each chunk in our dataset\ntypically contains between 70 and 80 tokens.\n22", "start_char_idx": 0, "end_char_idx": 2856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0aad11f5-66e6-4f96-8e3f-a12fdc42feb2": {"__data__": {"id_": "0aad11f5-66e6-4f96-8e3f-a12fdc42feb2", "embedding": null, "metadata": {"page_label": "23", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "daa9da1c-ea39-4154-bff7-ffc47d92b5c5", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "6dc153a90f1964fadfc02ed923b5f563b731369b13658f43b7d3436d92078a77", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59db3101-fe89-4f49-af6c-524894cd5e25", "node_type": "1", "metadata": {"page_label": "22", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "fe99f70ae0bdeba6a4d443cb4690152b9cddbaac63bad2f6dca0c789a54b7a32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9323b3ad-9c4c-4984-b858-720ed251718e", "node_type": "1", "metadata": {}, "hash": "4ba7e807807b882529a276377a924e982b854e1a564fc203b6a32262ee80b697", "class_name": "RelatedNodeInfo"}}, "text": "BloombergGPT GPT-NeoX OPT 66B BLOOM 176B\nEquity News 79.63 14.17 20.98 19.96\nEquity Social Media 72.40 66.48 71.36 68.04\nEquity Transcript 65.06 25.08 37.58 34.82\nES News 46.12 26.99 31.44 28.07\nCountry News 49.14 13.45 17.41 16.06\nAll Tasks (avg) 62.47 29.23 35.76 33.39\nAll Tasks (WR) 1.00 0.00 0.67 0.33\nTable 10: Results on internal aspect-speci\ufb01c sentiment analysis datasets.\nBloombergGPT far outperforms all other models on sentiment analysis\ntasks.\nName Tokens Test Size LOC ORG PER\nBFW /tildelow21 500 0.2 1.6 0.0\nBN /tildelow30 500 0.7 1.0 0.6\nFilings /tildelow32 500 0.1 1.3 0.4\nHeadlines /tildelow50 500 0.7 2.7 1.0\nPremium /tildelow29 500 0.6 1.4 0.3\nTranscripts /tildelow23 500 0.6 0.6 0.3\nSocial Media /tildelow12 500 0.4 1.4 0.2\nTable 11: An overview of statistics of our internal NER test set. We report average number\nof LOCation, ORGanization, PERson per example.\n\u2022ES News Sentiment : While this task is to predict the aspect-speci\ufb01c sentiment\nexpressed in the news story towards a company (aspect), the goal is not to indicate\ne\ufb00ect on investor con\ufb01dence. The stories are annotated \u201cpositive\u201d, \u201cnegative\u201d, or\n\u201cneutral\u201d if the news story contains content that re\ufb02ects good, bad, or neutral news\nabout the company\u2019s environmental and social policies.\n\u2022Country News Sentiment : This task is di\ufb00erent from the other sentiment tasks\nin that the goal is to predict the sentiment expressed in the news story towards a\ncountry. The dataset consists of English news stories from Bloomberg, premium, and\nweb content. The stories are annotated \u201cpositive\u201d, \u201cnegative\u201d, or \u201cneutral\u201d if the\nnews story alludes to the growth, shrinkage, or status quo of that country\u2019s economy.\nTable 10 shows that across the four internal aspect-speci\ufb01c sentiment tasks BloombergGPT\nperforms better than all the other tested models, by a wide margin. The only task in which\nthe models perform similarly is the social media sentiment task, while BloombergGPT\noutperforms the other models by at least 25 and up to over 60 points in the other three.\n5.3.3 Exploratory Task: NER\nEven though NER is a well-established NLP task with state-of-the-art results using BERT\n(Wu and Dredze, 2019; Luoma and Pyysalo, 2020) and T5 (Liu et al., 2022) style models,\n23", "start_char_idx": 0, "end_char_idx": 2240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9323b3ad-9c4c-4984-b858-720ed251718e": {"__data__": {"id_": "9323b3ad-9c4c-4984-b858-720ed251718e", "embedding": null, "metadata": {"page_label": "24", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "28fff96f-c15a-422f-b2b7-50bc4329b2b9", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c9b55d1725c81cfb9dc4e574ddc9bb17231d3bed0e81d584d11179a726a0b0e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0aad11f5-66e6-4f96-8e3f-a12fdc42feb2", "node_type": "1", "metadata": {"page_label": "23", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "6dc153a90f1964fadfc02ed923b5f563b731369b13658f43b7d3436d92078a77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c009ee86-065f-4c23-a477-f4c34b40b966", "node_type": "1", "metadata": {}, "hash": "40891cfee23dfa9cff0e9b423c2710e343e9ff5273ccd09dc0ead0a93c78af0c", "class_name": "RelatedNodeInfo"}}, "text": "NER is largely an unexplored task for generative LLMs. NER is not in HELM (Liang et al.,\n2022), there is a single (Polish) task in BIG-bench (Srivastava et al., 2022), and none of the\nLLM papers we study report NER performance. Hence, we consider NER as an exploratory\ntask and report preliminary NER results given its importance in the Financial sector.\nThere are a few reasons for why NER may be a di\ufb03cult task for generative LLMs.\nNER is an information extraction task, and a better \ufb01t for encoder-decoder or encoder-only\narchitectures. The generative nature of LLMs does not confer an advantage for NER. We\n\ufb01nd that extensive prompt engineering and a greater number of shots are required to obtain\nreasonable results for NER than for other tasks. Finance-speci\ufb01c NER has subtleties that\nmake it especially di\ufb03cult for zero or few-shot learning.\nFor example, consider the (fabricated) headline \u201cBloomberg: Mr. Musk adds new fea-\ntures to Twitter and comments on China\u201d. Depending on our annotation guidelines and\ndownstream task needs: (a) the reporting news organization \u201cBloomberg\u201d can be tagged or\nnot, depending on whether we want only salient entities, (b) \u201cMr. Musk\u201d or just \u201cMusk\u201d\nis the PER to be tagged, (c) \u201cTwitter\u201d can be tagged as an ORG or a PRD (product)\nas features are added to the Twitter product and not the organization, and (d) \u201cChina\u201d\ncan be tagged ORG or LOC, though the right tag is likely ORG. Without adding extensive\nannotation guidelines in the prompt, the LLM does not know the intended tagging behavior.\nBased on preliminary testing, we determined the following setting to obtain the best\nperformance on the internal NER tasks from all models. First, we restrict the entity types\nto be predicted to be ORG, PER, and LOC. In all, we \ufb01ltered out less than 1% of entities.\nWe also remove all documents that contain no entities (i.e., all \u201cO\u201d\u2019s). Both of these modi-\n\ufb01cations are intended to increase the usefulness of the examples seen in few-shot prompting.\nWe expect that further work on prompt engineering for NER could produce better results.\nWe consider seven Bloomberg internal NER datasets from di\ufb00erent domains.\n\u2022BN NER : This is a named entity recognition task on entities occurring in English\nlong-form Bloomberg news content (the \u201cBN wire\u201d) between 2017 to 2020.\n\u2022BFW NER : Similar to \u201cBN NER\u201d but instead of using the long-form BN wire, we\nuse short-form stories from the \u201cBloomberg First Word\u201d wire between 2018 to 2020.\n\u2022Filings NER : The goal of this task is to identify entities that occur in mandatory\n\ufb01nancial disclosures \ufb01led by companies. The dataset contains \ufb01lings sampled between\n2016 and 2019.\n\u2022Headlines NER : The goal of this task is to identify entities that occur in headlines\nof English Bloomberg news content. The dataset contains headlines sampled between\n2016 and 2020.\n\u2022Premium NER : The goal of this task is to identify entities that occur in a subset\nof the third-party English news content ingested by Bloomberg. The dataset contains\nstories sampled between 2019 and 2021.\n\u2022Transcripts NER : The goal of this task is to identify entities that occur in transcripts\nof company press conferences. The dataset contains transcripts from 2019.\n24", "start_char_idx": 0, "end_char_idx": 3206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c009ee86-065f-4c23-a477-f4c34b40b966": {"__data__": {"id_": "c009ee86-065f-4c23-a477-f4c34b40b966", "embedding": null, "metadata": {"page_label": "25", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7bd03e2c-d7cc-409a-a469-89af880ab637", "node_type": "4", "metadata": {"page_label": "25", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "1648658bd2c9f1000b41febd17d4fde0a5cfe840a91c2f86b4f54ce72f403cff", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9323b3ad-9c4c-4984-b858-720ed251718e", "node_type": "1", "metadata": {"page_label": "24", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c9b55d1725c81cfb9dc4e574ddc9bb17231d3bed0e81d584d11179a726a0b0e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac85c774-e6a8-46f9-8a07-7b9000793683", "node_type": "1", "metadata": {}, "hash": "834cd20c1c10268564c2f9ed4dc175f22c0d1785c0045bf7927edcd4eca7d83d", "class_name": "RelatedNodeInfo"}}, "text": "BloombergGPT GPT-NeoX OPT 66B BLOOM 176B\nNER\nBFW 72.04 71.66 72.53 76.87\nBN 57.31 52.83 46.87 59.61\nFilings 58.84 59.26 59.01 64.88\nHeadlines 53.61 47.70 46.21 52.17\nPremium 60.49 59.39 57.56 61.61\nTranscripts 75.50 70.62 72.53 77.80\nSocial Media 60.60 56.80 51.93 60.88\nAll Tasks (avg) 62.63 59.75 58.09 64.83\nAll Tasks (WR) 0.57 0.29 0.19 0.95\nNER+NED\nBFW 55.29 34.92 36.73 39.36\nBN 60.09 44.71 54.60 49.85\nFilings 66.67 31.70 65.63 42.93\nHeadlines 67.17 36.46 56.46 42.93\nPremium 64.11 40.84 57.06 42.11\nTranscripts 73.15 23.65 70.44 34.87\nSocial Media 67.34 62.57 70.57 65.94\nAll Tasks (avg) 64.83 39.26 58.79 45.43\nAll Tasks (WR) 0.95 0.00 0.67 0.38\nTable 12: Results on internal NER and NED datasets. On NER, while the much larger\nBLOOM 176B model outperforms all other models, results from all models are\nrelatively close, with BloombergGPT outperforming the other two models. On\nNER+NED, BloombergGPT outperforms all other models by a large margin.\n\u2022Social Media NER : The goal of this task is to identify entities that occur in English\n\ufb01nancially-relevant social media content. The dataset contains social media content\nsampled between 2009 and 2020.\nAs our datasets are substantive, we randomly sample 4,000 training and 500 testing ex-\namples from each \ufb01ltered internal dataset. We utilize 20-shot prompts and evaluate using\nF1. The results from the internal NER tasks are mixed (Table 12). The much larger\nBLOOM 176Bwins most of the NER tasks. Of the like-sized models, BloombergGPT per-\nforms the best placing \ufb01rst once (Headlines), second four times (BN, Premium, Transcripts,\nSocial media), third once (BFW), and last once (Filings).\nExploratory Task: NER+NED Named entity disambiguation (NED) links entity\nmentions to known entities in knowledge bases or other structured information sources.\nWithin the \ufb01nancial world, we seek to link text mentions of companies to their ticker sym-\nbols, an abbreviation that uniquely identi\ufb01es publicly traded shares of a particular stock\non a particular stock market.\nWe directly test the ability of an LLM to complete this task by evaluating a joint\nNER+NED task: identify the stock tickers of companies mentioned in a document. This\n25", "start_char_idx": 0, "end_char_idx": 2190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac85c774-e6a8-46f9-8a07-7b9000793683": {"__data__": {"id_": "ac85c774-e6a8-46f9-8a07-7b9000793683", "embedding": null, "metadata": {"page_label": "26", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aac4e564-9b8e-41b8-a1b2-067cfc30e76e", "node_type": "4", "metadata": {"page_label": "26", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "5dcaf0f5ed7eb8e1af07289e851b9af8a49493880103c46ba7feb31b68fe6166", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c009ee86-065f-4c23-a477-f4c34b40b966", "node_type": "1", "metadata": {"page_label": "25", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "1648658bd2c9f1000b41febd17d4fde0a5cfe840a91c2f86b4f54ce72f403cff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c050c699-d858-4900-b333-bd529c18d160", "node_type": "1", "metadata": {}, "hash": "e4b2759da2e1e5d164b193b6d4fc68f653ba3ef8c547895843c6f00fd2937057", "class_name": "RelatedNodeInfo"}}, "text": "requires the model to \ufb01rst identify company mentions and then generate the corresponding\nstock ticker. For example, given \u201cAAPL announced that they will stop using Intel chips\nin future products.\u201d the correct NER output would be \u201cAAPL, Intel\u201d while the correct\nNER+NED output would be \u201cAAPL, INTC\u201d.\nOne of the advantages of this task is that it is robust to variations in extracting the\nexact text span. While NER evaluation requires exact matches, tickers may be successfully\nproduced without \ufb01rst identifying spans. Furthermore, it evaluates a model\u2019s knowledge of\ncompanies, their various surface forms, and company to ticker mappings.\nWe create evaluation data with linked tickers for this task by running a state-of-the-\nart entity linking system for companies in \ufb01nancial data over the Bloomberg internal NER\nannotated documents from each domain. We remove documents with no linked tickers. Fol-\nlowing our NER evaluations, we randomly sample 4,000 training and 500 testing examples\nfrom each \ufb01ltered internal dataset. We utilize 20-shot prompts and evaluate using F1.\nTable 12 shows that BloombergGPT outperforms all other models by a large margin,\nexcept on social media data where it comes in second behind BLOOM 176B. In our social\nmedia data, companies are often referenced by their tickers, removing the requirement of the\nmodel to link the mention and reverting the task to NER. These results further underscore\nthe advantage of BloombergGPT for \ufb01nancial tasks.\n5.4 BIG-bench Hard\nWe now turn to evaluate BloombergGPT on standard, general-purpose NLP tasks. While\nthe focus of our model is on \ufb01nancial tasks, our inclusion of general-purpose training data\nmay help improve not only the \ufb01nancial tasks, but also allow our model to perform well\non more standard NLP datasets. We start with BIG-bench Hard (Suzgun et al., 2022), a\nsubset of the most challenging tasks in BIG-bench (Srivastava et al., 2022). It only includes\ntasks in which the best available model at construction was unable to achieve a performance\nhigher than the average human rater via standard prompting techniques.\nResults for each task are shown in Table 13. Overall, while BloombergGPT falls\nbehind the much larger PaLM 540B (10x parameters) and BLOOM 176B (3.5x parameters),\nit is the best-performing among similarly sized models. In fact, its performance is closer to\nBLOOM 176Bthan it is to either GPT-NeoX or OPT 66B. It further achieves the best perfor-\nmance of all models in date understanding, hyperbaton (ordering of adjectives), and tracking\nshu\ufb04ed objects. In sum, according to this benchmark, we \ufb01nd that developing \ufb01nance-\nspeci\ufb01c BloombergGPT did not come at the expense of its general-purpose abilities.\n5.5 Knowledge Assessments\nWe next assess knowledge, which we de\ufb01ne as the ability to recall information seen during\nmodel training, via scenarios that have the model answer questions without providing addi-\ntional context or resources (closed-book question answering). This includes multiple-choice\nquestions, and we report accuracy. We follow the template of Brown et al. (2020). The list\nof scenarios is as follows:\n\u2022ARC (Clark et al., 2018): Multiple-choice questions collected from 3rd to 9th grade\nscience exams, includes easy and challenging splits.\n26", "start_char_idx": 0, "end_char_idx": 3262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c050c699-d858-4900-b333-bd529c18d160": {"__data__": {"id_": "c050c699-d858-4900-b333-bd529c18d160", "embedding": null, "metadata": {"page_label": "27", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83be183d-be91-4545-9073-281e10ca37fe", "node_type": "4", "metadata": {"page_label": "27", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "16a9739fde37b45cea2a2b1af725f748c08877116a7afbb38f372b59922ae5e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac85c774-e6a8-46f9-8a07-7b9000793683", "node_type": "1", "metadata": {"page_label": "26", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "5dcaf0f5ed7eb8e1af07289e851b9af8a49493880103c46ba7feb31b68fe6166", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5996284b-6f5f-4520-8ac9-439f323ebc18", "node_type": "1", "metadata": {}, "hash": "f49d73081089db48a2fb1f7dffc776d3d9061ff5a64abbadc6b21e86d01399c1", "class_name": "RelatedNodeInfo"}}, "text": "BIG-bench Hard Task BloombergGPT GPT-NeoX OPT 66B BLOOM 176B PaLM 540B\nBoolean Expressions\u03bb62.40 71.20 48.40 69.20 83.2\nCausal Judgement 49.73 52.41 51.87 51.87 61.0\nDate Understanding 54.80 45.60 49.60 50.00 53.6\nDisambiguation QA 34.00 40.80 40.40 40.40 60.8\nDyck Languages\u03bb15.60 26.00 14.80 42.00 28.4\nFormal Fallacies 50.80 52.80 54.00 52.80 53.6\nGeometric Shapes\u03bb15.20 8.00 11.60 22.40 37.6\nHyperbaton 92.00 92.00 91.60 92.00 70.8\nLogical Deduction\u03bb(avg) 34.53 30.93 31.87 34.00 60.4\nMovie Recommendation 90.40 86.40 91.20 91.20 87.2\nMulti-Step Arithmetic\u03bb[Two] 1.20 0.40 0.40 0.00 1.6\nNavigate\u03bb42.00 45.20 42.00 50.00 62.4\nObject Counting\u03bb33.20 21.20 26.00 36.80 51.2\nPenguins in a Table 37.67 33.56 28.08 40.41 44.5\nReasoning about Colored Objects 34.80 26.00 31.20 36.80 38.0\nRuin Names 56.00 54.00 52.80 54.80 76.0\nSalient Translation Error Detection 20.00 20.40 16.40 23.60 48.8\nSnarks 69.66 62.36 69.66 72.47 78.1\nSports Understanding 62.80 53.20 54.40 53.20 80.4\nTemporal Sequences\u03bb29.20 21.20 23.60 36.80 39.6\nTracking Shu\ufb04ed Objects\u03bb(avg) 25.33 24.53 24.00 23.47 19.6\nWeb of Lies\u03bb49.20 52.40 54.00 51.20 51.2\nWord Sorting\u03bb4.80 5.20 2.40 7.60 32.0\nNLP Task (avg) 54.39 51.63 52.60 54.96 62.7\nAlgorithmic Task\u03bb(avg) 28.42 27.84 25.37 33.95 40.9\nAll Tasks (avg) 41.97 40.25 39.58 44.91 52.3\nAll Tasks (WR) 0.57 0.45 0.39 0.75 -\nTable 13: BIG-bench hard results using standard 3-shot prompting. Following the conven-\ntion from Suzgun et al. (2022), we denote algorithmic tasks with the superscript\u03bb,\nand present averages for NLP and algorithmic categories. The baseline numbers\nfrom PaLM 540B(Chowdhery et al., 2022) are taken from the original BBH paper.\n\u2022CommonsenseQA (Talmor et al., 2019): Multiple-choice QA dataset that requires\ndi\ufb00erent types of commonsense knowledge.\n\u2022MMLU (Hendrycks et al., 2021): Manually collected multiple-choice knowledge ques-\ntions in 57 subjects.\n\u2022PhysicalQA (PiQA, Bisk et al., 2020): Questions about how the physical world\nworks.\nBloombergGPT achieves the highest performance among BLOOM 176B, GPT-NeoX, and\nOPT 66Bin one task, and comes second in the other three (Table 14). Similar to the\nprevious section, it outperforms models of similar size while almost being on par with the\nmuch larger models. The Massive Multitask Language Understanding (MMLU, Hendrycks\net al., 2021) covers 57 di\ufb00erent subjects and thus has a much wider coverage than the tasks\ndescribed above.", "start_char_idx": 0, "end_char_idx": 2418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5996284b-6f5f-4520-8ac9-439f323ebc18": {"__data__": {"id_": "5996284b-6f5f-4520-8ac9-439f323ebc18", "embedding": null, "metadata": {"page_label": "27", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83be183d-be91-4545-9073-281e10ca37fe", "node_type": "4", "metadata": {"page_label": "27", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "16a9739fde37b45cea2a2b1af725f748c08877116a7afbb38f372b59922ae5e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c050c699-d858-4900-b333-bd529c18d160", "node_type": "1", "metadata": {"page_label": "27", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "a5093fcb10f368539732e9fe52f08644b32df696534db4bb7dfebf00d7a23cb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61beed51-8a07-46be-9b78-2746cd56a7ae", "node_type": "1", "metadata": {}, "hash": "2eeef3373f4f76f5ecd22800a64c6def70af1d57c28295c77e75eee49e429d9a", "class_name": "RelatedNodeInfo"}}, "text": "\u2022MMLU (Hendrycks et al., 2021): Manually collected multiple-choice knowledge ques-\ntions in 57 subjects.\n\u2022PhysicalQA (PiQA, Bisk et al., 2020): Questions about how the physical world\nworks.\nBloombergGPT achieves the highest performance among BLOOM 176B, GPT-NeoX, and\nOPT 66Bin one task, and comes second in the other three (Table 14). Similar to the\nprevious section, it outperforms models of similar size while almost being on par with the\nmuch larger models. The Massive Multitask Language Understanding (MMLU, Hendrycks\net al., 2021) covers 57 di\ufb00erent subjects and thus has a much wider coverage than the tasks\ndescribed above. The aggregated results in Table 15 paint a more consistent picture and\nfollow the insights seen in BIG-bench hard. BloombergGPT consistently outperforms\nOPT 66B, which in turn outperforms GPT-NeoX, while GPT-3 performs best. In contrast\n27", "start_char_idx": 1786, "end_char_idx": 2658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61beed51-8a07-46be-9b78-2746cd56a7ae": {"__data__": {"id_": "61beed51-8a07-46be-9b78-2746cd56a7ae", "embedding": null, "metadata": {"page_label": "28", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c921241-32ed-466a-bc70-20c987edd916", "node_type": "4", "metadata": {"page_label": "28", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "cda58b6b296ed54f7f9c83b702927ae18e29781c21517cd5354413d3304cd37c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5996284b-6f5f-4520-8ac9-439f323ebc18", "node_type": "1", "metadata": {"page_label": "27", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "064fb902138ce18c52015df29ac7bbf42ddcdb90f840a9e9ae5d80ed2811efae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8e8975e-d599-4977-8b24-8e8a6db4a85f", "node_type": "1", "metadata": {}, "hash": "d63d42190a213233b3f66079b31c059a9709c8ef2fbc09ab1484d995787dc2a0", "class_name": "RelatedNodeInfo"}}, "text": "Task BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3\nARC (easy) 73.99 70.79 71.25 75.93 71.2\nARC (challenging) 48.63 45.39 44.54 50.85 53.2\nCommonsenseQA 65.52 60.36 66.42 64.21 -\nPiQA 77.86 75.84 77.58 77.04 80.5\nAll Tasks (avg) 66.50 63.10 64.95 67.01 -\nAll Tasks (WR) 0.75 0.08 0.33 0.67 -\nTable 14: Knowledge tasks 1-shot results. The baseline numbers from GPT-3 are taken\nfrom Brown et al. (2020). Among all models, BloombergGPT achieves the\nhighest win rate among the models we ran ourselves, and performs second best\non average.\nModel BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3\nHumanities 36.26 32.75 33.28 34.05 40.8\nSTEM 35.12 33.43 30.72 36.75 36.7\nSocial Sciences 40.04 36.63 38.32 41.50 50.4\nOther 46.36 42.29 42.63 46.48 48.8\nAverage 39.18 35.95 35.99 39.13 43.9\nTable 15: Results (5-shot) on the MMLU (Hendrycks et al., 2021) benchmark. The base-\nline numbers from GPT-3 are taken from Hendrycks et al. (2021). While\nBloombergGPT lacks behind BLOOM 176Bon three of the categories, its aver-\nage is the highest among all models we evaluated ourselves. The gap to GPT-3\nis largest on social sciences while the performance in other categories is close.\nto the previous sections, BloombergGPT also outperforms BLOOM 176Bin this category,\nalthough by a slim margin. It falls behind the reported performance of GPT-3, especially in\nthe social science category. The gap to GPT-3 is closest in the STEM and \u201cOther\u201d domains\nwhich include \ufb01nance and accounting-related questions.\n5.6 Reading Comprehension\nWe de\ufb01ne reading comprehension benchmarks as tasks in which the model can generate the\ncorrect response based on information contained in the presented input text. Our grouping\nincludes open-book QA tasks, as opposed to Brown et al. (2020), who separate them into\na di\ufb00erent categories. We follow the template of Brown et al. (2020), and report accuracy.\nWe include the following tasks:\n\u2022BoolQ (Clark et al., 2019): Yes/No questions about a passage from Wikipedia.\n\u2022OpenBookQA (Mihaylov et al., 2018): Multiple-choice elementary-level science\nquestions, given a book of science facts, applied to new situations.\n\u2022RACE (Lai et al., 2017): A multiple choice dataset of middle and high school English\nexaminations.\n28", "start_char_idx": 0, "end_char_idx": 2227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8e8975e-d599-4977-8b24-8e8a6db4a85f": {"__data__": {"id_": "b8e8975e-d599-4977-8b24-8e8a6db4a85f", "embedding": null, "metadata": {"page_label": "29", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf11b47f-2cee-4f29-ac7e-ddba1fb44e9f", "node_type": "4", "metadata": {"page_label": "29", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "021a4a51deafd08ca9e47a18864cbec2592e195368514685f4064f9cb3062606", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61beed51-8a07-46be-9b78-2746cd56a7ae", "node_type": "1", "metadata": {"page_label": "28", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "cda58b6b296ed54f7f9c83b702927ae18e29781c21517cd5354413d3304cd37c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0e7e587-4d5c-4f68-9f37-24080beff268", "node_type": "1", "metadata": {}, "hash": "c40b608334ee1f8d13f821c9b4bd661f06475a2ca55ff29e4abc1406ddc77de7", "class_name": "RelatedNodeInfo"}}, "text": "RC Scenario BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3\nBoolQ 74.59 46.36 57.46 52.94 76.7\nOpenBookQA 51.60 44.20 58.00 47.20 58.8\nRACE (middle) 54.32 41.23 47.42 52.30 57.4\nRACE (high) 41.74 34.33 37.02 39.14 45.9\nMultiRC 62.29 22.86 18.80 26.65 72.9\nReCoRD 82.79 67.86 82.53 78.01 90.2\nAll Tasks (avg) 61.22 42.81 50.21 49.37 67.0\nAll Tasks (WR) 0.94 0.06 0.50 0.50 -\nTable 16: Reading Comprehension Results (1-shot). The baseline numbers from GPT-3 are\ntaken from Brown et al. (2020). BloombergGPT far outclasses the models we\nevaluated ourselves, and is slightly behind GPT-3.\n\u2022Multi-Sentence Reading Comprehension (MultiRC, Khashabi et al., 2018): Short\nparagraphs and multi-sentence questions.\n\u2022Reading Comprehension with Commonsense Reasoning (ReCoRD, Zhang\net al., 2018): Automatically generated questions about CNN and Daily Mail news\narticles.\nTable 16 re\ufb02ects a similar ranking as in the above evaluations: While GPT-3 has the\nhighest performance, BloombergGPT is a close second. Except for OpenBookQA, The\nperformance of BloombergGPT is the highest among BLOOM 176B, GPT-NeoX, and\nOPT 66B. Surprisingly, BLOOM 176B falls behind signi\ufb01cantly in this category.\n5.7 Linguistic Tasks\nWe de\ufb01ne as linguistic tasks those scenarios that are not directly connected to user-facing\napplications. These include tasks that evaluate disambiguation, grammar, or entailment.\nThese tasks are designed to directly assess a model\u2019s ability to understand language. We\nfollow the template of Brown et al. (2020), and report accuracy. The list of tasks is as\nfollows:\n\u2022Recognizing Textual Entailment (RTE, Dagan et al., 2007; Haim et al., 2006;\nGiampiccolo et al., 2007; Bentivogli et al., 2009): Given two text fragments, identify\nwhether the meaning of one text is entailed.\n\u2022Adversarial NLI (ANLI, Nie et al., 2020): Adversarially constructed entailment\ndetection.\n\u2022CommitmentBank (CB, De Marne\ufb00e et al., 2019): Naturally occurring discourses\nwhose \ufb01nal sentence contains a clause-embedding predicate.\n\u2022Choice of Plausible Alternatives (COPA, Gordon et al., 2011): Premise and two\nalternatives, where the task is to select the alternative that more plausibly has a causal\nrelation with the premise.\n\u2022Words in Context (WIC Pilehvar and Camacho-Collados, 2019): Determine if a\nword is being used with the same meaning in two sentences.\n29", "start_char_idx": 0, "end_char_idx": 2333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0e7e587-4d5c-4f68-9f37-24080beff268": {"__data__": {"id_": "d0e7e587-4d5c-4f68-9f37-24080beff268", "embedding": null, "metadata": {"page_label": "30", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "548c70c4-a5c2-4af8-919f-e598b3a1894a", "node_type": "4", "metadata": {"page_label": "30", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "a0b1954bbf18fcfaf76a375636bda8ec3d72ee2d7923b047fdf74efb7c46b2bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8e8975e-d599-4977-8b24-8e8a6db4a85f", "node_type": "1", "metadata": {"page_label": "29", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "021a4a51deafd08ca9e47a18864cbec2592e195368514685f4064f9cb3062606", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49b83f48-8d9a-4115-9a3b-227b892deeb1", "node_type": "1", "metadata": {}, "hash": "8e3707b9087045a4db9e96362b2fdbf8184acf59185f609d67f81608ed970b60", "class_name": "RelatedNodeInfo"}}, "text": "Linguistic Scenario BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3\nRTE 69.31 53.79 54.87 57.40 70.4\nANLI Round 1 32.90 32.60 33.10 33.60 32.0\nANLI Round 2 34.40 33.80 34.20 33.80 33.9\nANLI Round 3 37.33 36.17 34.92 35.17 35.1\nCB 53.57 48.21 44.64 48.21 64.3\nCOPA 86.00 88.00 86.00 84.00 87.0\nWIC 52.51 50.00 52.51 50.16 48.6\nWinoGrad 80.95 79.12 82.78 78.02 89.7\nWinoGrande 64.09 60.62 66.14 67.01 73.2\nHellaSWAG 73.92 68.37 73.47 73.21 78.1\nStoryCloze 80.87 78.30 81.83 80.28 84.7\nAll Tasks (avg) 60.63 57.18 58.59 58.26 63.4\nAll Tasks (WR) 0.85 0.27 0.58 0.42 -\nTable 17: Results on the Linguistic Scenarios (1-shot). The baseline numbers from GPT-\n3 are taken from Brown et al. (2020). Win rates and averages are computed\nonly based on accuracy numbers. BloombergGPT consistently scores highest\namong the models we evaluate, achieving an 85% win rate.\n\u2022Winograd (Levesque et al., 2011): Determine which word a pronoun refers to when\nit is semantically unambiguous.\n\u2022Winogrande (Sakaguchi et al., 2019): Adversarially mined challenging Winograd\nexamples.\n\u2022HellaSWAG (Zellers et al., 2019): Pick the best ending to a story or set of instruc-\ntions.\n\u2022StoryCloze (Mostafazadeh et al., 2016): Select the correct ending sentence for \ufb01ve-\nsentence long stories.\nThe results (Table 17) for linguistic tasks follow a similar trend to the knowledge category.\nBloombergGPT falls slightly behind GPT-3 and outperforms the other models. Similar\nto the reading comprehension category, BLOOM 176B falls behind BloombergGPT .\n5.8 Summary\nAcross dozens of tasks in many benchmarks a clear picture emerges. Among the models\nwith tens of billions of parameters that we compare to, BloombergGPT performs the\nbest. Furthermore, in some cases, it is competitive or exceeds the performance of much\nlarger models (hundreds of billions of parameters). While our goal for BloombergGPT\nwas to be a best-in-class model for \ufb01nancial tasks, and we included general-purpose training\ndata to support domain-speci\ufb01c training, the model has still attained abilities on general-\npurpose data that exceed similarly sized models, and in some cases match or outperform\nmuch larger models.\n30", "start_char_idx": 0, "end_char_idx": 2156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49b83f48-8d9a-4115-9a3b-227b892deeb1": {"__data__": {"id_": "49b83f48-8d9a-4115-9a3b-227b892deeb1", "embedding": null, "metadata": {"page_label": "31", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51f2fc71-6868-4970-b9e8-abe69179bd49", "node_type": "4", "metadata": {"page_label": "31", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d6e5cec2ebcba45398882d150e3f0a4ddb31c213d7c2cd3894ae4b8ba68be47f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0e7e587-4d5c-4f68-9f37-24080beff268", "node_type": "1", "metadata": {"page_label": "30", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "a0b1954bbf18fcfaf76a375636bda8ec3d72ee2d7923b047fdf74efb7c46b2bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fbff83f-69e2-4544-b216-51118f93f6ae", "node_type": "1", "metadata": {}, "hash": "d8ef867f7f8b2e37e716deeedb9bb8179888b94d83b6f9e78ae4b706eb6f343d", "class_name": "RelatedNodeInfo"}}, "text": "Input : Get me the last price and market cap for Apple\nOutput :get(px_last,cur_mkt_cap) for([\u2019AAPL US Equity\u2019])\nInput : Tesla price\nOutput :get(px_last) for([\u2019TSLA US Equity\u2019])\nInput : Get the yield and spread for EC527035 Corp and AL580550 Corp\nOutput :get(yield,spread) for([\u2019EC527035 Corp\u2019,\u2019AL580550 Corp\u2019])\nInput : apple and ibm market cap and eps\nOutput :get(cur_mkt_cap,is_eps) for([\u2019AAPL US Equity\u2019,\u2019IBM US Equity\u2019])\nInput : industry subgroup of ibm apple microsoft google\nOutput :get(industry_subgroup()) for([\u2019AAPL US Equity\u2019,\u2019IBM US Equity\u2019,\n\u2019MSFT US Equity\u2019,\u2019GOOGL US Equity\u2019])\nFigure 4: Using BloombergGPT to generate valid Bloomberg Query Language. Using\nonly a few examples in a few-shot setting, the model can utilize its knowledge\nabout stock tickers and \ufb01nancial terms to compose valid queries to retrieve the\ndata, given a request in natural language. In each case, the model is given 3\nexamples (not shown) followed by the \u2018Input\u201d and a prompt of \u201cOutput:\u201d.\n6 Qualitative Samples\nWe now share qualitative examples from our model that highlight the bene\ufb01ts of our domain\nspecialization.\nGeneration of Bloomberg Query Language. One use case for BloombergGPT is to\nmake interactions with \ufb01nancial data more natural. An existing way to retrieve data is via\nthe Bloomberg Query Language (BQL). BQL can be used to interact with di\ufb00erent classes\nof securities, each with its own \ufb01elds, functions, and parameters. BQL is an incredibly\npowerful but complex tool. As we show in Figure 4, BloombergGPT can be utilized to\nmake BQL more accessible by transforming natural language queries into valid BQL.\nSuggestion of News Headlines. Other use cases that are well supported are in the news\nspace. Since it is trained on many news articles, it can be used for many news applications\nand assist journalists in their day-to-day work. For example, when constructing newsletters,\njournalists may have to write short headlines for each new section. While a dedicated model\nto help with this task may be too expensive to maintain, BloombergGPT performs well\nout of the box (Figure 5).\nFinancial Question Answering. Due to the \ufb01nancial domain training data, we are able\nto query BloombergGPT for knowledge relevant to the \ufb01nancial world. For example, it\nperforms well at identifying the CEO of a company. Figure 6 shows several examples in-\ncluding output from other models. While BloombergGPT correctly identi\ufb01es the CEOs,\nGPT-NeoX does not, and FLAN-T5-XXL completely fails, consistently ignoring the com-\n31", "start_char_idx": 0, "end_char_idx": 2509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fbff83f-69e2-4544-b216-51118f93f6ae": {"__data__": {"id_": "4fbff83f-69e2-4544-b216-51118f93f6ae", "embedding": null, "metadata": {"page_label": "32", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a25a127-5a7b-46f9-9b2b-6eb046d2a48d", "node_type": "4", "metadata": {"page_label": "32", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e37dbf874c5f3ed07b00977b788c4e369f9dbda989afa19cab5e7814c96cc172", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49b83f48-8d9a-4115-9a3b-227b892deeb1", "node_type": "1", "metadata": {"page_label": "31", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d6e5cec2ebcba45398882d150e3f0a4ddb31c213d7c2cd3894ae4b8ba68be47f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4057834c-f144-4091-b762-156af184d7d3", "node_type": "1", "metadata": {}, "hash": "598aae5f47e0f4a13b9d1ca19148b77476a00aa50202f82c86294f8a3394574c", "class_name": "RelatedNodeInfo"}}, "text": "Input : The US housing market shrank in value by $2.3 trillion, or 4.9%, in the second\nhalf of 2022, according to Red\ufb01n. That\u2019s the largest drop in percentage terms since the\n2008 housing crisis, when values slumped 5.8% during the same period.\nOutput : Home Prices See Biggest Drop in 15 Years\nInput : The global economy is in a better place today than many predicted months ago,\nJanet Yellen said at the G20. At home, she pointed to a resilient US economy, where\nheadline in\ufb02ation has moderated and the labor market is strong. She also called for the\nIMF to move swiftly toward a fully-\ufb01nanced program for Ukraine.\nOutput : Yellen Sees Global Economy More Resilient Than Expected\nInput : Google was sued by the US and eight states seeking the breakup of its\nad-tech business for allegedly monopolizing the digital advertising market. The lawsuit\nis the Biden administration\u2019s \ufb01rst big challenge to a tech titan and one of the rare times\nsince 1982 that the DOJ has sought to cleave up a major company.\nOutput : Google Sued for Monopoly in Online Ad Market\nFigure 5: Using BloombergGPT to generate short headline suggestions in a three-shot\nsetting. Bloomberg News sends many newsletters a day that requires these head-\nlines. BloombergGPT could help with the editing process by suggesting initial\nheadlines from the text.\npany and instead predicting the CEO at Cirrus Logic who was included in the prompt.\nWhile BloombergGPT does not perfectly solve this task and makes mistakes, we were\nnot able to \ufb01nd any example where the other models solved the task while BloombergGPT\ndid not.\n7 Related Work\nLanguage Models. Language modeling has a long history in the NLP community. The\nidea of training a probabilistic language model for scoring word sequences was likely \ufb01rst\nintroduced by Jelinek (1976). N-gram models were popular for decades (Brown et al.,\n1992), and were trained on corpora up to 2 trillion tokens (Brants et al., 2007). Research\non training language models accelerated over the last decade due to innovations in machine\nlearning, data availability, and compute. Early work in autoregressive language modeling\n(e.g., Mikolov et al., 2010; Sutskever et al., 2011) used recurrent neural networks, but\nthese were small models trained on small datasets. The introduction of the transformer\narchitecture (Vaswani et al., 2017) facilitated the scaling of these models in terms of data,\ncompute, and the number of parameters.\nThe process of developing models that could better approximate the distribution of\nlanguage over large corpora led to the discovery that the representations these models\nproduce are useful starting points for many downstream tasks. This was demonstrated by\nRadford et al. (2018) and Howard and Ruder (2018) who showed that generative pretraining\n32", "start_char_idx": 0, "end_char_idx": 2783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4057834c-f144-4091-b762-156af184d7d3": {"__data__": {"id_": "4057834c-f144-4091-b762-156af184d7d3", "embedding": null, "metadata": {"page_label": "33", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c10ac9c0-2e0f-4be3-9b88-f8f332227633", "node_type": "4", "metadata": {"page_label": "33", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "07ad429e8bdb87bda07bacb2bc86259985b05c2fe5bb7912fa5dd618349fd2ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fbff83f-69e2-4544-b216-51118f93f6ae", "node_type": "1", "metadata": {"page_label": "32", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e37dbf874c5f3ed07b00977b788c4e369f9dbda989afa19cab5e7814c96cc172", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e54fcc51-9ffd-4486-a814-59483440aa4f", "node_type": "1", "metadata": {}, "hash": "4fa73de5b0c2506982f2393ca8acbf808347418fe411626569cd1c6dc782150a", "class_name": "RelatedNodeInfo"}}, "text": "Input : CEO of Assicurazioni Generali?\nBloombergGPT : Philippe Donnet\nGPT-NeoX : Antonio De Lorenzo, Simone Gambarini, Enrico Zanetti\nFLAN-T5-XXL : John M Forsyth, Christopher K Peters, {empty string}\nInput : CEO of Silicon Valley Bank?\nBloombergGPT : Greg Becker\nGPT-NeoX : John Hodor, Mike Fridgen, Peter R Briger\nFLAN-T5-XXL : Christian Klein, John M Forsyth, John M Forsyth\nInput : CEO of IHS Markit Ltd?\nBloombergGPT : Lance Uggla\nGPT-NeoX : John F Lechleiter, David Steward, Niall O Broin\nFLAN-T5-XXL : William M Hawkins, John M Forsyth, John M Forsyth\nInput : CEO of Walgreens Boots Alliance?\nBloombergGPT : Rosalind Brewer\nGPT-NeoX : Mark V Dei, Stefano Pessina, Stefano Pessina\nFLAN-T5-XXL : Christian Klein, John M Forsyth, John M Forsyth\nInput : CEO of Citigroup Inc?\nBloombergGPT : Jane Fraser\nGPT-NeoX : Michael L Corbat, Michael L Corbat, Michael L Corbat*\nFLAN-T5-XXL : Christian Sewing, John M Forsyth, John M Forsyth\nFigure 6: Testing the ability of BloombergGPT , GPT-NeoX, and FLAN-T5-XXL to re-\ncall the names of CEOs of companies. Each model is run in a 10-shot setting.\nWe sample up to three answers and present all of them if they are incorrect.\n*Michael Corbat was CEO of Citigroup until 2021, highlighting the importance\nof an up-to-date model.\nwith an autoregressive language modeling objective achieves strong performance in transfer\nlearning. Radford et al. (2019) further showed scaling the model size and training data led\nto autoregressive language models that perform well in di\ufb00erent downstream tasks without\nany additional supervised \ufb01ne-tuning.\nBrown et al. (2020) showed that further scaling the models led to the emergence of new\nmodel capabilities and increased model robustness. Since the release of GPT-3 by Brown\net al. (2020), many other researchers built large language models to study data quantity,\ndata quality, network architecture, parameter scaling, data scaling, tokenization, and open-\nsourcing strategies (Ra\ufb00el et al., 2020; Zhang et al., 2022a; Black et al., 2022; Rae et al.,\n2021; Ho\ufb00mann et al., 2022; Chowdhery et al., 2022; Lieber et al., 2021; Zeng et al., 2022;\nTafjord and Clark, 2021; Smith et al., 2022; Scao et al., 2022; Taylor et al., 2022; Lin et al.,\n33", "start_char_idx": 0, "end_char_idx": 2222, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e54fcc51-9ffd-4486-a814-59483440aa4f": {"__data__": {"id_": "e54fcc51-9ffd-4486-a814-59483440aa4f", "embedding": null, "metadata": {"page_label": "34", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce7abbe7-6965-4e78-8da4-ec33851d2224", "node_type": "4", "metadata": {"page_label": "34", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "5edb049a7f849bc33ab2bca4cf0ff25998c82a0ac52487b61e542746116b4b37", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4057834c-f144-4091-b762-156af184d7d3", "node_type": "1", "metadata": {"page_label": "33", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "07ad429e8bdb87bda07bacb2bc86259985b05c2fe5bb7912fa5dd618349fd2ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a864d87-bb21-4df9-b4fd-8c2ccc4d6023", "node_type": "1", "metadata": {}, "hash": "0b22eaae18e9d7c6c97ea3c4211674f1cdc5845d954ec46e7ff911e1802cba4b", "class_name": "RelatedNodeInfo"}}, "text": "2022; Soltan et al., 2022; Thoppilan et al., 2022; Bao et al., 2022; Sanh et al., 2022; Roller\net al., 2021; Glaese et al., 2022; Wang et al., 2021; Peng et al., 2022, among many others).\nDomain-Speci\ufb01c Large Language Models. The value of domain-speci\ufb01c training for\nmasked (encoder only) language models is well established. Commonly accepted approaches\nare to train BERT models (Devlin et al., 2019) from scratch on domain-speci\ufb01c data or to\ncontinue pretraining an existing model on new domain-speci\ufb01c data (Gururangan et al.,\n2020). Following these strategies, BioBERT (Lee et al., 2020) adapts BERT to the biomed-\nical domain and SciBERT is trained on scienti\ufb01c publications (Beltagy et al., 2019). The\nresults of these papers showed that in-domain training allows models to outperform previ-\nous state-of-the-art models in a variety of biomedical text mining tasks. Further examples\nof this paradigm are ClinicalBERT for the clinical domain (Huang et al., 2019), BioMed-\nRoBERTa for scienti\ufb01c biomedical papers (Gururangan et al., 2020), and BERTweet and\nBernice for Twitter data (Nguyen et al., 2020; DeLucia et al., 2022).\nSince the training of auto-regressive\u2014decoder-only\u2014language models of more than 10B\nparameters is signi\ufb01cantly more costly than training masked LMs under 1B parameters,\nthere have been much fewer examples of domain-speci\ufb01c autoregressive models. However,\nexisting approaches follow the same two strategies. Adapting an existing model, medPaLM\n(Singhal et al., 2022) adapted PaLM to the biomedical domain and Minerva (Lewkowycz\net al., 2022) to mathematical reasoning tasks.\nRecently, several examples of from-scratch trained decoder-only models for domain-\nspeci\ufb01c data have emerged. One popular domain is protein sequences since they can be\nrepresented using language-like sequences but are not covered by natural language mod-\nels (e.g., Lin et al., 2022; Xiao et al., 2021; Nijkamp et al., 2022). However, there can be\nbene\ufb01ts even for models in natural language domains. Galactica is trained exclusively on\na large collection of scienti\ufb01c datasets, and includes special processing to handle scienti\ufb01c\nnotations (Taylor et al., 2022). While performing very well on scienti\ufb01c tasks, Galactica\nalso surprisingly also performs well on more standard NLP tasks. BioGPT (Luo et al.,\n2022) and BioMedLM (Bolton et al., 2023) are both smaller GPT-style models trained on\nbiomedical data. Lehman et al. (2023) compares encoder/decoder models trained exclusively\non domain-speci\ufb01c data, versus those adapted from general-purpose training. Researchers\nworking on large generative language dialog models have reached similar conclusions about\nthe bene\ufb01ts of using domain-speci\ufb01c training data (Zhang et al., 2020; Roller et al., 2021;\nThoppilan et al., 2022).\nThese \ufb01ndings highlight the advantages of in-domain pretraining, especially if su\ufb03cient\ndata is available, as it is in our case. Inspired by the general capabilities of Galactica, we\naugment our private data with public data with the goal of investigating whether a model\ncan gain in-domain capabilities without sacri\ufb01cing general-domain performance.\nTraining Data. Large corpora of raw text data are critical for training LLMs. As a\nresult, there are now several corpora available that cover a wide range of sources.\nThe Colossal Clean Crawled Corpus (C4, Ra\ufb00el et al., 2020) draws from Common Crawl\nto create a processed training corpus. The Pile is a carefully curated corpus that contains\na wide range of data sources (Gao et al., 2021). These datasets are built on or include\nweb crawls (OpenWebText2) augmented with an array of data from high-quality sources\n(Pubmed, Arxiv). Various e\ufb00orts aim to clean datasets, especially web data, by removing\n34", "start_char_idx": 0, "end_char_idx": 3737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a864d87-bb21-4df9-b4fd-8c2ccc4d6023": {"__data__": {"id_": "0a864d87-bb21-4df9-b4fd-8c2ccc4d6023", "embedding": null, "metadata": {"page_label": "35", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b56aac4d-70fb-4af1-9dc2-1159e6f70a90", "node_type": "4", "metadata": {"page_label": "35", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "a4f133c92f9e599be72f6a87c8f4100442e12f37f9a66b745efcc8c2f953773e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e54fcc51-9ffd-4486-a814-59483440aa4f", "node_type": "1", "metadata": {"page_label": "34", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "5edb049a7f849bc33ab2bca4cf0ff25998c82a0ac52487b61e542746116b4b37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd927af0-be9b-4059-9f49-ffc85c80591e", "node_type": "1", "metadata": {}, "hash": "30a2cb389ef054506bd1febda60fdad45b3f2a9de95a88304ae52c280e14a4dd", "class_name": "RelatedNodeInfo"}}, "text": "unwanted or harmful text (Touvron et al., 2023; Rae et al., 2020). BLOOM (Scao et al.,\n2022) carefully selected data sources and included various \ufb01ltering mechanisms (Jernite\net al., 2022).\nWhile web data is an e\ufb00ective strategy for obtaining large amounts of diverse data,\nrobust cleaning e\ufb00orts still result in data artifacts, duplicates (Carlini et al., 2020), various\ntypes of toxic language (Welbl et al., 2021), and it can lead to unintended marginalization\nof minority voices (Xu et al., 2021). Dodge et al. (2021) studied C4 to better understand\nthe metadata, and the included and excluded data. Their \ufb01ndings suggest that C4 contains\nmachine-generated text, is biased due to exclusion \ufb01lters and might contain examples drawn\nfrom evaluation datasets for NLP tasks. A similar e\ufb00ort was undertaken by Zeng et al.\n(2022) to document the pre-processing they undertook to train their Chinese large language\nmodel.\nLee et al. (2022a) investigated the impact of deduplication on model performance for\nseveral datasets and found that deduplication reduces the emission of memorized training\ndata, allows better estimation of the generalization error, and improves training time and\ncost without impacting performance. These insights highlight the importance and challenges\nof constructing high-quality training corpora. As discussed in \u00a72, Bloomberg\u2019s core business\ncurates and provides access to datasets, which we use to construct a high-quality dataset\nFinPile to train BloombergGPT , resulting in best-in-class \ufb01nancial performance.\nEvaluation. The tasks addressed by language models have vastly increased and require a\nvery di\ufb00erent evaluation process from traditional task-speci\ufb01c systems. There have been two\nparadigms for LLM evaluation: The \ufb01rst is to evaluate a model in many di\ufb00erent scenarios\nvia automatic evaluation (Liang et al., 2022; Srivastava et al., 2022) and the second is to\nperform extrinsic and task-speci\ufb01c evaluations by integrating them into user work\ufb02ows (e.g.,\nLee et al., 2022b; Goyal et al., 2022).\nWhile the second strategy is necessary for assessing deployments of models in products,\nit is infeasible to run these human evaluations at a scale of the \ufb01rst strategy and it is thus\nstandard to follow the \ufb01rst strategy when introducing new models. In our case, we combine\nmultiple general-purpose evaluations from multiple existing benchmarks that have di\ufb00erent\ngoals. Srivastava et al. (2022) aim for maximum coverage by soliciting tasks from the\nentire research community, while HELM (Liang et al., 2022) suggests evaluation in various\n\u201cscenarios\u201d that are represented through speci\ufb01c datasets. Earlier language model papers\ndeveloped their own evaluation schemata (Brown et al., 2020). While these benchmarks\nallow for a side-by-side comparison between models, it is challenging to ensure that all\nexperimental parameters (prompts, decoding strategies, few-shot examples, etc.) are the\nsame. For that reason, we di\ufb00erentiate between reported and veri\ufb01ed numbers in our\nevaluation ( \u00a75).\nBeyond the general-purpose evaluation, we also require a targeted domain evaluation.\nPrior domain-speci\ufb01c models like Galactica (Taylor et al., 2022) chose a set of tasks that\nthe model is likely to perform well on. In their case, these were various scienti\ufb01c tasks.\nHowever, there exists no standard benchmark for the \ufb01nancial NLP domain. While the\nrecent work on FLUE (Shah et al., 2022) aims to provide such a benchmark, it has limited\ncoverage of relevant tasks, no suggested evaluation strategy for few-shot learning, and the\nquality of some annotations is low. To provide externally comparable results, we developed\n35", "start_char_idx": 0, "end_char_idx": 3646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd927af0-be9b-4059-9f49-ffc85c80591e": {"__data__": {"id_": "dd927af0-be9b-4059-9f49-ffc85c80591e", "embedding": null, "metadata": {"page_label": "36", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f0cce281-4ff8-4f1f-ae48-fb2b43dcb158", "node_type": "4", "metadata": {"page_label": "36", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "f612fd0aef14d38ab4ef708f70264f7e71555ea5e34de5722615da52ded9aadf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a864d87-bb21-4df9-b4fd-8c2ccc4d6023", "node_type": "1", "metadata": {"page_label": "35", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "a4f133c92f9e599be72f6a87c8f4100442e12f37f9a66b745efcc8c2f953773e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3142acbf-d7fe-4ffe-b876-1ef3639689d1", "node_type": "1", "metadata": {}, "hash": "7dc69f7cb7e613e85d628670da4c43d3feb5f4856ccf5093c6f969c2c0b0d524", "class_name": "RelatedNodeInfo"}}, "text": "a few-shot strategy for FLUE, but also decided to augment the publicly available evaluation\ntasks with company-internal benchmarks.\nModel Size. Large language model training remains expensive in terms of the compu-\ntational cost and human e\ufb00ort to assemble data and train the model. Determining the\noptimal amount of training data and model shape and size for the best utilization of re-\nsources becomes important.\nKaplan et al. (2020) \ufb01rst studied the dependence of language model performance on\narchitecture, parameter size, compute power, and dataset size. They reported that the\nnumber of model parameters, the dataset size, and the amount of compute improves perfor-\nmance on the autoregressive language modeling objective smoothly according to the power\nlaw. A similar investigation by Hernandez et al. (2021) into data transfer for di\ufb00ering dis-\ntributions found that this also follows a power law. Moving beyond studying the e\ufb00ect on\nloss, Rae et al. (2021) analyzed the e\ufb00ect of scale on undesirable properties such as bias\nand toxicity by training a wide range of model sizes.\nComparing model architectures, Levine et al. (2020) studied the scaling of models that\nuse self-attention and derived guidelines for depth-to-width allocation. Tay et al. (2021)\nreported that model shape (depth-width ratio) impacted performance on downstream tasks\neven if it had minimal impact on the pretraining objective. Tay et al. (2022a) further\nstudied the e\ufb00ect of scaling for di\ufb00erent model architectures and showed that architecture\nchoice is pertinent when scaling and that the vanilla transformer architecture scales best.\nOf particular importance to this work is the study of Ho\ufb00mann et al. (2022), who inves-\ntigated the e\ufb00ect of model size and the number of training tokens on the performance of a\nmodel given a \ufb01xed compute budget. They posited that existing large language models were\nundertrained and that model size and the number of training tokens should be scaled equally.\nThey demonstrated this hypothesis through Chinchilla, a model signi\ufb01cantly smaller, yet\nhigher performing, than most of the largest LLMs. These \ufb01ndings opened the door for\n\u201cChinchilla optimal\u201d training of smaller models that achieve strong performance, and for\nwhich inference can be run much more e\ufb03ciently than for their larger counterparts. These\n\ufb01ndings led us to consider a nearly Chinchilla-optimal model using a standard architecture.\nTokenization. Tokenization and vocabulary choice play a critical role in model perfor-\nmance as they can help the model learn meaningful representations and generalize to unseen\nwords. Byte-Pair encoding (BPE) (Sennrich et al., 2016) learns a greedy bottom-up vo-\ncabulary by repeatedly merging the most frequent sequence pairs in the training set till a\npredetermined vocabulary size is reached. Radford et al. (2018) adapted BPE by limiting\nthe base vocabulary to be all possible bytes as opposed to all Unicode characters. Wordpiece\ntokenization (Schuster and Nakajima, 2012) also learns a greedy bottom-up vocabulary by\nrepeatedly merging the sequence-pair that maximizes the likelihood of the training data,\nwhich is a slight deviation from the method in Sennrich et al. (2016).\nIn contrast to BPE and Wordpiece, the Unigram tokenizer (Kudo, 2018) learns a top-\ndown vocabulary by \ufb01rst initializing a large vocabulary and repeatedly discarding those\nvocabulary items that increase loss (e.g., log-likelihood of the training data) the least. By\nconstruction, the Unigram model can tokenize an input text in several di\ufb00erent ways. That\nis, the Unigram model saves probabilities allowing for smarter tokenization at inference time.\n36", "start_char_idx": 0, "end_char_idx": 3660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3142acbf-d7fe-4ffe-b876-1ef3639689d1": {"__data__": {"id_": "3142acbf-d7fe-4ffe-b876-1ef3639689d1", "embedding": null, "metadata": {"page_label": "37", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7a40f37-e82b-4365-b89b-4cc973ccc7d0", "node_type": "4", "metadata": {"page_label": "37", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "492baf2cdb7fa78588600907084d45eecb6715b25e3d867bc3eca459ddeb69b4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd927af0-be9b-4059-9f49-ffc85c80591e", "node_type": "1", "metadata": {"page_label": "36", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "f612fd0aef14d38ab4ef708f70264f7e71555ea5e34de5722615da52ded9aadf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b24cc7e5-9e3b-44c0-a98f-a4af24fc870c", "node_type": "1", "metadata": {}, "hash": "e98432055e776e1e484fcbe1d0c3febc27b42f520eb822522bec3ed0b8344d10", "class_name": "RelatedNodeInfo"}}, "text": "Finally, SentencePiece (Kudo and Richardson, 2018) adapts the schemes mentioned\nabove to handle languages that are not space separated. Beltagy et al. (2019) constructed a\nvocabulary speci\ufb01c to scienti\ufb01c text and observed that their domain-speci\ufb01c trained vocab-\nulary only had a 42% overlap with the non-domain-speci\ufb01c BERT vocabulary trained on\ngeneral domain text. Similarly, Lewis et al. (2020) showed that a dedicated biomedical vo-\ncabulary improved performance on sequence labeling tasks consistently. Lieber et al. (2021)\nconstructed a larger vocabulary to ensure token e\ufb03ciency, which the authors claim resulted\nin reduced training time and better semantic representation. These \ufb01ndings demonstrate\nthe importance of selecting a tokenizer and accompanying vocabulary that best re\ufb02ects that\ntraining domain. For those reasons, we decided to train our own unigram tokenizer instead\nof relying on existing public ones.\nPositional Embeddings. Transformer-based models rely on positional embeddings to\nencode position and location information of words in a text. Encoding the sequence posi-\ntion and the e\ufb00ect of this choice on model performance have been studied extensively. These\ninclude sinusoidal embeddings (Vaswani et al., 2017), rotary position embeddings (Su et al.,\n2021a), adding relative position bias (Ra\ufb00el et al., 2020), and adding linear biases to atten-\ntion heads (Press et al., 2022). A side-e\ufb00ect of the strategy in Press et al. (2022) is that one\ncan train on shorter sequences without loss in performance on longer sequences. This has\ntwo bene\ufb01ts: \ufb01rst, models learn to generalize (extrapolate) to longer sequences and second,\nmodels can be trained on shorter sequences reducing training time.\n8 Ethics, Limitations, and Implications\nThe rapid development and adoption of large language models have been accompanied by\na rigorous conversation about the ethics, uses, and limitations of these models. For a more\ncomplete treatment of these topics, we direct the reader to Bommasani et al. (2021); Bender\net al. (2021); Birhane et al. (2022); Weidinger et al. (2021, 2022). We discuss issues that\nare directly relevant to the development of BloombergGPT .\n8.1 Ethical Use\nFinance is a sensitive area for technology, and ensuring accurate, factual information is\ncrucial for our products, our clients, and the \ufb01rm\u2019s reputation in the marketplace. On\nthe other hand, our clients are also eager to adopt state-of-the-art technology to support\ntheir work\ufb02ows. To provide natural language applications to the \ufb01nancial community, we\nhave developed a rigorous risk and testing assessment process. This process includes careful\nannotation guidelines (Tseng et al., 2020), pre-launch review at multiple levels by the central\nrisk and compliance organizations, and by the product leaders (e.g., the newsroom) as\napplicable, and post-launch monitoring. Moreover, we conduct our research, development,\nand deployment of NLP and AI systems in accordance with all applicable regulations.\nSimilarly, toxicity and bias are areas where, as a company, we take extraordinary care\nwith any content we produce, whether from humans or machines. Since the measurement of\ntoxicity and bias in our model depends on its application areas, quantifying the potential for\nthe generation of harmful language remains an open question. We are particularly interested\nin studying whether FinPile , which is cleaner and contains fewer examples of overtly biased\n37", "start_char_idx": 0, "end_char_idx": 3454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b24cc7e5-9e3b-44c0-a98f-a4af24fc870c": {"__data__": {"id_": "b24cc7e5-9e3b-44c0-a98f-a4af24fc870c", "embedding": null, "metadata": {"page_label": "38", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a969031d-199f-4d4f-a121-9e31ec421446", "node_type": "4", "metadata": {"page_label": "38", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "3726e6919f4bff88372811a5b56c69b233d709800f17f81ed1d3c9c6609faf8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3142acbf-d7fe-4ffe-b876-1ef3639689d1", "node_type": "1", "metadata": {"page_label": "37", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "492baf2cdb7fa78588600907084d45eecb6715b25e3d867bc3eca459ddeb69b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ad967cb-c751-486e-94bf-25c8263a6d54", "node_type": "1", "metadata": {}, "hash": "e5e407d86d8a0ea6413189a49103ecb98e54522e614578166158f86abcf0dd52", "class_name": "RelatedNodeInfo"}}, "text": "or toxic language (e.g., Press Releases), reduces the proclivity of the model to generate\ninappropriate content. As we move to develop products built on this technology, we will\napply existing testing procedures, as well as risk and compliance controls, to ensure safe use.\n8.2 Openness\nAn ongoing debate in the community concerns how LLMs should be released, if at all. While\nmodels that are not publicly available cannot be fully evaluated by the community, distribut-\ning models can lead to nefarious purposes. Especially for a model like BloombergGPT ,\nwhich is trained on a signi\ufb01cant amount of press releases, news articles, and \ufb01lings, a release\ncarries a high risk for abuse through imitation.\nWe have witnessed many di\ufb00erent strategies to mitigate risks associated with the release\nof LLMs. One strategy is to freely and openly share trained models (Scao et al., 2022), and\nrely on a license that dictates how the model should and should not be used. Another\nrequires individuals to apply for access to the trained model parameters (Zhang et al.,\n2022a; Touvron et al., 2023). A more restrictive approach is to provide API access to\nmodels, but no access to the underlying model parameters or detailed information on the\ndata the model was trained on (Brown et al., 2020). Finally, some have provided no access\nto the model (Chowdhery et al., 2022; Ho\ufb00mann et al., 2022). Each decision re\ufb02ects a\ncombination of factors, including model use, potential harms, and business decisions.\nOne of Bloomberg\u2019s core business propositions is around providing access to data that\nhas been collected over the course of decades. As is well known, LLMs are susceptible to\ndata leakage attacks and it is possible to extract signi\ufb01cant segments of text given model\nweights (Carlini et al., 2020, 2022). Moreover, even giving selective access to researchers\nisn\u2019t a guarantee that the model cannot be leaked. Without strong privacy guarantees, we\nmust be concerned that providing access to model weights entails giving access to FinPile .\nFor this reason, we err on the side of caution and follow the practice of other LLM developers\nin not releasing our model.\nNevertheless, our insights and experiences in training and evaluating BloombergGPT\ncontribute to the developing understanding of these models. In particular, our experience\nmay be useful to those building domain-speci\ufb01c models. During the process of developing\nBloombergGPT , we found the OPT chronicles, experiences of the BLOOM team, as\nwell as work of non-open models like GPT-3, PaLM, Chinchilla, and Gopher, to be crucial\nenablers of our work. In support of this tradition, we include our Training Chronicles\n(Appendix C).\n9 Conclusion\nWe have presented BloombergGPT , a best-in-class LLM for \ufb01nancial NLP.\nOur model contributes to the ongoing dialog on e\ufb00ective ways to train domain-speci\ufb01c\nmodels. Our training strategy of mixing domain-speci\ufb01c and general-purpose data results in\na model that balances performance in both domains. Additionally, our work o\ufb00ers another\ndata point on selecting Chinchilla optimal-sized models. Finally, we hope that our model\ntraining logs will provide a guide for those training their own LLMs.\nWe have several interesting directions to pursue. First, task \ufb01ne-tuning has yielded\nsigni\ufb01cant improvements in LLMs, and we plan to consider what unique opportunities exist\n38", "start_char_idx": 0, "end_char_idx": 3364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ad967cb-c751-486e-94bf-25c8263a6d54": {"__data__": {"id_": "7ad967cb-c751-486e-94bf-25c8263a6d54", "embedding": null, "metadata": {"page_label": "39", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ddf8e06-3dfa-4453-8241-b3ccdb1e4686", "node_type": "4", "metadata": {"page_label": "39", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "da154cf33cfe398b66c6f852793a02a3d2ab1def196a609b13f4fbb94df9011b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b24cc7e5-9e3b-44c0-a98f-a4af24fc870c", "node_type": "1", "metadata": {"page_label": "38", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "3726e6919f4bff88372811a5b56c69b233d709800f17f81ed1d3c9c6609faf8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff9e2105-0efc-4af2-aad8-655238572e31", "node_type": "1", "metadata": {}, "hash": "fe60a30d6febb5143fb3134c79babfd9da39fdfc62b11e8591422f3987039a7b", "class_name": "RelatedNodeInfo"}}, "text": "for model alignment in the \ufb01nancial domain (Wei et al., 2021; Ouyang et al., 2022). Second,\nby training on data in FinPile , we are selecting data that may exhibit less toxic and biased\nlanguage. The e\ufb00ects of this on the \ufb01nal model are unknown as yet, which we plan to test.\nThird, we seek to understand how our tokenization strategy changes the resulting model.\nThese are a few of the new research directions we hope to pursue with BloombergGPT .\nWe achieve strong results on general LLM benchmarks and outperform comparable mod-\nels on \ufb01nancial tasks. We attribute this, in decreasing order of impact, to 1. a well-curated\ninternal dataset, 2. our unique choice in tokenizer, and 3. an up-to-date architecture. We\nwill continue to develop \ufb01nancial applications with BloombergGPT to further explore\nthe bene\ufb01ts of these modeling choices.\nAcknowledgments and Disclosure of Funding\nWe would like to acknowledge the people who helped us, including Emmanuel Scoullos\n(NVIDIA) and Can Karakus (Amazon Web Services).\nReferences\nDogu Araci. Finbert: Financial sentiment analysis with pre-trained language models. arXiV\npreprint arXiV:1908.10063 , 2019.\nSiqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhihua Wu, Zhen\nGuo, Hua Lu, Xinxian Huang, Xin Tian, Xinchao Xu, Yingzhan Lin, and Zheng-Yu Niu.\nPLATO-XL: Exploring the large-scale pre-training of dialogue generation. In Findings\nof the Association for Computational Linguistics: AACL-IJCNLP 2022 , pages 107\u2013118,\nOnline only, November 2022. Association for Computational Linguistics. URL https:\n//aclanthology.org/2022.findings-aacl.10 .\nIz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for\nscienti\ufb01c text. In Proceedings of the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3615\u20133620, Hong Kong, China, November\n2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1371. URL\nhttps://aclanthology.org/D19-1371 .\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.\nOn the dangers of stochastic parrots: Can language models be too big? In Proceedings of\nthe 2021 ACM conference on fairness, accountability, and transparency , pages 610\u2013623,\n2021.\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo.\nThe \ufb01fth PASCAL recognizing textual entailment challenge. In Proceedings of the Sec-\nond Text Analysis Conference, TAC 2009, Gaithersburg, Maryland, USA, November 16-\n17, 2009 . NIST, 2009. URL https://tac.nist.gov/publications/2009/additional.\npapers/RTE5_overview.proceedings.pdf .\n39", "start_char_idx": 0, "end_char_idx": 2680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff9e2105-0efc-4af2-aad8-655238572e31": {"__data__": {"id_": "ff9e2105-0efc-4af2-aad8-655238572e31", "embedding": null, "metadata": {"page_label": "40", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d0f1663-f311-4760-bf06-5446e5a11ac8", "node_type": "4", "metadata": {"page_label": "40", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "b005ac7b590cf814c8efb7a8a8b4d93bceb7fdcbcfef1af3ff6e924fdd7df57a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ad967cb-c751-486e-94bf-25c8263a6d54", "node_type": "1", "metadata": {"page_label": "39", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "da154cf33cfe398b66c6f852793a02a3d2ab1def196a609b13f4fbb94df9011b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "caa9f08c-26a6-4152-aa43-33bb82691896", "node_type": "1", "metadata": {}, "hash": "6541b6aa8e7675976374d3c92ae88513970d70663ba9220fa1a07b882d69553f", "class_name": "RelatedNodeInfo"}}, "text": "Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle\nBao. The values encoded in machine learning research. In 2022 ACM Conference on\nFairness, Accountability, and Transparency , pages 173\u2013184, 2022.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA:\nreasoning about physical commonsense in natural language. In The Thirty-Fourth\nAAAI Conference on Arti\ufb01cial Intelligence, AAAI 2020, The Thirty-Second Innova-\ntive Applications of Arti\ufb01cial Intelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Arti\ufb01cial Intelligence, EAAI 2020, New York,\nNY, USA, February 7-12, 2020 , pages 7432\u20137439. AAAI Press, 2020. URL https:\n//ojs.aaai.org/index.php/AAAI/article/view/6239 .\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale\nAutoregressive Language Modeling with Mesh-Tensor\ufb02ow, March 2021. URL https:\n//doi.org/10.5281/zenodo.5297715 . If you use this software, please cite it using these\nmetadata.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence\nGolding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler,\nUsvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang,\nand Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model.\nInProceedings of BigScience Episode #5 \u2013 Workshop on Challenges & Perspectives\nin Creating Large Language Models , pages 95\u2013136, virtual+Dublin, May 2022. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9. URL\nhttps://aclanthology.org/2022.bigscience-1.9 .\nElliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy Liang.\nBioMedLM. https://github.com/stanford-crfm/BioMedLM , 2023.", "start_char_idx": 0, "end_char_idx": 1783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "caa9f08c-26a6-4152-aa43-33bb82691896": {"__data__": {"id_": "caa9f08c-26a6-4152-aa43-33bb82691896", "embedding": null, "metadata": {"page_label": "40", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d0f1663-f311-4760-bf06-5446e5a11ac8", "node_type": "4", "metadata": {"page_label": "40", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "b005ac7b590cf814c8efb7a8a8b4d93bceb7fdcbcfef1af3ff6e924fdd7df57a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff9e2105-0efc-4af2-aad8-655238572e31", "node_type": "1", "metadata": {"page_label": "40", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "31d9ae76a5552ecc7dc8a514f3d95a35a98981c75b96a3a87fbc8bb86ae669be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c6d3478-0708-466d-8879-612b1c553116", "node_type": "1", "metadata": {}, "hash": "1ef5f23eb6259b266a1e681c859f074d2d00e9f92c83dcd5704705a5c765f89a", "class_name": "RelatedNodeInfo"}}, "text": "GPT-NeoX-20B: An open-source autoregressive language model.\nInProceedings of BigScience Episode #5 \u2013 Workshop on Challenges & Perspectives\nin Creating Large Language Models , pages 95\u2013136, virtual+Dublin, May 2022. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9. URL\nhttps://aclanthology.org/2022.bigscience-1.9 .\nElliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy Liang.\nBioMedLM. https://github.com/stanford-crfm/BioMedLM , 2023.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney\nvon Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik\nBrynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S.\nChen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doum-\nbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei,\nChelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby\nGrossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E.\nHo, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Juraf-\nsky, Pratyusha Kalluri, Siddharth Karamcheti, Geo\ufb00 Keeling, Fereshte Khani, O. Khat-\ntab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Ku-\nmar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa\nLi, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani,\nEric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben-\njamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray\nOgut, Laurel J. Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Porte-\nlance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong,\nYusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R\u2019e, Dorsa Sadigh, Shiori\nSagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin,\n40", "start_char_idx": 1286, "end_char_idx": 3270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c6d3478-0708-466d-8879-612b1c553116": {"__data__": {"id_": "4c6d3478-0708-466d-8879-612b1c553116", "embedding": null, "metadata": {"page_label": "41", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72441c3d-08f1-4370-b886-13ae6988801b", "node_type": "4", "metadata": {"page_label": "41", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "41a6b10258299c0ebc7581839eabf187b822e3c6b78a726d40eb9c08e8b899c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "caa9f08c-26a6-4152-aa43-33bb82691896", "node_type": "1", "metadata": {"page_label": "40", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "afb86c45f1300e41a22389fc4ac1367eb1c77c6544449ff0835f4f139081d36f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ce758f1-3382-434c-b822-e3df25c82dd1", "node_type": "1", "metadata": {}, "hash": "05ebcd3ccd03fccbb0b14b6d73e9a0c3f4b8ae4a8328d0bbe91ed4498f7ac1fa", "class_name": "RelatedNodeInfo"}}, "text": "Rohan Taori, Armin W. Thomas, Florian Tram` er, Rose E. Wang, William Wang, Bo-\nhan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You,\nMatei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia\nZheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation\nmodels. ArXiV , abs/2108.07258, 2021.\nKaj Bostrom and Greg Durrett. Byte pair encoding is suboptimal for language model\npretraining. In Findings of the Association for Computational Linguistics: EMNLP\n2020, pages 4617\u20134624, Online, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.\ufb01ndings-emnlp.414. URL https://aclanthology.org/\n2020.findings-emnlp.414 .\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Je\ufb00rey Dean. Large language\nmodels in machine translation. In Proceedings of the 2007 Joint Conference on Empirical\nMethods in Natural Language Processing and Computational Natural Language Learning\n(EMNLP-CoNLL) , pages 858\u2013867, Prague, Czech Republic, June 2007. Association for\nComputational Linguistics. URL https://aclanthology.org/D07-1090 .\nPeter F Brown, Vincent J Della Pietra, Peter V Desouza, Jennifer C Lai, and Robert L\nMercer. Class-based n-gram models of natural language. Computational linguistics , 18\n(4):467\u2013480, 1992.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel M. Ziegler, Je\ufb00rey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are\nfew-shot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina\nBalcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/\n2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html .\nNicholas Carlini, Florian Tram` er, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss,\nKatherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, \u00b4Ulfar Erlingsson,\nAlina Oprea, and Colin Ra\ufb00el. Extracting training data from large language models. In\nUSENIX Security Symposium , 2020.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and\nChiyuan Zhang. Quantifying memorization across neural language models, 2022. URL\nhttps://arxiv.org/abs/2202.07646 .\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan,\nHarrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke\nChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\nhammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W.\n41", "start_char_idx": 0, "end_char_idx": 3117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ce758f1-3382-434c-b822-e3df25c82dd1": {"__data__": {"id_": "0ce758f1-3382-434c-b822-e3df25c82dd1", "embedding": null, "metadata": {"page_label": "42", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0bf30a6-9b8b-477b-8ce8-3e76e373ff84", "node_type": "4", "metadata": {"page_label": "42", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "212292e737dae8429e5724a41ff9caadcf11a6701909591efe24fc54027d5316", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c6d3478-0708-466d-8879-612b1c553116", "node_type": "1", "metadata": {"page_label": "41", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "41a6b10258299c0ebc7581839eabf187b822e3c6b78a726d40eb9c08e8b899c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30c1a09a-883d-4149-aa13-7ad1fec264cd", "node_type": "1", "metadata": {}, "hash": "83b6d31e6c22da338ca693a98c78c70c26bbd5b9fb0d169f27423f181f9e1268", "class_name": "RelatedNodeInfo"}}, "text": "Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss,\nWilliam H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, An-\ndrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford,\nMatthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Eval-\nuating large language models trained on code. arXiV , abs/2107.03374, 2021a.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with\nsublinear memory cost. arXiV preprint arXiV:1604.06174 , 2016.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon,\nReema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang\nWang. FinQA: A dataset of numerical reasoning over \ufb01nancial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language Processing , pages\n3697\u20133711, Online and Punta Cana, Dominican Republic, November 2021b. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.300. URL https:\n//aclanthology.org/2021.emnlp-main.300 .\nZhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang\nWang. ConvFinQA: Exploring the chain of numerical reasoning in conversational \ufb01nance\nquestion answering. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing , pages 6279\u20136292, Abu Dhabi, United Arab Emirates, De-\ncember 2022. Association for Computational Linguistics. URL https://aclanthology.\norg/2022.emnlp-main.421 .\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben-\nton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy\nGur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,\nHenryk Michalewski, Xavier Garc\u00b4 \u0131a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark D\u00b4 \u0131az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Dou-\nglas Eck, Je\ufb00 Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling\nwith pathways. arXiV , abs/2204.02311, 2022.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. BoolQ: Exploring the surprising di\ufb03culty of natural yes/no ques-\ntions. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2924\u20132936, Minneapolis, Minnesota, June 2019. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https:\n//aclanthology.org/N19-1300 .\n42", "start_char_idx": 0, "end_char_idx": 3245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30c1a09a-883d-4149-aa13-7ad1fec264cd": {"__data__": {"id_": "30c1a09a-883d-4149-aa13-7ad1fec264cd", "embedding": null, "metadata": {"page_label": "43", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76ad06c5-5675-4484-a6a0-d18b08d4ec51", "node_type": "4", "metadata": {"page_label": "43", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "34974354506f03d0e22c8df748bc1ca4b4df17850e2fe7cb71a42c2257ceb55f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ce758f1-3382-434c-b822-e3df25c82dd1", "node_type": "1", "metadata": {"page_label": "42", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "212292e737dae8429e5724a41ff9caadcf11a6701909591efe24fc54027d5316", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e9b55c4-d95f-47e9-ac5d-7185664aae28", "node_type": "1", "metadata": {}, "hash": "99c85d2519257e07b22b0a16cfea9062ec80096fd8c466e4af15803ad9585977", "class_name": "RelatedNodeInfo"}}, "text": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa\nSchoenick, and Oyvind Tafjord. Think you have solved question answering? try arc,\nthe ai2 reasoning challenge. arXiV , abs/1803.05457, 2018.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entail-\nment challenge. In Machine Learning Challenges Workshop , 2007.\nMarie-Catherine De Marne\ufb00e, Mandy Simons, and Judith Tonhauser. The commitment-\nbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn\nund Bedeutung , pages 107\u2013124, 2019.\nAlexandra DeLucia, Shijie Wu, Aaron Mueller, Carlos Aguirre, Philip Resnik, and Mark\nDredze. Bernice: A multilingual pre-trained encoder for Twitter. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natural Language Processing , pages 6191\u2013\n6205, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational\nLinguistics. URL https://aclanthology.org/2022.emnlp-main.415 .\nTim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-\nwise quantization. In International Conference on Learning Representations , 2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages\n4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguis-\ntics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423 .\nJesse Dodge, Maarten Sap, Ana Marasovi\u00b4 c, William Agnew, Gabriel Ilharco, Dirk Groen-\neveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A\ncase study on the colossal clean crawled corpus. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Processing , pages 1286\u20131305, Online\nand Punta Cana, Dominican Republic, November 2021. Association for Computational\nLinguistics. doi: 10.18653/v1/2021.emnlp-main.98. URL https://aclanthology.org/\n2021.emnlp-main.98 .\nMark Dredze, Prabhanjan Kambadur, Gary Kazantsev, Gideon Mann, and Miles Osborne.\nHow twitter is changing the nature of \ufb01nancial news discovery. In proceedings of the\nsecond international workshop on data science for macro-modeling , pages 1\u20135, 2016.\nIngrid E Fisher, Margaret R Garnsey, and Mark E Hughes. Natural language processing in\naccounting, auditing and \ufb01nance: A synthesis of the literature with a roadmap for future\nresearch. Intelligent Systems in Accounting, Finance and Management , 23(3):157\u2013214,\n2016.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\nJason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor\nLeahy. The pile: An 800gb dataset of diverse text for language modeling, 2021. URL\nhttps://arxiv.org/abs/2101.00027 .\n43", "start_char_idx": 0, "end_char_idx": 2964, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e9b55c4-d95f-47e9-ac5d-7185664aae28": {"__data__": {"id_": "5e9b55c4-d95f-47e9-ac5d-7185664aae28", "embedding": null, "metadata": {"page_label": "44", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9421f734-ac27-415c-ba4b-3f33a1c94d7d", "node_type": "4", "metadata": {"page_label": "44", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "98024c2c5a0237d13ee8069b1824e8f8f9682f25bf97ee3f6c3556a1dcb9ae06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30c1a09a-883d-4149-aa13-7ad1fec264cd", "node_type": "1", "metadata": {"page_label": "43", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "34974354506f03d0e22c8df748bc1ca4b4df17850e2fe7cb71a42c2257ceb55f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea08b061-dc73-494b-b7b8-1d27ed7eca59", "node_type": "1", "metadata": {}, "hash": "1ac871a46b1669206221bec785fbea8ef4ca1be6c44de4b67aadbc850223bfba", "class_name": "RelatedNodeInfo"}}, "text": "Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. Repairing the cracked foun-\ndation: A survey of obstacles in evaluation practices for generated text, 2022. URL\nhttps://arxiv.org/abs/2202.06935 .\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL\nrecognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop\non Textual Entailment and Paraphrasing , pages 1\u20139, Prague, June 2007. Association for\nComputational Linguistics. URL https://aclanthology.org/W07-1401 .\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds,\nMaribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-\nGillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail\nSee, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias,\nRichard Green, So\u02c7 na Mokr\u00b4 a, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah\nYoung, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu,\nLisa Anne Hendricks, and Geo\ufb00rey Irving. Improving alignment of dialogue agents via\ntargeted human judgements, 2022. URL https://arxiv.org/abs/2209.14375 .\nAndrew S. Gordon, Zornitsa Kozareva, and Melissa Roemmele. Semeval-2012 task 7: Choice\nof plausible alternatives: An evaluation of commonsense causal reasoning. In International\nWorkshop on Semantic Evaluation , 2011.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. News summarization and evaluation in the\nera of gpt-3, 2022. URL https://arxiv.org/abs/2209.12356 .\nSuchin Gururangan, Ana Marasovi\u00b4 c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug\nDowney, and Noah A. Smith. Don\u2019t stop pretraining: Adapt language models to domains\nand tasks. In Proceedings of the 58th Annual Meeting of the Association for Computa-\ntional Linguistics , pages 8342\u20138360, Online, July 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.acl-main.740. URL https://aclanthology.org/\n2020.acl-main.740 .\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and\nIdan Szpektor. The second pascal recognising textual entailment challenge. In Proceed-\nings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment ,\nvolume 7, 2006.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiV preprint\narXiV:1606.08415 , 2016.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. In International\nConference on Learning Representations , 2021. URL https://openreview.net/forum?\nid=d7KBjmI3GmQ .\nAlex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen.\nQuery-key normalization for transformers. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020 , pages 4246\u20134253, Online, November 2020. Associ-\n44", "start_char_idx": 0, "end_char_idx": 2877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea08b061-dc73-494b-b7b8-1d27ed7eca59": {"__data__": {"id_": "ea08b061-dc73-494b-b7b8-1d27ed7eca59", "embedding": null, "metadata": {"page_label": "45", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4aed0592-1b53-48c4-98d3-b5ee8f234150", "node_type": "4", "metadata": {"page_label": "45", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "498be9419469954352410606d1da42f1ae162d938a0585db6523989a10d8c5b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e9b55c4-d95f-47e9-ac5d-7185664aae28", "node_type": "1", "metadata": {"page_label": "44", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "98024c2c5a0237d13ee8069b1824e8f8f9682f25bf97ee3f6c3556a1dcb9ae06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3672d4a5-84e8-49f8-88c9-147083c07bf4", "node_type": "1", "metadata": {}, "hash": "465807b26317ed15e3530ebafd45ee433340ddc3a5bd3281d041862b25fa4adc", "class_name": "RelatedNodeInfo"}}, "text": "ation for Computational Linguistics. doi: 10.18653/v1/2020.\ufb01ndings-emnlp.379. URL\nhttps://aclanthology.org/2020.findings-emnlp.379 .\nDanny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for\ntransfer. arXiV preprint arXiV:2102.01293 , 2021.\nJordan Ho\ufb00mann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,\nEliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche,\nBogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol\nVinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of compute-optimal\nlarge language model training. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho, editors, Advances in Neural Information Processing Systems , 2022. URL\nhttps://openreview.net/forum?id=iBBcRUlOAPR .\nJeremy Howard and Sebastian Ruder. Universal language model \ufb01ne-tuning for text clas-\nsi\ufb01cation. In Proceedings of the 56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pages 328\u2013339, Melbourne, Australia,\nJuly 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1031. URL\nhttps://aclanthology.org/P18-1031 .\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes\nand predicting hospital readmission. arXiV , 4 2019. URL http://arxiv.org/abs/1904.\n05342 .\nFrederick Jelinek. Continuous speech recognition by statistical methods. Proceedings of the\nIEEE , 64(4):532\u2013556, 1976.\nYacine Jernite, Huu Nguyen, Stella Biderman, Anna Rogers, Maraim Masoud, Valentin\nDanchev, Samson Tan, Alexandra Sasha Luccioni, Nishant Subramani, Isaac Johnson,\nGerard Dupont, Jesse Dodge, Kyle Lo, Zeerak Talat, Dragomir Radev, Aaron Gokaslan,\nSomaieh Nikpoor, Peter Henderson, Rishi Bommasani, and Margaret Mitchell. Data\ngovernance in the age of large-scale data-driven language technology. In 2022 ACM\nConference on Fairness, Accountability, and Transparency . ACM, jun 2022. doi: 10.\n1145/3531146.3534637. URL https://doi.org/10.1145%2F3531146.3534637 .\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Je\ufb00rey Wu, and Dario Amodei. Scaling laws for neural\nlanguage models. arXiV , 1 2020. URL http://arxiv.org/abs/2001.08361 .\nCan Karakus, Rahul Huilgol, Fei Wu, Anirudh Subramanian, Cade Daniel, Derya Cavdar,\nTeng Xu, Haohan Chen, Arash Rahnama, and Luis Quintela. Amazon sagemaker model\nparallelism: A general and \ufb02exible framework for large model training. arXiV preprint\narXiV:2111.05972 , 2021.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.\nLooking beyond the surface: A challenge set for reading comprehension over multiple\nsentences. In Proceedings of the 2018 Conference of the North American Chapter of\n45", "start_char_idx": 0, "end_char_idx": 2884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3672d4a5-84e8-49f8-88c9-147083c07bf4": {"__data__": {"id_": "3672d4a5-84e8-49f8-88c9-147083c07bf4", "embedding": null, "metadata": {"page_label": "46", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ec42e5c-347d-44e6-bdcf-ee4075bfcf16", "node_type": "4", "metadata": {"page_label": "46", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bb55c4c156e396a371723589268158fc32ddcdaa5e4a05a3d97390c5b1877f36", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea08b061-dc73-494b-b7b8-1d27ed7eca59", "node_type": "1", "metadata": {"page_label": "45", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "498be9419469954352410606d1da42f1ae162d938a0585db6523989a10d8c5b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12ec2990-47b7-4d98-983e-47ca5b137a6c", "node_type": "1", "metadata": {}, "hash": "c8a75636a798872839ba1f720ff6b49207231b52726c37a9b5a5103c0845414e", "class_name": "RelatedNodeInfo"}}, "text": "the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 252\u2013262, New Orleans, Louisiana, June 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/N18-1023. URL https://aclanthology.\norg/N18-1023 .\nVijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mo-\nhammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large\ntransformer models, 2022. URL https://arxiv.org/abs/2205.05198 .\nTaku Kudo. Subword regularization: Improving neural network translation models with\nmultiple subword candidates. In Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1: Long Papers) , pages 66\u201375, Mel-\nbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/\nv1/P18-1007. URL https://aclanthology.org/P18-1007 .\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing: System Demonstra-\ntions , pages 66\u201371, Brussels, Belgium, November 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012 .\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale\nReAding comprehension dataset from examinations. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language Processing , pages 785\u2013794, Copen-\nhagen, Denmark, September 2017. Association for Computational Linguistics. doi:\n10.18653/v1/D17-1082. URL https://aclanthology.org/D17-1082 .\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Stas Bekman, M Saiful Bari, Stella Bider-\nman, Hady Elsahar, Niklas Muennigho\ufb00, Jason Phang, O\ufb01r Press, Colin Ra\ufb00el, Victor\nSanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, and\nIz Beltagy. What language model to train if you have one million GPU hours? In\nFindings of the Association for Computational Linguistics: EMNLP 2022 , pages 765\u2013\n782, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational\nLinguistics. URL https://aclanthology.org/2022.findings-emnlp.54 .\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. Biobert: A pre-trained biomedical language representation model for\nbiomedical text mining. Bioinformatics , 36:1234\u20131240, 2 2020. ISSN 14602059. doi:\n10.1093/bioinformatics/btz682.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\nCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language models\nbetter. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 8424\u20138445, Dublin, Ireland, May 2022a.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.577. URL\nhttps://aclanthology.org/2022.acl-long.577 .\n46", "start_char_idx": 0, "end_char_idx": 3004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12ec2990-47b7-4d98-983e-47ca5b137a6c": {"__data__": {"id_": "12ec2990-47b7-4d98-983e-47ca5b137a6c", "embedding": null, "metadata": {"page_label": "47", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d209b906-a365-42d4-adc8-0c3063379b38", "node_type": "4", "metadata": {"page_label": "47", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "86cfa3a7b0052198ddd9043e6ea08e9bf21579183f35a7fe2321ecd82497600d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3672d4a5-84e8-49f8-88c9-147083c07bf4", "node_type": "1", "metadata": {"page_label": "46", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "bb55c4c156e396a371723589268158fc32ddcdaa5e4a05a3d97390c5b1877f36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bdffaf9-f218-433d-b2b1-6008e437af15", "node_type": "1", "metadata": {}, "hash": "a325dc1e46a8b4f6041b7ca1b8ec4b4f8c8a2ad7871915d82715861ce71759ad", "class_name": "RelatedNodeInfo"}}, "text": "Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin\nParanjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, Rose E.\nWang, Minae Kwon, Joon Sung Park, Hancheng Cao, Tony Lee, Rishi Bommasani,\nMichael S. Bernstein, and Percy Liang. Evaluating human-language model interac-\ntion. CoRR , abs/2212.09746, 2022b. doi: 10.48550/arXiv.2212.09746. URL https:\n//doi.org/10.48550/arXiv.2212.09746 .\nEric Lehman, Evan Hernandez, Diwakar Mahajan, Jonas Wul\ufb00, Micah J. Smith, Zachary\nZiegler, Daniel Nadler, Peter Szolovits, Alistair Johnson, and Emily Alsentzer. Do we\nstill need clinical language models?, 2023. URL https://arxiv.org/abs/2302.08091 .\nHector J. Levesque, Ernest Davis, and L. Morgenstern. The winograd schema challenge.\nInInternational Conference on Principles of Knowledge Representation and Reasoning ,\n2011.\nYoav Levine, Noam Wies, Or Sharir, Ho\ufb01t Bata, and Amnon Shashua. Limits to depth\ne\ufb03ciencies of self-attention. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and\nH. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages\n22640\u201322651. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/\npaper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf .\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoyanov. Pretrained language models\nfor biomedical and clinical tasks: Understanding and extending the state-of-the-art. In\nProceedings of the 3rd Clinical Natural Language Processing Workshop , pages 146\u2013157,\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n2020.clinicalnlp-1.17. URL https://aclanthology.org/2020.clinicalnlp-1.17 .\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai\nWu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning\nproblems with language models, 2022. URL https://arxiv.org/abs/2206.14858 .\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Ya-\nsunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin New-\nman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher R\u00b4 e, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin\nDurmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav\nSanthanam, Laurel J. Orr, Lucia Zheng, Mert Y\u00a8 uksekg\u00a8 on\u00a8 ul, Mirac Suzgun, Nathan\nKim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang,\nRyan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto,\nThomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan\nMai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. CoRR ,\nabs/2211.09110, 2022. doi: 10.48550/arXiv.2211.09110. URL https://doi.org/10.\n48550/arXiv.2211.09110 .", "start_char_idx": 0, "end_char_idx": 2909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bdffaf9-f218-433d-b2b1-6008e437af15": {"__data__": {"id_": "9bdffaf9-f218-433d-b2b1-6008e437af15", "embedding": null, "metadata": {"page_label": "47", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d209b906-a365-42d4-adc8-0c3063379b38", "node_type": "4", "metadata": {"page_label": "47", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "86cfa3a7b0052198ddd9043e6ea08e9bf21579183f35a7fe2321ecd82497600d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12ec2990-47b7-4d98-983e-47ca5b137a6c", "node_type": "1", "metadata": {"page_label": "47", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c58cff9e08fd33db5c404b977e6f1d2cf1719d56f0999519fce0210997bc1792", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecc63690-ae4d-43b9-89da-2382380a859e", "node_type": "1", "metadata": {}, "hash": "d9969e143ed19c44549984193d62bbb182f5725758c2ba3338acff26f18bda5e", "class_name": "RelatedNodeInfo"}}, "text": "Holistic evaluation of language models. CoRR ,\nabs/2211.09110, 2022. doi: 10.48550/arXiv.2211.09110. URL https://doi.org/10.\n48550/arXiv.2211.09110 .\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and\nevaluation. White Paper. AI21 Labs , 1, 2021.\n47", "start_char_idx": 2760, "end_char_idx": 3046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecc63690-ae4d-43b9-89da-2382380a859e": {"__data__": {"id_": "ecc63690-ae4d-43b9-89da-2382380a859e", "embedding": null, "metadata": {"page_label": "48", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffd86702-ba69-4f62-8c92-80c61026da92", "node_type": "4", "metadata": {"page_label": "48", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c53ac6331d3a0c971de72dbac06dbdba891724262f1a6cc7f24e02a57eaa4c47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bdffaf9-f218-433d-b2b1-6008e437af15", "node_type": "1", "metadata": {"page_label": "47", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "072125746ce4fecb81cdea61d0872c749b62be8cadd334c435597d79588079cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "220a5241-6967-40bb-be6a-88f9f5bab38f", "node_type": "1", "metadata": {}, "hash": "c2ced7a6289c830115e0974fd780cf0fa2e909237386a8e1e89b98134d1955f6", "class_name": "RelatedNodeInfo"}}, "text": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos\nSantos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, and Alexander Rives.\nLanguage models of protein sequences at the scale of evolution enable accurate structure\nprediction. bioRxiv , 2022. doi: 10.1101/2022.07.20.500902. URL https://www.biorxiv.\norg/content/early/2022/07/21/2022.07.20.500902 .\nTianyu Liu, Yuchen Eleanor Jiang, Nicholas Monath, Ryan Cotterell, and Mrinmaya\nSachan. Autoregressive structured prediction with language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2022 , pages 993\u20131005, Abu Dhabi,\nUnited Arab Emirates, December 2022. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2022.findings-emnlp.70 .\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\nConference on Learning Representations , 2019. URL https://openreview.net/forum?\nid=Bkg6RiCqY7 .\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan\nLiu. BioGPT: generative pre-trained transformer for biomedical text generation and\nmining. Brie\ufb01ngs in Bioinformatics , 23(6), sep 2022. doi: 10.1093/bib/bbac409. URL\nhttps://doi.org/10.1093%2Fbib%2Fbbac409 .\nJouni Luoma and Sampo Pyysalo. Exploring cross-sentence contexts for named entity\nrecognition with BERT. In Proceedings of the 28th International Conference on Compu-\ntational Linguistics , pages 904\u2013914, Barcelona, Spain (Online), December 2020. Interna-\ntional Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.78.\nURL https://aclanthology.org/2020.coling-main.78 .\nMacedo Maia, Siegfried Handschuh, Andr\u00b4 e Freitas, Brian Davis, Ross McDermott, Manel\nZarrouk, and Alexandra Balahur. Www\u201918 open challenge: Financial opinion mining and\nquestion answering. In Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and\nPanagiotis G. Ipeirotis, editors, Companion of the The Web Conference 2018 on The\nWeb Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018 , pages 1941\u20131942.\nACM, 2018. doi: 10.1145/3184558.3192301. URL https://doi.org/10.1145/3184558.\n3192301 .\nPekka Malo, Ankur Sinha, Pekka J. Korhonen, Jyrki Wallenius, and Pyry Takala. Good\ndebt or bad debt: Detecting semantic orientations in economic texts. J. Assoc. Inf.\nSci. Technol. , 65(4):782\u2013796, 2014. doi: 10.1002/asi.23062. URL https://doi.org/10.\n1002/asi.23062 .\nSabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Ra\ufb00el, Manan Dey, Matthias\nGall\u00b4 e, Arun Raja, Chenglei Si, Wilson Y. Lee, Beno\u02c6 \u0131t Sagot, and Samson Tan. Between\nwords and characters: A brief history of open-vocabulary modeling and tokenization in\nnlp, 2021. URL https://arxiv.org/abs/2112.10508 .\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor\nconduct electricity? a new dataset for open book question answering. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing , pages\n48", "start_char_idx": 0, "end_char_idx": 2977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "220a5241-6967-40bb-be6a-88f9f5bab38f": {"__data__": {"id_": "220a5241-6967-40bb-be6a-88f9f5bab38f", "embedding": null, "metadata": {"page_label": "49", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a70665c0-b121-40ed-9595-c3076e4bb1e5", "node_type": "4", "metadata": {"page_label": "49", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "0ba2d6000a88423108ebef9818095ecf3e84cb19dc522a3c3868626dda233907", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecc63690-ae4d-43b9-89da-2382380a859e", "node_type": "1", "metadata": {"page_label": "48", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "c53ac6331d3a0c971de72dbac06dbdba891724262f1a6cc7f24e02a57eaa4c47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0862d41e-8859-4095-8b86-4fa56754799b", "node_type": "1", "metadata": {}, "hash": "28dfd19c6157ae09517783f59b61bc9c2b3ac02b36da896ddc87c6619f6fd2e0", "class_name": "RelatedNodeInfo"}}, "text": "2381\u20132391, Brussels, Belgium, October-November 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260 .\nTomas Mikolov, Martin Kara\ufb01\u00b4 at, Lukas Burget, Jan Cernock` y, and Sanjeev Khudanpur. Re-\ncurrent neural network based language model. In Interspeech , pages 1045\u20131048. Makuhari,\n2010.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy\nVanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for\ndeeper understanding of commonsense stories. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies , pages 839\u2013849, San Diego, California, June 2016. Association for\nComputational Linguistics. doi: 10.18653/v1/N16-1098. URL https://aclanthology.\norg/N16-1098 .\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. BERTweet: A pre-trained language\nmodel for English tweets. In Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations , pages 9\u201314, Online, October\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.2.\nURL https://aclanthology.org/2020.emnlp-demos.2 .\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\nAdversarial NLI: A new benchmark for natural language understanding. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics , pages 4885\u2013\n4901, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n2020.acl-main.441. URL https://aclanthology.org/2020.acl-main.441 .\nErik Nijkamp, Je\ufb00rey Ru\ufb00olo, Eli N. Weinstein, Nikhil Naik, and Ali Madani. Progen2:\nExploring the boundaries of protein language models. CoRR , abs/2206.13517, 2022. doi:\n10.48550/arXiv.2206.13517. URL https://doi.org/10.48550/arXiv.2206.13517 .\nNVIDIA. Train with mixed precision, 2023. URL https://docs.nvidia.com/\ndeeplearning/performance/mixed-precision-training/index.html .\nLong Ouyang, Je\ufb00rey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul\nChristiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions\nwith human feedback. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun\nCho, editors, Advances in Neural Information Processing Systems , 2022. URL https:\n//openreview.net/forum?id=TG8KACxEON .\nBaolin Peng, Michel Galley, Pengcheng He, Chris Brockett, Lars Liden, Elnaz Nouri, Zhou\nYu, Bill Dolan, and Jianfeng Gao. Godel: Large-scale pre-training for goal-directed\ndialog. arXiV preprint arXiV:2206.11309 , 2022.\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset\nfor evaluating context-sensitive meaning representations. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguistics:\n49", "start_char_idx": 0, "end_char_idx": 3074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0862d41e-8859-4095-8b86-4fa56754799b": {"__data__": {"id_": "0862d41e-8859-4095-8b86-4fa56754799b", "embedding": null, "metadata": {"page_label": "50", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e6d7ce8-174e-4c01-9b11-175b862efb5c", "node_type": "4", "metadata": {"page_label": "50", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "0d747c86c8a7f74b790c637b5a58bd9f10635c2269151d65a85c6b6aff1bf9be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "220a5241-6967-40bb-be6a-88f9f5bab38f", "node_type": "1", "metadata": {"page_label": "49", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "0ba2d6000a88423108ebef9818095ecf3e84cb19dc522a3c3868626dda233907", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "663ec988-571a-4d48-bcc8-77579b12a90d", "node_type": "1", "metadata": {}, "hash": "5a7b9e3cdf55d9893bb37896018f138983dd19f7192bf8bc45984f710e6ead63", "class_name": "RelatedNodeInfo"}}, "text": "Human Language Technologies, Volume 1 (Long and Short Papers) , pages 1267\u20131273,\nMinneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1128. URL https://aclanthology.org/N19-1128 .\nO\ufb01r Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear\nbiases enables input length extrapolation. In International Conference on Learning Rep-\nresentations , 2022. URL https://openreview.net/forum?id=R8sQPpGCv0 .\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training, 2018. URL https://gluebenchmark.com/\nleaderboard .\nAlec Radford, Je\ufb00rey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners, 2019. URL https://github.com/\ncodelucas/newspaper .\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lil-\nlicrap. Compressive transformers for long-range sequence modelling. In 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SylKikSYDH .\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Ho\ufb00mann, Francis\nSong, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Ruther-\nford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den\nDriessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Sa\ufb00ron Huang, Jonathan Uesato, John Mellor, Irina\nHiggins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar,\nElena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\nmatzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch,\nJean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Mas-\nson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake\nHechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osin-\ndero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Je\ufb00 Stanway, Lorrayne\nBennett, Demis Hassabis, Koray Kavukcuoglu, and Geo\ufb00rey Irving. Scaling language\nmodels: Methods, analysis & insights from training gopher. arXiV , 12 2021. URL\nhttp://arxiv.org/abs/2112.11446 .\nColin Ra\ufb00el, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning\nwith a uni\ufb01ed text-to-text transformer. Journal of Machine Learning Research , 21(140):\n1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html .\nSamyam Rajbhandari, Je\ufb00 Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory opti-\nmizations toward training trillion parameter models. In SC20: International Conference\n50", "start_char_idx": 0, "end_char_idx": 3096, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "663ec988-571a-4d48-bcc8-77579b12a90d": {"__data__": {"id_": "663ec988-571a-4d48-bcc8-77579b12a90d", "embedding": null, "metadata": {"page_label": "51", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd745a49-a5f7-4006-9784-b1a86f585eee", "node_type": "4", "metadata": {"page_label": "51", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "60ffc833d80fd4fa0b693a8c6beb7d533dd1b091b1e2784890e5eb4931f648ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0862d41e-8859-4095-8b86-4fa56754799b", "node_type": "1", "metadata": {"page_label": "50", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "0d747c86c8a7f74b790c637b5a58bd9f10635c2269151d65a85c6b6aff1bf9be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5865e9ff-6d2f-49a3-a974-cdf4abde8c13", "node_type": "1", "metadata": {}, "hash": "15e7704a364169fc8a4f55d4e88b8847683d81b176fb4d382a0571e95bfa6ff7", "class_name": "RelatedNodeInfo"}}, "text": "for High Performance Computing, Networking, Storage and Analysis , pages 1\u201316. IEEE,\n2020.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing\nXu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. Recipes for\nbuilding an open-domain chatbot. In Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume , pages 300\u2013325,\nOnline, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\neacl-main.24. URL https://aclanthology.org/2021.eacl-main.24 .\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WINO-\nGRANDE: An adversarial winograd schema challenge at scale. Commun. ACM , 64:\n99\u2013106, 2019.\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. Domain adaption\nof named entity recognition to support credit risk assessment. In Proceedings of the\nAustralasian Language Technology Association Workshop 2015 , pages 84\u201390, Parramatta,\nAustralia, December 2015. URL https://aclanthology.org/U15-1010 .\nVictor Sanh, Albert Webson, Colin Ra\ufb00el, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Cha\ufb03n, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu,\nUrmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chh-\nablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Baw-\nden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli,\nThibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo\nGao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-\nshot task generalization. In International Conference on Learning Representations , 2022.\nURL https://openreview.net/forum?id=9Vrb9D0WI4 .", "start_char_idx": 0, "end_char_idx": 1852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5865e9ff-6d2f-49a3-a974-cdf4abde8c13": {"__data__": {"id_": "5865e9ff-6d2f-49a3-a974-cdf4abde8c13", "embedding": null, "metadata": {"page_label": "51", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd745a49-a5f7-4006-9784-b1a86f585eee", "node_type": "4", "metadata": {"page_label": "51", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "60ffc833d80fd4fa0b693a8c6beb7d533dd1b091b1e2784890e5eb4931f648ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "663ec988-571a-4d48-bcc8-77579b12a90d", "node_type": "1", "metadata": {"page_label": "51", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "eda1b3e633e77858da2c7b39acde90487d29fb5d1e1de0d58a20ed4ea9462b38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70dea20e-ff43-40ca-b4a7-b98a33cd17af", "node_type": "1", "metadata": {}, "hash": "e8bc8de7f0ad08b23dce7006368b907ead35d42df829e246a58810c7aa21abc4", "class_name": "RelatedNodeInfo"}}, "text": "Multitask prompted training enables zero-\nshot task generalization. In International Conference on Learning Representations , 2022.\nURL https://openreview.net/forum?id=9Vrb9D0WI4 .\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4 c, Daniel Hesslow,\nRoman Castagn\u00b4 e, Alexandra Sasha Luccioni, Fran\u00b8 cois Yvon, Matthias Gall\u00b4 e, Jonathan\nTow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammana-\nmanchi, Thomas Wang, Beno\u02c6 \u0131t Sagot, Niklas Muennigho\ufb00, Albert Villanova del Moral,\nOlatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy,\nHuu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lau-\nren\u00b8 con, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Ra\ufb00el, Aaron Gokaslan,\nAdi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg\nNitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong,\nDaniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz\u00b4 alez Ponfer-\nrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G\u00b4 erard Dupont,\nGerm\u00b4 an Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian\nYu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny\nChim, Jesse Dodge, Jian Zhu, Jonathan Chang, J\u00a8 org Frohberg, Joseph Tobing, Joydeep\nBhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon We-\nber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u02dc noz,\nMaraim Masoud, Mar\u00b4 \u0131a Grandury, Mario \u02c7Sa\u02c7 sko, Max Huang, Maximin Coavoux, Mayank\n51", "start_char_idx": 1672, "end_char_idx": 3289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70dea20e-ff43-40ca-b4a7-b98a33cd17af": {"__data__": {"id_": "70dea20e-ff43-40ca-b4a7-b98a33cd17af", "embedding": null, "metadata": {"page_label": "52", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8de0aef-41dd-4b0b-b165-eb9ad9fdaac7", "node_type": "4", "metadata": {"page_label": "52", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "782d035d61d5e97c93b4bfda855559c1c0157c261ca8e4cb53e0567b283fab68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5865e9ff-6d2f-49a3-a974-cdf4abde8c13", "node_type": "1", "metadata": {"page_label": "51", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "8d00c5c64b151881e8634edb84a5ee77dab4a69d7c90a778f701e596cb2cca50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abb5da81-1366-4cac-84c0-71aa17add4bc", "node_type": "1", "metadata": {}, "hash": "af370946355ff4d230338d430a64c614470b524627ff3f077b872ce337c97889", "class_name": "RelatedNodeInfo"}}, "text": "Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb,\nNishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espe-\njel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok,\nQuentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L\u00b4 opez, Rui Ribeiro,\nSalomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan\nMuhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg,\nSuhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin\nDanchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid\nAlyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta\u00b8 sar,\nElizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli,\nAntoine Cha\ufb03n, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani,\nHan Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao,\nLintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak,\nRyan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon\nKim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru\nTang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam\nRoberts, Hyung Won Chung, Jaesung Tae, Jason Phang, O\ufb01r Press, Conglong Li, Deepak\nNarayanan, Hatim Bourfoune, Jared Casper, Je\ufb00 Rasley, Max Ryabinin, Mayank Mishra,\nMinjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi,\nOmar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Fran\u00b8 cois Lavall\u00b4 ee, R\u00b4 emi\nLacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00b4 ephane Requena, Suraj\nPatil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-\nLaure Ligozat, Arjun Subramonian, Aur\u00b4 elie N\u00b4 ev\u00b4 eol, Charles Lovering, Dan Garrette,\nDeepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bog-\ndanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova,\nJessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine\nCarpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg,\nOskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin,\nShani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena\nRieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov,\nZachary Bamberger, Zden\u02c7 ek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour,\nAmmar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash\nAghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini,", "start_char_idx": 0, "end_char_idx": 2728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abb5da81-1366-4cac-84c0-71aa17add4bc": {"__data__": {"id_": "abb5da81-1366-4cac-84c0-71aa17add4bc", "embedding": null, "metadata": {"page_label": "52", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8de0aef-41dd-4b0b-b165-eb9ad9fdaac7", "node_type": "4", "metadata": {"page_label": "52", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "782d035d61d5e97c93b4bfda855559c1c0157c261ca8e4cb53e0567b283fab68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70dea20e-ff43-40ca-b4a7-b98a33cd17af", "node_type": "1", "metadata": {"page_label": "52", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "fb79ce52ac2bad6999d83f85118708c338c83a7921bf8e8f303580f2e8df4b67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71cccb65-61d6-4c8b-ace1-45ada0e771fc", "node_type": "1", "metadata": {}, "hash": "2f383179c63a93d92227607cdbc03cba083be3e345c990997c5f9a6dda252ce7", "class_name": "RelatedNodeInfo"}}, "text": "Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg,\nOskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin,\nShani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena\nRieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov,\nZachary Bamberger, Zden\u02c7 ek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour,\nAmmar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash\nAghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi,\nBenjamin Ajibade, Bharat Saxena, Carlos Mu\u02dc noz Ferrandis, Danish Contractor, David\nLansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezin-\nwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, In-\ndrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore,\nJosh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot\nMieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed\nGhauri, Mykola Burynok, Na\ufb01s Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olan-\nrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shub-\nber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo\nYang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, An-\nima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio\nBrito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00b4 ementine Fourrier, Daniel Le\u00b4 on Peri\u02dc n\u00b4 an,\n52", "start_char_idx": 2190, "end_char_idx": 3754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71cccb65-61d6-4c8b-ace1-45ada0e771fc": {"__data__": {"id_": "71cccb65-61d6-4c8b-ace1-45ada0e771fc", "embedding": null, "metadata": {"page_label": "53", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4d5f94e-5740-424c-b113-569c7060a94c", "node_type": "4", "metadata": {"page_label": "53", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "41b49fe5bb1eb3dd6074b7b5de8c35d0d8b3c32183f190fe5b7c551907729b42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abb5da81-1366-4cac-84c0-71aa17add4bc", "node_type": "1", "metadata": {"page_label": "52", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "f4944847d3ded66f45b88bcf7353f6a6cafd1d113bf019f65f85a068ff0fd11f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b013476a-e3c6-40d8-bb2c-428c49e47399", "node_type": "1", "metadata": {}, "hash": "b4983f0f04d66afe2c0dc10e35cd98193dc80f7f3007aecc5fa0588acc2e1413", "class_name": "RelatedNodeInfo"}}, "text": "Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel\nAltay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash,\nJihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivara-\nman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc P` amies, Maria A Castillo, Marianna Nezhurina, Mario S\u00a8 anger, Matthias\nSamwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna\nLiu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Mi-\nchio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Re-\nnata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyaw-\nijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott,\nSinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud,\nTh\u00b4 eo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash\nVenkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde\nBras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multi-\nlingual language model. arXiV , 11 2022. URL http://arxiv.org/abs/2211.05100 .\nMike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE in-\nternational conference on acoustics, speech and signal processing (ICASSP) , pages 5149\u2013\n5152. IEEE, 2012.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of\nrare words with subword units. In Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) , pages 1715\u2013\n1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi:\n10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162 .\nRaj Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Na-\ntraj Raman, Charese Smiley, Jiaao Chen, and Diyi Yang. When FLUE meets FLANG:\nBenchmarks and large pretrained language model for \ufb01nancial domain. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natural Language Processing , pages 2322\u2013\n2335, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational\nLinguistics. URL https://aclanthology.org/2022.emnlp-main.148 .\nNoam Shazeer. GLU variants improve transformer. arXiV preprint arXiV:2002.05202 ,\n2020.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using\nmodel parallelism. arXiV preprint arXiV:1909.08053 , 2019.", "start_char_idx": 0, "end_char_idx": 2628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b013476a-e3c6-40d8-bb2c-428c49e47399": {"__data__": {"id_": "b013476a-e3c6-40d8-bb2c-428c49e47399", "embedding": null, "metadata": {"page_label": "53", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4d5f94e-5740-424c-b113-569c7060a94c", "node_type": "4", "metadata": {"page_label": "53", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "41b49fe5bb1eb3dd6074b7b5de8c35d0d8b3c32183f190fe5b7c551907729b42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71cccb65-61d6-4c8b-ace1-45ada0e771fc", "node_type": "1", "metadata": {"page_label": "53", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "6b08beda9ff34e46b12219566c8634aad4f6d2520e749b4d56941ab364fd1f4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8d3c995-142b-4bec-ac7d-f34bd9f6e721", "node_type": "1", "metadata": {}, "hash": "971008c1ea695fb841a2beb4c4a0db71a978838869001fd0c6f8ac2d396a62d1", "class_name": "RelatedNodeInfo"}}, "text": "When FLUE meets FLANG:\nBenchmarks and large pretrained language model for \ufb01nancial domain. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natural Language Processing , pages 2322\u2013\n2335, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational\nLinguistics. URL https://aclanthology.org/2022.emnlp-main.148 .\nNoam Shazeer. GLU variants improve transformer. arXiV preprint arXiV:2002.05202 ,\n2020.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using\nmodel parallelism. arXiV preprint arXiV:1909.08053 , 2019.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung,\nNathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Mar-\ntin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery,\nPhilip Mans\ufb01eld, Blaise Aguera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias,\nKatherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Bar-\nral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language\nmodels encode clinical knowledge, 2022. URL https://arxiv.org/abs/2212.13138 .\n53", "start_char_idx": 1971, "end_char_idx": 3214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8d3c995-142b-4bec-ac7d-f34bd9f6e721": {"__data__": {"id_": "b8d3c995-142b-4bec-ac7d-f34bd9f6e721", "embedding": null, "metadata": {"page_label": "54", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df4bc328-3a90-417d-9072-96ad5d79f56a", "node_type": "4", "metadata": {"page_label": "54", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "773303e937c11aae6bdbd5fddde9cc48a53dc2538e17ba6450eed63a96a5ecc1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b013476a-e3c6-40d8-bb2c-428c49e47399", "node_type": "1", "metadata": {"page_label": "53", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "15fb412c4aa2dafcf61efa27439d1902f35a9fc380a33944ed34664b2befd46a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7dbe9c08-6254-4374-89e8-7bd57b98dab2", "node_type": "1", "metadata": {}, "hash": "c8b9e0d8c75c8e21b91dc59b193d435f7e93fff92d7b153e4b9f3c163cd9baf8", "class_name": "RelatedNodeInfo"}}, "text": "Ankur Sinha and Tanmay Khandait. Impact of news on the commodity market: Dataset\nand results. CoRR , abs/2009.04202, 2020. URL https://arxiv.org/abs/2009.04202 .\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari,\nJared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton\nZhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad\nShoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Us-\ning deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative\nlanguage model, 2022. URL https://arxiv.org/abs/2201.11990 .\nSaleh Soltan, Shankar Ananthakrishnan, Jack G. M. FitzGerald, Rahul Gupta, Wael\nHamza, Haidar Khan, Charith S. Peris, Stephen Rawls, Andrew Rosenbaum, Anna\nRumshisky, Chandan Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma,\nGokhan Tur, and Premkumar Natarajan. Alexatm 20b: Few-shot learning using a large-\nscale multilingual seq2seq model. arXiV , abs/2208.01448, 2022.", "start_char_idx": 0, "end_char_idx": 1018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7dbe9c08-6254-4374-89e8-7bd57b98dab2": {"__data__": {"id_": "7dbe9c08-6254-4374-89e8-7bd57b98dab2", "embedding": null, "metadata": {"page_label": "54", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df4bc328-3a90-417d-9072-96ad5d79f56a", "node_type": "4", "metadata": {"page_label": "54", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "773303e937c11aae6bdbd5fddde9cc48a53dc2538e17ba6450eed63a96a5ecc1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8d3c995-142b-4bec-ac7d-f34bd9f6e721", "node_type": "1", "metadata": {"page_label": "54", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "340303d3032510d98c76703f939504f259685cde7628a444fe009bde72ca99b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58c643a8-66b5-480d-b4e2-23d32784ec7e", "node_type": "1", "metadata": {}, "hash": "ffe82c7fee855af606fde25c3854e84e1b9d7a937f423ef941beaeabd5b346d5", "class_name": "RelatedNodeInfo"}}, "text": "Us-\ning deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative\nlanguage model, 2022. URL https://arxiv.org/abs/2201.11990 .\nSaleh Soltan, Shankar Ananthakrishnan, Jack G. M. FitzGerald, Rahul Gupta, Wael\nHamza, Haidar Khan, Charith S. Peris, Stephen Rawls, Andrew Rosenbaum, Anna\nRumshisky, Chandan Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma,\nGokhan Tur, and Premkumar Natarajan. Alexatm 20b: Few-shot learning using a large-\nscale multilingual seq2seq model. arXiV , abs/2208.01448, 2022.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar\nAbid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri` a Garriga-\nAlonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex\nRay, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Ali-\ncia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet An-\nnasaheb Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas\nStuhlmuller, Andrew M. Dai, Andrew D. La, Andrew Kyle Lampinen, Andy Zou, Angela\nJiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli,\nAnu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubara-\njan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Er-\ndem, Ayla Karakacs, Bridget R. Roberts, Bao Sheng Loe, Barret Zoph, Bartlomiej Bo-\njanowski, Batuhan Ozyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden,\nBenno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Stephen Howald, Cameron Diao,\nCameron Dour, Catherine Stinson, Cedrick Argueta, C\u2019esar Ferri Ram\u2019irez, Chandan\nSingh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch,\nChris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ta-\ntiana Ramirez, Clara Rivera, Clemencia Siro, Colin Ra\ufb00el, Courtney Ashcraft, Cristina\nGarbacea, Damien Sileo, Daniel H Garrette, Dan Hendrycks, Dan Kilman, Dan Roth,\nDaniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Gonz\u2019alez, Danny Hernandez,\nDanqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, D. Drakard, David Jurgens,\nDebajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen,\nDerek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi\nYang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hager-\nman, Elizabeth Barnes, Elizabeth P. Donoway, Ellie Pavlick, Emanuele Rodol` a, Emma FC\nLam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer,\nEthan J. Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fan Xia,\nFatemeh Siar, Fernando Mart\u2019inez-Plumed, Francesca Happ\u2019e, Fran\u00b8 cois Chollet, Frieda\nRong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germ\u00b4 an Kruszewski, Gi-\nambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-L\u2019opez,\nGregor Betz, Guy Gur-Ari, Hana Galijasevic, Han Sol Kim, Hannah Rashkin, Hanna Ha-\n54", "start_char_idx": 487, "end_char_idx": 3474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58c643a8-66b5-480d-b4e2-23d32784ec7e": {"__data__": {"id_": "58c643a8-66b5-480d-b4e2-23d32784ec7e", "embedding": null, "metadata": {"page_label": "55", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c88fa152-4bcc-489f-9ac5-741b47ad117c", "node_type": "4", "metadata": {"page_label": "55", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "9d0399806d5a3331ed7b05e0113e5527b606a30f141dff45aa7c7b493c56feed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7dbe9c08-6254-4374-89e8-7bd57b98dab2", "node_type": "1", "metadata": {"page_label": "54", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d648eba4c73d0e3b84a7c7b1752536030d9eb8fa9ac1220e7e33cabdd59c0452", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39a8bfa6-51e5-4271-9026-9893a14e8b98", "node_type": "1", "metadata": {}, "hash": "2a9fcc2ec4873c6da00d65952b7e937d9217c8455633f34407b29d4822fc966b", "class_name": "RelatedNodeInfo"}}, "text": "jishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Sch\u00a8 utze, Hiromu Yakura,\nHongming Zhang, Hubert Wong, Ian Aik-Soon Ng, Isaac Noble, Jaap Jumelet, Jack\nGeissinger, John Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fern\u00b4 andez Fisac, J. Brooker\nSimon, James Koppel, James Zheng, James Zou, Jan Koco\u2019n, Jana Thompson, Jared\nKaplan, Jarema Radom, Jascha Narain Sohl-Dickstein, Jason Phang, Jason Wei, Ja-\nson Yosinski, Jekaterina Novikova, Jelle Bosscher, Jenni Marsh, Jeremy Kim, Jeroen\nTaal, Jesse Engel, Jesujoba Oluwadara Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang,\nJane W Waweru, John Burden, John Miller, John U. Balis, Jonathan Berant, Jorg Fro-\nhberg, Jos Rozen, Jos\u00b4 e Hern\u00b4 andez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B.\nTenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth,\nKarthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin\nGimpel, Kevin Ochieng\u2019 Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia\nShkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency,\nLuca Moschella, Luca Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros\nCol\u2019on, Luke Metz, Lut\ufb01 Kerem cSenel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Madotto Andrea, Maheen Saleem Farooqi, Manaal Faruqui, Mantas Mazeika,\nMarco Baturan, Marco Marelli, Marco Maru, M Quintana, Marie Tolkiehn, Mario Giu-\nlianelli, Martha Lewis, Martin Potthast, Matthew Leavitt, Matthias Hagen, M\u2019aty\u2019as\nSchubert, Medina Baitemirova, Melissa Arnaud, Melvin Andrew McElrath, Michael A.\nYee, Michael Cohen, Mi Gu, Michael I. Ivanitskiy, Michael Starritt, Michael Strube,\nMichal Swkedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain,\nMimee Xu, Mirac Suzgun, Monica Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva,\nMozhdeh Gheini, T MukundVarma, Nanyun Peng, Nathan Chi, Nayeon Lee, Neta Gur-\nAri Krakover, Nicholas Cameron, Nicholas S. Roberts, Nicholas Doiron, Nikita Nan-\ngia, Niklas Deckers, Niklas Muennigho\ufb00, Nitish Shirish Keskar, Niveditha Iyer, Noah\nConstant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi,\nOmer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung,\nPaul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Pe-\nter W. Chang, Peter Eckersley, Phu Mon Htut, Pi-Bei Hwang, P. Milkowski, Piyush S.\nPatil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, QING LYU, Qinlang Chen, Rabin\nBanjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ram\u2019on Risco Delgado,\nRapha\u00a8 el Milli` ere, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe\nRaymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le\nBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov,", "start_char_idx": 0, "end_char_idx": 2849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39a8bfa6-51e5-4271-9026-9893a14e8b98": {"__data__": {"id_": "39a8bfa6-51e5-4271-9026-9893a14e8b98", "embedding": null, "metadata": {"page_label": "55", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c88fa152-4bcc-489f-9ac5-741b47ad117c", "node_type": "4", "metadata": {"page_label": "55", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "9d0399806d5a3331ed7b05e0113e5527b606a30f141dff45aa7c7b493c56feed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58c643a8-66b5-480d-b4e2-23d32784ec7e", "node_type": "1", "metadata": {"page_label": "55", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "19fab4823846d4c5a04fda42e7728ad5472a20a630a553bf0bc1378049e8e207", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6aa86388-192e-4157-8f47-ceb12aaeaca8", "node_type": "1", "metadata": {}, "hash": "d92de4dece07565424519754f59abe8218bedcc8bf14d57d8569670ccfbfdfd3", "class_name": "RelatedNodeInfo"}}, "text": "Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Pe-\nter W. Chang, Peter Eckersley, Phu Mon Htut, Pi-Bei Hwang, P. Milkowski, Piyush S.\nPatil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, QING LYU, Qinlang Chen, Rabin\nBanjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ram\u2019on Risco Delgado,\nRapha\u00a8 el Milli` ere, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe\nRaymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le\nBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan\nLee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib J. Singh, Saif M. Mohammad, Sa-\njant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Sam Bowman,\nSamuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian,\nSayan Ghosh, Sean Casey, Sebastian Bischo\ufb00, Sebastian Gehrmann, Sebastian Schus-\nter, Sepideh Sadeghi, Shadi S. Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi,\nShikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshni-\nwal, Shyam Upadhyay, Shyamolima Debnath, Siamak Shakeri, Simon Thormeyer, Simone\nMelzi, Siva Reddy, Sneha Priscilla Makini, Soo hwan Lee, Spencer Bradley Torene, Sri-\nharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Rose Biderman,\nStephanie C. Lin, S. Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Mish-\n55", "start_char_idx": 2318, "end_char_idx": 3698, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6aa86388-192e-4157-8f47-ceb12aaeaca8": {"__data__": {"id_": "6aa86388-192e-4157-8f47-ceb12aaeaca8", "embedding": null, "metadata": {"page_label": "56", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13b2d855-039c-4589-991f-bb81c9ff9347", "node_type": "4", "metadata": {"page_label": "56", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "370b73321008228ee68232459aef648705994a692064371fd58bf79bacc3df73", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39a8bfa6-51e5-4271-9026-9893a14e8b98", "node_type": "1", "metadata": {"page_label": "55", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "046e3a3c71dcc23fff2d54e348114d2f78ebdae95202867a559b1a91e52e6230", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52a388a0-0eb6-45ea-8044-c1c9d4ec602d", "node_type": "1", "metadata": {}, "hash": "1ca91df80cc49e4c12e6fa311d58247da70ad5d82f9ddf995e1a20de0c32c348", "class_name": "RelatedNodeInfo"}}, "text": "erghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu,\nTariq A. Ali, Tatsuo Hashimoto, Te-Lin Wu, Theo Desbordes, Theodore Rothschild,\nThomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, T. N. Kornev, Timothy\nTelleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj,\nTushar Khot, Tyler O\u2019Brien Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria\nNyamai, Vikas Raunak, Vinay Venkatesh Ramasesh, Vinay Uday Prabhu, Vishakh Pad-\nmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, W Vossen,\nXiang Ren, Xiaoyu Tong, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz,\nYang Song, Yasaman Bahri, Ye Ji Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan\nBelinkov, Yu Hou, Yu Hou, Yuntao Bai, Zachary Seid, Zhao Xinran, Zhuoye Zhao, Zi Fu\nWang, Zijie J. Wang, Zirui Wang, Ziyi Wu, Sahib Singh, and Uri Shaham. Beyond\nthe imitation game: Quantifying and extrapolating the capabilities of language models.\narXiv , abs/2206.04615, 2022.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu.\nRoformer: Enhanced transformer with rotary position embedding. arXiV preprint\narXiV:2104.09864 , 2021a.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. CoRR , abs/2104.09864, 2021b. URL\nhttps://arxiv.org/abs/2104.09864 .\nIlya Sutskever, James Martens, and Geo\ufb00rey E Hinton. Generating text with recurrent\nneural networks. In Proceedings of the 28th international conference on machine learning\n(ICML-11) , pages 1017\u20131024, 2011.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00a8 arli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei.\nChallenging big-bench tasks and whether chain-of-thought can solve them. CoRR ,\nabs/2210.09261, 2022. doi: 10.48550/arXiv.2210.09261. URL https://doi.org/10.\n48550/arXiv.2210.09261 .\nOyvind Tafjord and Peter Clark. General-purpose question-answering with macaw. arXiV\npreprint arXiV:2109.02593 , 2021.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA:\nA question answering challenge targeting commonsense knowledge. In Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages\n4149\u20134158, Minneapolis, Minnesota, June 2019. Association for Computational Linguis-\ntics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421 .\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won\nChung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale\ne\ufb03ciently: Insights from pre-training and \ufb01ne-tuning transformers. arXiV preprint\narXiV:2109.10686 , 2021.\n56", "start_char_idx": 0, "end_char_idx": 2826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52a388a0-0eb6-45ea-8044-c1c9d4ec602d": {"__data__": {"id_": "52a388a0-0eb6-45ea-8044-c1c9d4ec602d", "embedding": null, "metadata": {"page_label": "57", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "efd79ef3-5949-46b5-96b3-d0ad75fff690", "node_type": "4", "metadata": {"page_label": "57", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "62a4222265eb2e4633989affb648d3d46b62fab5a16a4000e9e26525a14363c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6aa86388-192e-4157-8f47-ceb12aaeaca8", "node_type": "1", "metadata": {"page_label": "56", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "370b73321008228ee68232459aef648705994a692064371fd58bf79bacc3df73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28769e7d-85f3-4e9f-a6b5-3e0a22053343", "node_type": "1", "metadata": {}, "hash": "ef9c0f5c58bffcd336d8c6b1fc96912a7d0dff6e42baadde16e7b41a3eb54608", "class_name": "RelatedNodeInfo"}}, "text": "Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng\nRao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws\nvs model architectures: How does inductive bias in\ufb02uence scaling? arXiV preprint\narXiV:2207.10551 , 2022a.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang,\nHyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nDenny Zhou, Neil Houlsby, and Donald Metzler. Ul2: Unifying language learning\nparadigms, 2022b. URL https://arxiv.org/abs/2205.05131 .\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis\nSaravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language\nmodel for science. arXiV , 11 2022. URL http://arxiv.org/abs/2211.09085 .\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha,\nHeng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae\nLee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim\nKrikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam\nRoberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Kri-\nvokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-\nHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke,\nJohnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchin-\nson, Kristen Olson, Alejandra Molina, Erin Ho\ufb00man-John, Josh Lee, Lora Aroyo, Ravi\nRajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron\nCohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak,\nEd Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022. URL\nhttps://arxiv.org/abs/2201.08239 .\nErik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared\ntask: Language-independent named entity recognition. In Proceedings of the Seventh\nConference on Natural Language Learning at HLT-NAACL 2003 , pages 142\u2013147, 2003.\nURL https://aclanthology.org/W03-0419 .\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00b4 ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and\ne\ufb03cient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971 .\nTina Tseng, Amanda Stent, and Domenic Maida. Best practices for managing data anno-\ntation projects, 2020. URL http://rgdoi.net/10.13140/RG.2.2.34497.58727 .\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,\nU. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Processing Systems , volume 30.", "start_char_idx": 0, "end_char_idx": 2923, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28769e7d-85f3-4e9f-a6b5-3e0a22053343": {"__data__": {"id_": "28769e7d-85f3-4e9f-a6b5-3e0a22053343", "embedding": null, "metadata": {"page_label": "57", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "efd79ef3-5949-46b5-96b3-d0ad75fff690", "node_type": "4", "metadata": {"page_label": "57", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "62a4222265eb2e4633989affb648d3d46b62fab5a16a4000e9e26525a14363c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52a388a0-0eb6-45ea-8044-c1c9d4ec602d", "node_type": "1", "metadata": {"page_label": "57", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "f80ad0552ef462dde0497044df7ebe035cdaf3160d0070a1ea60777ae1243bb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "456b8062-8b29-4194-8544-f00a12fab2db", "node_type": "1", "metadata": {}, "hash": "50da915277e74224bab628607ae0d3a7449e4177fba88e387d5bb6a2517795ea", "class_name": "RelatedNodeInfo"}}, "text": "URL https://arxiv.org/abs/2302.13971 .\nTina Tseng, Amanda Stent, and Domenic Maida. Best practices for managing data anno-\ntation projects, 2020. URL http://rgdoi.net/10.13140/RG.2.2.34497.58727 .\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,\nU. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran\nAssociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\n57", "start_char_idx": 2403, "end_char_idx": 3054, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "456b8062-8b29-4194-8544-f00a12fab2db": {"__data__": {"id_": "456b8062-8b29-4194-8544-f00a12fab2db", "embedding": null, "metadata": {"page_label": "58", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "81560a8f-4e99-4e34-a475-dfb6369c27d1", "node_type": "4", "metadata": {"page_label": "58", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "ab8fdfeb24a61d44f0ce87de4991a797f18d090ee3798a777a239642dfd3d109", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28769e7d-85f3-4e9f-a6b5-3e0a22053343", "node_type": "1", "metadata": {"page_label": "57", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "4d320ae141d0fe06ecfd922be58bf4a88d06531c6b641452a538b14ac9245af3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "caa1196b-e3bb-4666-ae04-9f27049045c3", "node_type": "1", "metadata": {}, "hash": "e93ec7b3caca4bc04a34b30ee75855d9f518f6fb68c0443748e239eafc9ded7a", "class_name": "RelatedNodeInfo"}}, "text": "Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/mesh-transformer-jax , May 2021.\nYue Wang, Weishi Wang, Sha\ufb01q Joty, and Steven C.H. Hoi. CodeT5: Identi\ufb01er-aware\nuni\ufb01ed pre-trained encoder-decoder models for code understanding and generation. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Process-\ning, pages 8696\u20138708, Online and Punta Cana, Dominican Republic, November 2021. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.685. URL\nhttps://aclanthology.org/2021.emnlp-main.685 .\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester,\nNan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot\nlearners, 2021. URL https://arxiv.org/abs/2109.01652 .\nJason Wei, Yi Tay, Rishi Bommasani, Colin Ra\ufb00el, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori\nHashimoto, Oriol Vinyals, Percy Liang, Je\ufb00 Dean, and William Fedus. Emergent abilities\nof large language models. Transactions on Machine Learning Research (TMLR) , 2022a.\ndoi: 10.48550/ARXIV.2206.07682. URL https://arxiv.org/abs/2206.07682 .\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nChi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large\nlanguage models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun\nCho, editors, Advances in Neural Information Processing Systems , 2022b. URL https:\n//openreview.net/forum?id=_VjQlMeSB_J .\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Gri\ufb03n, Jonathan Uesato, Po-Sen\nHuang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha\nBrown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura\nRimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geo\ufb00rey Irving, and Iason\nGabriel. Ethical and social risks of harm from language models, 2021. URL https:\n//arxiv.org/abs/2112.04359 .\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Gri\ufb03n, Po-Sen Huang, John\nF. J. Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney\nBiles, Sande Minnich Brown, Zachary Kenton, William T. Hawkins, Tom Stepleton,\nAbeba Birhane, Lisa Anne Hendricks, Laura Rimell, William S. Isaac, Julia Haas, Sean\nLegassick, Geo\ufb00rey Irving, and Iason Gabriel. Taxonomy of risks posed by language\nmodels. 2022 ACM Conference on Fairness, Accountability, and Transparency , 2022.\nJohannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor,\nLisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang.\nChallenges in detoxifying language models. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2021 , pages 2447\u20132469, Punta Cana, Dominican Repub-\nlic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\n\ufb01ndings-emnlp.210. URL https://aclanthology.org/2021.findings-emnlp.210 .\n58", "start_char_idx": 0, "end_char_idx": 3045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "caa1196b-e3bb-4666-ae04-9f27049045c3": {"__data__": {"id_": "caa1196b-e3bb-4666-ae04-9f27049045c3", "embedding": null, "metadata": {"page_label": "59", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "252b8374-5bea-45e9-a0d6-e4946391d66a", "node_type": "4", "metadata": {"page_label": "59", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "842fb0b3680c67f1a52440d27c8b1828267c9c9441a1ea313c8bae433e77b25a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "456b8062-8b29-4194-8544-f00a12fab2db", "node_type": "1", "metadata": {"page_label": "58", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "ab8fdfeb24a61d44f0ce87de4991a797f18d090ee3798a777a239642dfd3d109", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf75c5af-383f-4bcf-a6ab-651996356030", "node_type": "1", "metadata": {}, "hash": "3b27b74a3e076c1294aec7a9f05b5dbaa50d8fb596b90c3d2bb03eaf04551335", "class_name": "RelatedNodeInfo"}}, "text": "Shijie Wu and Mark Dredze. Beto, bentz, becas: The surprising cross-lingual e\ufb00ectiveness of\nBERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP) , pages 833\u2013844, Hong Kong, China, November 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/D19-1077. URL https://aclanthology.\norg/D19-1077 .\nYonghui Wu, Mike Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Je\ufb00 Klingner, Apurva\nShah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang,\nCli\ufb00 Young, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gregory S. Cor-\nrado, Macdu\ufb00 Hughes, and Je\ufb00rey Dean. Google\u2019s neural machine translation system:\nBridging the gap between human and machine translation. ArXiV , abs/1609.08144, 2016.\nYijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, and Jie Tang. Modeling protein\nusing large-scale pretrain language model. CoRR , abs/2108.07435, 2021. URL https:\n//arxiv.org/abs/2108.07435 .\nFrank Z Xing, Erik Cambria, and Roy E Welsch. Natural language based \ufb01nancial forecast-\ning: a survey. Arti\ufb01cial Intelligence Review , 50(1):49\u201373, 2018.\nAlbert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan Klein.\nDetoxifying language models risks marginalizing minority voices. In Proceedings of the\n2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages 2390\u20132397, Online, June 2021. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.190. URL\nhttps://aclanthology.org/2021.naacl-main.190 .\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can\na machine really \ufb01nish your sentence? In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages 4791\u20134800, Florence, Italy, July 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https:\n//aclanthology.org/P19-1472 .\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,\nYifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai,\nWenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. Glm-130b: An open bilingual\npre-trained model. arXiV , 10 2022. URL http://arxiv.org/abs/2210.02414 .\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van\nDurme. Record: Bridging the gap between human and machine commonsense reading\ncomprehension. arXiV , abs/1810.12885, 2018.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott,\nSam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models.\narXiV , 5 2022a. URL http://arxiv.org/abs/2205.01068 .\n59", "start_char_idx": 0, "end_char_idx": 3114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf75c5af-383f-4bcf-a6ab-651996356030": {"__data__": {"id_": "bf75c5af-383f-4bcf-a6ab-651996356030", "embedding": null, "metadata": {"page_label": "60", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17fd7791-7164-43fa-b030-bf9103575a6b", "node_type": "4", "metadata": {"page_label": "60", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "dc1ced4fc62d56492ff15eab53b0b60b3cdf7fff3705f0e5a2feab40500dca9b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "caa1196b-e3bb-4666-ae04-9f27049045c3", "node_type": "1", "metadata": {"page_label": "59", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "842fb0b3680c67f1a52440d27c8b1828267c9c9441a1ea313c8bae433e77b25a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d87d0f5-c036-4393-80e6-b9e50679fe79", "node_type": "1", "metadata": {}, "hash": "79a261e6105140f097923f27bc1891b6a3046ba62fc8cf29b670ddfbcbda9214", "class_name": "RelatedNodeInfo"}}, "text": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng\nGao, Jingjing Liu, and Bill Dolan. DIALOGPT : Large-scale generative pre-training\nfor conversational response generation. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics: System Demonstrations , pages 270\u2013278,\nOnline, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nacl-demos.30. URL https://aclanthology.org/2020.acl-demos.30 .\nZhen Zhang, Shuai Zheng, Yida Wang, Justin Chiu, George Karypis, Trishul Chilimbi,\nMu Li, and Xin Jin. Mics: Near-linear scaling for training gigantic model on public\ncloud, 2022b. URL https://arxiv.org/abs/2205.00119 .\n60", "start_char_idx": 0, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d87d0f5-c036-4393-80e6-b9e50679fe79": {"__data__": {"id_": "9d87d0f5-c036-4393-80e6-b9e50679fe79", "embedding": null, "metadata": {"page_label": "61", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2117b9b4-41eb-4f4e-82f4-ce9c8d1dfaa3", "node_type": "4", "metadata": {"page_label": "61", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "368ca53496aa26c651862bf8cf2d09c877831b550c123e035d80c86f04b5150b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf75c5af-383f-4bcf-a6ab-651996356030", "node_type": "1", "metadata": {"page_label": "60", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "dc1ced4fc62d56492ff15eab53b0b60b3cdf7fff3705f0e5a2feab40500dca9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9350bd68-ed09-403d-9c83-ebb2ba7b4a61", "node_type": "1", "metadata": {}, "hash": "b7388013b858afddcb72bd590d7368a01d83dc1df666a11a275fa242cd1a5bb9", "class_name": "RelatedNodeInfo"}}, "text": "Appendix A. Architecture\nA.0 Notation\nStyling. Unstyled variables denote scalars, bold lower-case variables represent [column]\nvectors, and bold capitalized variables represent matrices. For instance, hi,jcould be an\nelement in the vector hj, which could in turn be the j-th column of matrix H.\nNamed functions are typed in non-italicized regular typeface, such as softmax( \u00b7) and\nFFN(\u00b7).\nRed color is used to denote trainable parameters, or functions that are parametrized by\ntrainable parameters, such as Wor FFN(\u00b7).\nSequences. A sequence ( x1,...,xn) ofnelements is denoted by {xi}n\ni=1. We treat a\nsequence of (column) vectors as a matrix, i.e. X={xi}n\ni=1\u2208Rm\u00d7n, where each xi\u2208Rm.\nOperators.\n\u2022f:Rn\u2192Rn: A function on vectors, that is, y=f(x) wherex,y\u2208Rnaren-\ndimensional real valued vectors. Whenever such a function is applied to a matrix, it\nis applied column-wise: f(X) ={f(xj)}m\nj=1,X\u2208Rn\u00d7m.\n\u2022A\u2299B: Element-wise (or Hadamard) product of matrices or vectors AandB(of the\nsame shape).\n\u2022 1(P): Indicator function that returns 1 if the predicate Pis true and 0 otherwise.\n\u2022[n]: For integer n, the set of all positive integers up to (including) n, i.e.{1,...,n}.\n\u2022A+b: Adding a vector to a matrix is de\ufb01ned as repeated addition to each column.\nThat is,A+b={ai+b}n\ni=1.\n\u2022Softmax: softmax( x) =exp(x)\u2211n\niexp(xi)where exp(\u00b7) is applied element-wise to a vector.\n\u2022Dropout: dropp(x) =1\n1\u2212p\u00b7m\u2299xwhere,m= [mi]n\ni=1\u22a4, andmi\u223cBernoulli(1\u2212p).\nRandom variables miare drawn independently for each presentation of an example.\nA.1 Full Architecture\nEmbedding. Let (x1,...,xt) ={xt}T\nt=1\u2208VTdenote an input sequence of length T,\nwhere each element xtdenotes an integer identi\ufb01er of a token from the vocabulary V= [|V|].\nInitial input representations H0={h0\nt}T\nt=1are obtained by\n\u00afh0\nt=Wemext \u2200t (1)\nh0\nt= LNem(\u00afh0\nt) \u2200t (2)\nwhereWem\u2208RD\u00d7|V|is the token embedding matrix, ext\u2208R|V|is thext-th standard basis\nvector, and LNemis the embedding LayerNorm function, to be de\ufb01ned in the following\nsubsections.\nObserve that no positional embedding is applied here due to how ALiBi works.\n61", "start_char_idx": 0, "end_char_idx": 2063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9350bd68-ed09-403d-9c83-ebb2ba7b4a61": {"__data__": {"id_": "9350bd68-ed09-403d-9c83-ebb2ba7b4a61", "embedding": null, "metadata": {"page_label": "62", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b4382b29-0500-44fd-8ed4-2deaabb2e05b", "node_type": "4", "metadata": {"page_label": "62", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "12370c5f58e0389492cd327efeb43ac234aa5c05252b070944fd851abb0a8465", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d87d0f5-c036-4393-80e6-b9e50679fe79", "node_type": "1", "metadata": {"page_label": "61", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "368ca53496aa26c651862bf8cf2d09c877831b550c123e035d80c86f04b5150b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50980f3f-048e-441c-a5f8-4f7e8b8defd3", "node_type": "1", "metadata": {}, "hash": "2c9c916662ff895cbb4022590d1a27553e438fa90b028acece8069a0da8d8f17", "class_name": "RelatedNodeInfo"}}, "text": "Layers. Layer representations H\u2113\u2208RD\u00d7Tfor each layer \u2113= 1,...,L can be sequentially\nde\ufb01ned as follows (this computation is sometimes referred to as a \u201cblock\u201d):\n\u00afH\u2113=H\u2113\u22121+ SA\u2113(LNin\n\u2113(H\u2113\u22121)) \u2200\u2113 (3)\nH\u2113=\u00afH\u2113+ FFN\u2113(LNat\n\u2113(\u00afH\u2113)) \u2200\u2113 (4)\nwhere SA \u2113, FFN\u2113, and LN\u2022\n\u2113denote SelfAttention, FeedForwardNetwork, and LayerNorm\nfunctions at layer \u2113, respectively, as de\ufb01ned in the following subsections. The red color\nindicates that the functions depend on trainable parameters. LN\u2022\n\u2113is further parametrized\nby an indication of what the function is applied to, such as LNin\n\u2113when applied to the block\ninput and LNat\n\u2113when applied to the attention output. We designate these separately since\nthey use di\ufb00erent (i.e. untied) trainable parameters.\nLogits. Given the \ufb01nal layer representation HL, logitsY\u2208R|V|\u00d7Tare obtained as:\nY=Wem\u22a4LNf(HL) (5)\nwhereWem\u2208RD\u00d7|V|is the same embedding matrix we used in the embedding part and\nLNfis the final LayerNorm application. We follow the PaLM approach in omitting a bias\nterm.\nThe token distribution for position j+ 1, conditioned on the pre\ufb01x ( x1,...,xj), is given\nby\nP(xj+1=w|{xt}j\nt=1) = softmax(yj)w (6)\nwhereyjis thej\u2019th column of Y.\nA.2 SelfAttention with ALiBi ( SA)\nSelfAttention with ALiBi at layer \u2113, SA\u2113:RD\u00d7T\u2192RD\u00d7Tis de\ufb01ned as follows.\nLetn\u2208{1,...,N}denote an attention head where Nis the total number of heads. Let\nDndenote the dimensionality of each head. Let An,M\u2208RT\u00d7Tdenote the ALiBi matrix\nand the attention mask, respectively, which will be de\ufb01ned later.\nThen,Y= SA\u2113(X) such that:\nQn=Wn,q\n\u2113X+bn,q\n\u2113\u2200n (7)\nKn=Wn,k\n\u2113X+bn,k\n\u2113\u2200n (8)\nVn=Wn,v\n\u2113X+bn,v\n\u2113\u2200n (9)\n\u00afSn=An+Kn\u22a4Qn\n\u221a\nDn\u2208RT\u00d7T\u2200n (10)\nSn= droppat(softmax( \u00afSn\u2299M))\u2208RT\u00d7T\u2200n (11)\n\u00afYn=VnSn\u2208RDn\u00d7T\u2200n (12)\nY= dropph((N\u2211\nn=1Un\n\u2113\u00afYn) +c\u2113)\u2208RD\u00d7T(13)\n62", "start_char_idx": 0, "end_char_idx": 1722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50980f3f-048e-441c-a5f8-4f7e8b8defd3": {"__data__": {"id_": "50980f3f-048e-441c-a5f8-4f7e8b8defd3", "embedding": null, "metadata": {"page_label": "63", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f75c8b03-4488-4383-a657-d7f10c309d6e", "node_type": "4", "metadata": {"page_label": "63", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e8c963ea15bd663ae06904b5dd6da166470abc46fe312143396f319a42750cc2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9350bd68-ed09-403d-9c83-ebb2ba7b4a61", "node_type": "1", "metadata": {"page_label": "62", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "12370c5f58e0389492cd327efeb43ac234aa5c05252b070944fd851abb0a8465", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "891bbe3d-5f0a-4124-8182-209d3f664c7b", "node_type": "1", "metadata": {}, "hash": "81f90f862a9acd5c2ea35d94221ecfc27ef1558f49bcddef02679496fdf7e05f", "class_name": "RelatedNodeInfo"}}, "text": "whereWn,q\n\u2113,Wn,k\n\u2113,Wn,v\n\u2113\u2208RDn\u00d7D,Un\n\u2113\u2208RD\u00d7Dn,\u2200nare the trainable weight parameters,\nbn,q\n\u2113,bn,k\n\u2113,bn,v\n\u2113\u2208RDn,\u2200n,c\u2113\u2208RD, are the trainable bias parameters, and pat,ph\u2208[0,1)\nare the attention and hidden unit dropout probabilities.\nThe ALiBi matrix An= [an\ni,j]i,j\u2208RT\u00d7Tis constructed as:\n\u02dcN= 2\u230alog2(N)\u230b(14)\n\u02dcn= 1 + ((n\u22121) mod \u02dcN)\u22120.5\u230an\u22121\n\u02dcN\u230b\n(15)\nan\ni,j= 2\u22128\nN\u02dcn\u00b7(i\u2212j)\u00b7 1(i<j ) \u2200i,j\u2208[T],n\u2208[N] (16)\nand the attention mask M= [mn\ni,j]i,j\u2208RT\u00d7Tis constructed as:\nmi,j= 1(i\u2264j)\u2212\u221e\u00b7 1(i>j ) \u2200i,j\u2208[T] (17)\nwhere we follow the convention that \u221e\u00b70 = 0.\nA.3 LayerNorm ( LN)\nLayerNorm, LN\u03b8:RD\u2192RD, is de\ufb01ned as follows:\ny= LN\u03b8(x) =x\u2212\u00b5(x)\u221a\n\u03c32(x) +\u03f5\u2299\u03b3\u03b8+\u03b2\u03b8(18)\nwhere\n\u00b5(x) =1\nD\u2211\nixi \u2208R (19)\n\u03c32(x) =1\nD\u2211\ni(xi\u2212\u00b5(x))2\u2208R (20)\nand,\u03b3\u03b8,\u03b2\u03b8\u2208RDare the trainable gain and bias parameters, and \u03f5\u2208Ris a small constant.\n\u03b8is used as the parametrization variable to emphasize LNem, LNf, and LNin\n\u2113, LNat\n\u2113,\u2200\u2113\nhave di\ufb00erent (untied) \u03b3and\u03b2parameters.\nA.4 FeedForwardNetwork ( FFN)\nFeedforward network component FFN \u2113:RD\u2192RDis de\ufb01ned as a simple multilayer per-\nceptron.y= FFN\u2113(x) such that:\nh= gelu(Wf\n\u2113x+bf\n\u2113) \u2208RD\u2032(21)\ny= droppf(Uf\n\u2113h+cf\n\u2113) \u2208RD(22)\nwhere gelu( x) = 0.5\u00b7x\u00b7(1+tanh(0.79788456\u00b7x\u00b7(1+0.044715\u00b7x2))) is applied element-wise,\nWf\n\u2113\u2208RD\u2032\u00d7D,Uf\n\u2113\u2208RD\u00d7D\u2032are the trainable weight parameters, bf\n\u2113\u2208RD\u2032,cf\n\u2113\u2208RD\nare the trainable bias parameters, and pf\u2208[0,1) denotes the dropout probability at this\ncomponent.\n63", "start_char_idx": 0, "end_char_idx": 1370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "891bbe3d-5f0a-4124-8182-209d3f664c7b": {"__data__": {"id_": "891bbe3d-5f0a-4124-8182-209d3f664c7b", "embedding": null, "metadata": {"page_label": "64", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab4702da-d279-4896-8925-a65b7e313065", "node_type": "4", "metadata": {"page_label": "64", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "748daa9cd5882293238877f990ffe62a6865fa4e807105dc4c1d0977c41b794b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50980f3f-048e-441c-a5f8-4f7e8b8defd3", "node_type": "1", "metadata": {"page_label": "63", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "e8c963ea15bd663ae06904b5dd6da166470abc46fe312143396f319a42750cc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d06bee6a-5e6f-49c1-88be-e7ed10778b1b", "node_type": "1", "metadata": {}, "hash": "4a9797630ce165d32b970938686a7733f681ffe528ff9844021a327244df9380", "class_name": "RelatedNodeInfo"}}, "text": "A.5 List of All Trainable Parameters\nList of shape hyperparameters and their values are as follows:\n\u2022L= 70 (number of layers)\n\u2022N= 40 (number of heads)\n\u2022|V|= 131072 (vocabulary size)\n\u2022D= 7,680 (hidden dimension)\n\u2022Dn= 192,\u2200n\u2208[N] (hidden dimension of each head)\n\u2022D\u2032= 4D= 30,720 (hidden dimension of FFN)\nInitialization hyperparameters are as follows:\n\u2022z= 0.006588\u22481/\u221a\n3Dis the default range (standard deviation).\n\u2022z\u2032=z\u00b7(1/\u221a\n2L) is the rescaled range for the second layer in FFN and the \ufb01nal linear\nmap in SA.\nList of all parameters with their sizes and (element-wise) initialization:\nRange Group Param Shape Size Total size Init\nWemD\u00d7|V| 1,006,632,960 1,006,632,960\u223cN(0,z)\nLNem\u03b3emD 7,680 7,680 = 1\n\u03b2emD 7,680 7,680 = 0\n\u2113\u2208[70] LNin\n\u2113\u03b3in\n\u2113D 7,680 537,600 = 1\n\u03b2in\n\u2113D 7,680 537,600 = 0\n\u2113\u2208[70], SA\u2113Wn,q\n\u2113Dn\u00d7D 1,474,560 4,128,768,000\u223cN(0,z)\nn\u2208[40] Wn,k\n\u2113Dn\u00d7D 1,474,560 4,128,768,000\u223cN(0,z)\nWn,v\n\u2113Dn\u00d7D 1,474,560 4,128,768,000\u223cN(0,z)\nUn\n\u2113D\u00d7Dn1,474,560 4,128,768,000\u223cN(0,z\u2032)\nbn,q\n\u2113Dn192 537,600 = 0\nbn,k\n\u2113Dn192 537,600 = 0\nbn,v\n\u2113Dn192 537,600 = 0\n\u2113\u2208[70] SA\u2113c\u2113D 7,680 537,600 = 0\n\u2113\u2208[70] \u03b3at\n\u2113D 7,680 537,600 = 1\n\u03b2at\n\u2113D 7,680 537,600 = 0\n\u2113\u2208[70] FFN\u2113Wf\n\u2113D\u2032\u00d7D 235,929,600 16,515,072,000 \u223cN(0,z)\nUf\n\u2113D\u00d7D\u2032235,929,600 16,515,072,000\u223cN(0,z\u2032)\nbf\n\u2113D\u203230,720 2,150,400 = 0\ncf\n\u2113D 7,680 537,600 = 0\nLNf\u03b3fD 7,680 7,680 = 1\n\u03b2fD 7,680 7,680 = 0\n50,558,868,480\n64", "start_char_idx": 0, "end_char_idx": 1333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d06bee6a-5e6f-49c1-88be-e7ed10778b1b": {"__data__": {"id_": "d06bee6a-5e6f-49c1-88be-e7ed10778b1b", "embedding": null, "metadata": {"page_label": "65", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "01af99df-40e7-40b0-b4a0-794650092dc1", "node_type": "4", "metadata": {"page_label": "65", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "03206a160e6a429c1d04442978869eedea7514ebc6ecae8366c5ec6dbe62c3b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "891bbe3d-5f0a-4124-8182-209d3f664c7b", "node_type": "1", "metadata": {"page_label": "64", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "748daa9cd5882293238877f990ffe62a6865fa4e807105dc4c1d0977c41b794b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40d9babf-5240-4874-99e5-c2295fef478e", "node_type": "1", "metadata": {}, "hash": "b7933326811baee23ff55c74acffbbd7d4837a8ee1236fc27eaf846e69969ad7", "class_name": "RelatedNodeInfo"}}, "text": "Tag Question\nprice or not Does the news headline talk about price(?)\nprice up Does the news headline talk about price going up(?)\nprice stable Does the news headline talk about price staying constant(?)\nprice down Does the news headline talk about price going down(?)\npast price Does the news headline talk about price in the past(?)\nfuture price Does the news headline talk about price in the future(?)\npast general Does the news headline talk about a general event (apart from prices) in the past(?)\nfuture general Does the news headline talk about a general event (apart from prices) in the future(?)\nasset comparison Does the news headline compare gold with any other asset(?)\nTable 18: O\ufb03cial documentation of each tag (Sinha and Khandait, 2020).\nAppendix B. Details on external \ufb01nancial tasks\nFPB (Malo et al., 2014): The Financial Phrasebank Dataset includes a sentiment classi-\n\ufb01cation task on /tildelow5,000 sentences in the English language taken from \ufb01nancial news about\ncompanies listed on OMX Helsinki. Sentiment annotations of positive, negative, neutral\nare adjudicated from the perspective of an investor: any news that could bene\ufb01t/hurt an\ninvestor is considered positive/negative and neutral otherwise. Each sentence is annotated\nby 5 to 8 annotators who have su\ufb03cient knowledge of \ufb01nance, whereas the source sentences\nwere written by \ufb01nancial journalists. For example, news about shrinking revenue would be\nlabeled negative and company growth as positive. While there are di\ufb00erent con\ufb01gurations\nof this dataset with each con\ufb01guration denoting the percentage agreement between anno-\ntators (\u226550%,\u226566%,\u226575%, 100%), we choose to use the con\ufb01guration with \u226550%. Since\nan o\ufb03cial train-test split is not available, we create our own random split. Our training\nsplit contains 3,876 sentences with 1,086 positive, 488 negative, and 2,302 neutral sentences\nand our test set contains 970 sentences with 277 positive, 116 negative, and 577 neutral\nsentences. We choose 5 shots and report F1 score weighted by support.\nFiQA SA (Maia et al., 2018): The second sentiment analysis task is to predict the aspect-\nspeci\ufb01c sentiment in English \ufb01nancial news and microblog headlines, which were published\nas a part of the 2018 challenge on \ufb01nancial question answering and opinion mining. In the\noriginal task, sentiment is annotated on a continuous scale of [ \u22121,+1]; the details on the\nannotation task are not readily available. To make this regression dataset amenable for\nfew-shot LLM setup, we convert it into a classi\ufb01cation task: Negative ( \u22121\u2264x <\u22120.1),\nneutral (\u22120.1\u2264x<+0.1), and positive (+0 .1\u2264x\u2264+1), where xis the original sentiment\nscore. We selected this discretization based on a manual examination of the dataset. Like\nwith FPB, we create our own random split combining both microblogs and news. After\ndiscretization, our training set contains 938 sentences with 576 positive, 287 negative, and\n75 neutral sentences and our test set contains 235 sentences with 141 positive, 76 negative,\nand 18 neutral sentences. We select 5 shots and report weighted F1.\nHeadline (Sinha and Khandait, 2020): This is a binary classi\ufb01cation task of whether a\nnews headline in the gold commodity domain includes certain information. This human-\nannotated dataset consists of 11,412 English news headlines from 2000 to 2019 about \u201cgold\u201d\nscraped from providers such as Reuters, The Hindu, The Economic Times, Bloomberg, and\n65", "start_char_idx": 0, "end_char_idx": 3420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40d9babf-5240-4874-99e5-c2295fef478e": {"__data__": {"id_": "40d9babf-5240-4874-99e5-c2295fef478e", "embedding": null, "metadata": {"page_label": "66", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ce5b866-dad9-4d22-bdf8-14e0101cc40f", "node_type": "4", "metadata": {"page_label": "66", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "b5ac7c568bc9b42d870f671aef2fc7edd33edc295be509c1d81341dae7fa6a43", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d06bee6a-5e6f-49c1-88be-e7ed10778b1b", "node_type": "1", "metadata": {"page_label": "65", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "03206a160e6a429c1d04442978869eedea7514ebc6ecae8366c5ec6dbe62c3b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fb1d4ef-3343-4248-aec6-f7bf88df2779", "node_type": "1", "metadata": {}, "hash": "19c76c992f2b83d4fc2f17162d33f579eb3acf3e024d9d7844549166332ff2ef", "class_name": "RelatedNodeInfo"}}, "text": "from aggregator sites such as Kitco, and MetalsDaily. Each news article carries a subset of\nthe following tags: \u201cprice or not\u201d, \u201cprice up\u201d, \u201cprice down\u201d, \u201cprice stable\u201d, \u201cpast price\u201d,\n\u201cfuture price\u201d, \u201cpast general\u201d, \u201cfuture general\u201d, \u201casset comparison\u201d. The dataset is created\nusing annotator consensus and Cohen\u2019s Kappa for each of the categories is \u22650.85 indicating\na high-quality dataset. Like with FPB, we create our own random split. Our training set\ncontains 9,129 sentences with 7,780, 3,785, 3,392, 414, 7,482, 299, 1,285, 67, 1,696 examples\nof \u201cprice or not\u201d, \u201cprice up\u201d, \u201cprice down\u201d, \u201cprice stable\u201d, \u201cpast price\u201d, \u201cfuture price\u201d,\n\u201cpast general\u201d, \u201cfuture general\u201d, \u201casset comparison\u201d classes, respectively. Similarly, the\ntest set contains 2283 sentences with 1,955, 962, 838, 109, 1,873, 82, 313, 15, 454 examples\nof the same classes. We verbalize each tag into a question using the o\ufb03cial documentation\non each tag as shown in Table 18. We used 5 shots, and report the average weighted F1\nscore across all categories.\nNER (Salinas Alvarado et al., 2015): This is a named entity recognition task on \ufb01nancial\ndata gathered for credit risk assessment. The dataset consists of 8 documents with /tildelow55,000\nwords of \ufb01nancial agreements \ufb01led with the SEC. The annotated entity types follow the\nstandard CoNLL format (Tjong Kim Sang and De Meulder, 2003) and are annotated with\nPER, LOC, ORG, and MISC. We use Fin-5 as the training data for context sampling and\ntest on the Fin-3 split. As MISC cannot be de\ufb01ned on its own but \u201cnames (that) are\nnot already in the other categories\u201d (Tjong Kim Sang and De Meulder, 2003), we drop all\nentities with type MISC. Additionally, as it is nontrivial to learn to predict empty output in\nthe few-shot set-up, we drop sentences that do not contain any entity. After preprocessing,\nour training set contains 504 sentences with 168 PER, 745 LOC, and 241 ORG, and our\ntest set consists of 98 sentences with 39 PER, 216 LOC, and 56 ORG. We found that all\nthe models required more shots to perform well. Hence, we selected 20 shots and report the\nentity-level F1 score.\nConvFinQA (Chen et al., 2022): Given an input that includes text and at least one table\nwith \ufb01nancial data, the task is to answer conversational questions that require numerical\nreasoning over the input. The source data is earning reports of S&P 500 companies and\nconsists of 3,892 conversations consisting 14,115 questions. This task requires numerical\nreasoning, an understanding of structured data and \ufb01nancial concepts, and a model needs\nto relate follow-up questions to the dialog turns. To solve this task, we use \u201c1 shot\u201d where\nan entire gold conversation and its context is input to the models. In addition, as each\n\u201cturn\u201d of the conversation concludes, the \u201cturn\u201d along with the \u201cgold\u201d answer for that\nturn is appended as context for future turns. Tables are linearized in the context (as\nsuggested by the authors) as Markdown tables, and we replace an empty entry with \u201d-\u201d.\nThe reported score is the exact match accuracy of the direct answer produced by a model.\nAs test set labels are not publicly available, we report results on the dev set instead. Our\ntraining set contains 11,104 conversations and 45,888 questions and our test set contains\n1,490 conversations and 5,932 questions.\n66", "start_char_idx": 0, "end_char_idx": 3312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fb1d4ef-3343-4248-aec6-f7bf88df2779": {"__data__": {"id_": "0fb1d4ef-3343-4248-aec6-f7bf88df2779", "embedding": null, "metadata": {"page_label": "67", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "110bc2e0-e46b-4815-916c-8a83238ce208", "node_type": "4", "metadata": {"page_label": "67", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "9f2da30118b6b6cebef049d96c19c7d1a229414fb6a496b040ab4b8882687b5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40d9babf-5240-4874-99e5-c2295fef478e", "node_type": "1", "metadata": {"page_label": "66", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "b5ac7c568bc9b42d870f671aef2fc7edd33edc295be509c1d81341dae7fa6a43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67ad00fd-6021-413e-90ab-08f08567b531", "node_type": "1", "metadata": {}, "hash": "d79feab0059ac62a87e53e0e8ca5d85453f71dbdf7fec45b2ef537f32eb421f1", "class_name": "RelatedNodeInfo"}}, "text": "Appendix C. Training Chronicles\nC.0 Still\nOur \ufb01rst training run was called v0. In this run, we experimented with curriculum learning.\nData that the model would see in the future would likely be similar to the newer data in\nour training corpus, so we wanted the model to do better on those future documents.\nAdditionally, since there are facts that change over time, newer information should ideally\noverride the old. Therefore, we temporally ordered the training data by month in FinPile .\nFigure 7 shows the learning curve for run v0. We observed a large gap between training\nand validation losses, which was expected: early stages of training would observe the oldest\ndata (starting from 2007) whereas our validation set was strictly from the future (i.e., 2022).\nHowever, one week into training we found the model stuck on both training and validation\nloss, as seen by the very limited validation progress between steps 15k-20k and almost no\nprogress after step 20k. There was the possibility that the training loss and the divergence\nof training and validation loss would both resolve themselves as the training data became\nmore and more similar to the validation data as the curriculum progressed. However, we\ndeemed this to be too risky to catch any other potential problems with the training that\nmight require early intervention, since it would mean training for many steps without any\ndiagnostic signal. We thus decided to abandon curriculum learning altogether.\n0 5000 10000 15000 20000 25000 30000\nSteps2.002.252.502.753.003.253.503.754.00LossLearning curve\nconfig\nv0\nmetric\nsmooth train loss\nval loss\nFigure 7: Learning curve of our \ufb01rst training attempt named v0. Observe the large gap\nbetween training and validation losses, as well as the \ufb02atness of both curves after\nstep 20k. The \ufb01nal 6k steps lasted about /tildelow2.3 days.\n67", "start_char_idx": 0, "end_char_idx": 1845, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67ad00fd-6021-413e-90ab-08f08567b531": {"__data__": {"id_": "67ad00fd-6021-413e-90ab-08f08567b531", "embedding": null, "metadata": {"page_label": "68", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9be1181-982d-4805-806c-2bbac3fd3325", "node_type": "4", "metadata": {"page_label": "68", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "4eeb7c30b21165938d8524bc5a6f0de7289dda9142531c823434fc6186a66c66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fb1d4ef-3343-4248-aec6-f7bf88df2779", "node_type": "1", "metadata": {"page_label": "67", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "9f2da30118b6b6cebef049d96c19c7d1a229414fb6a496b040ab4b8882687b5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d9d0885-581d-487a-8c22-b79f04d94586", "node_type": "1", "metadata": {}, "hash": "66d1ced0c7ea6c1972caf02707fdaf9aeea8f0d77d806a9395fb0032142afc43", "class_name": "RelatedNodeInfo"}}, "text": "0.001.002.003.004.005.00Grad normconfig\nv1.0\nv1.1\nv1.2\nv1.3\nv1.4\nmetric\nsmooth grad norm\n10000 12000 14000 16000 18000 20000 22000\nSteps2.302.402.502.602.70Lossconfig\nv1.0\nv1.1\nv1.2\nv1.3\nv1.4\nmetric\nsmooth train loss\nval lossFigure 8: Gradient norms (top) and train & validation loss (bottom) of v1.x runs.\nWe removed curriculum learning by shu\ufb04ing all of our training data uniformly on the\nshard level.3We then started a new run (v1.0), which led to much faster improvements\nin the validation loss. We were unable to ascertain if curriculum learning had a negative\nimpact on training or if the loss plateaued due to other factors, for example, the other\ndiscovered issue in v1.x.\nC.1 Elbow\nDuring our new run without curriculum learning (v1.0), we observed that the gradient norm\nshowed a steady increase after about 12k steps ( /tildelow4.5 days of training), with occasional spikes\n(see Figure 8). This was accompanied by sudden jumps in the validation loss, possibly\nindicating that the model might be becoming sensitive to small changes in its weights.\nTraining loss seemed to have been plateauing again, as well.\nWe believed that the gradient norm increases were the cause of the validation loss\nproblems (notice the alignment between sudden validation loss jumps with some of the\n3. Instead of loading one shard of data at a time, we load multiple random shards (without replacement)\nat the same time and shu\ufb04e them on the \ufb02y.\n68", "start_char_idx": 0, "end_char_idx": 1436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d9d0885-581d-487a-8c22-b79f04d94586": {"__data__": {"id_": "7d9d0885-581d-487a-8c22-b79f04d94586", "embedding": null, "metadata": {"page_label": "69", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5297f41d-0971-4b2b-81df-1e27a1f387d7", "node_type": "4", "metadata": {"page_label": "69", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "afa27c220cbb74e0a2d727ef57054174fad4cb59bbc3a12510ae3b5f05a0a95e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67ad00fd-6021-413e-90ab-08f08567b531", "node_type": "1", "metadata": {"page_label": "68", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "4eeb7c30b21165938d8524bc5a6f0de7289dda9142531c823434fc6186a66c66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fff73933-4ce0-460c-bd35-2227653fc61e", "node_type": "1", "metadata": {}, "hash": "24d9e63430403849a3c395798810342f9906ceda4e6476d1c961603b5cec09b0", "class_name": "RelatedNodeInfo"}}, "text": "0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nSteps0.00.20.40.60.81.01.2ValueFigure 9: Rescaled norms for each component in v1.0 run. Input LayerNorm at Layer 1\nstood out.\nsudden gradient norm jumps for v1.0, in Figure 8). We made several attempts across\nseveral model runs to \ufb01x the gradient norm increases:\nRun Changes from v1.0 run\nShared Change- Fully shu\ufb04e any data not seen by the model checkpoint that we chose\nto start (or restart) from instead of shard-level shu\ufb04ing\nv1.1- Start from step 10.8k of v1.0, prior to any gradient increase\n- Reduce max learning rate from 1e-4 to 8e-5\nv1.2- Continue from step 20.1k of v1.1 (most recent checkpoint)\n- Reduce max learning rate from 1e-4 to 6e-5\n- Reduce gradient clip from 1.0 to 0.3\nv1.3- Start from step 10.8k of v1.0, prior to any gradient increase\n- Use FP32 precision for LM-head computation (prior to softmax)\nv1.4- Start from step 12.0k of v1.3\n- Reduce max learning rate from 1e-4 to 6e-5\n- Reduce gradient clip from 1.0 to 0.3\n- Use FP32 precision for LM-head computation (prior to softmax)\nAll of these attempted \ufb01xes were made after we observed a trend of increasing gradient\nnorms similar to the original run (v1.0), or some early signs of a similar path that we\nhypothesized would eventually grow more. Since we didn\u2019t want to waste training time, we\n69", "start_char_idx": 0, "end_char_idx": 1325, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fff73933-4ce0-460c-bd35-2227653fc61e": {"__data__": {"id_": "fff73933-4ce0-460c-bd35-2227653fc61e", "embedding": null, "metadata": {"page_label": "70", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ca061ec-19f8-4d1f-ae48-67aadc971609", "node_type": "4", "metadata": {"page_label": "70", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "5715f400fce8607259844f9dbce84df57f4a4a5e95057ea226c50c00e426a8fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d9d0885-581d-487a-8c22-b79f04d94586", "node_type": "1", "metadata": {"page_label": "69", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "afa27c220cbb74e0a2d727ef57054174fad4cb59bbc3a12510ae3b5f05a0a95e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b73d2e4b-5fb6-4498-b288-bb2a2dea8204", "node_type": "1", "metadata": {}, "hash": "dfd70f8584940bd8f422a7ab210f86da37bc5c0a9460fc70f27aec3df47036a3", "class_name": "RelatedNodeInfo"}}, "text": "0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nSteps0.80.91.01.11.2ValueFigure 10: Values for Input LayerNorm at Layer 1 in v1.0 run.\ndid our best to make decisions early instead of allowing the model to continue down a bad\ntraining path.\nWe investigated the norms of the weights themselves to see if any peculiar trends were\naligning with the gradient growth. In particular, we were curious to see if there were\nparticular layers or components that were responsible for the large gradient norms.\nFigure 9 plots L2 norms for each component, averaged by the square root of the number\nof elements (layer norm multipliers start from 1 and all the others start close to zero due\nto initialization). We observed that all components follow a similar benign trend except\none: Input LayerNorm at layer 1 (i.e. LNin\n1), which suddenly elbows and starts increasing\nroughly linearly after step /tildelow12k. This also aligns with the initial growth of the gradient\nnorms.\nTo take a closer look, we inspected individual values of the multiplier weights \u03b3in\n1(there\nare 60 such values in a single model shard out of 128) in Figure 10. We observed all values\ncontributing to the same trend of shrinking until steps 11-12k and then shifting to move\nupward instead.\nDuring this investigation, we discovered another bug: Weight decay was applied to all\nthe non-bias parameters, as opposed to skipping the LayerNorm multiplier weight \u03b3since\nthey are initialized at 1. To the best of our knowledge, this practice has been inherited from\nthe BERT implementation4.\n4.https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/\noptimization.py#L59-L65\n70", "start_char_idx": 0, "end_char_idx": 1664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b73d2e4b-5fb6-4498-b288-bb2a2dea8204": {"__data__": {"id_": "b73d2e4b-5fb6-4498-b288-bb2a2dea8204", "embedding": null, "metadata": {"page_label": "71", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c80a239f-38a6-408d-ab3d-6f220a4389b1", "node_type": "4", "metadata": {"page_label": "71", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "cab44818be2db456f791293ba059aff2f8212fff5dfe2c71f3652448d86c93aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fff73933-4ce0-460c-bd35-2227653fc61e", "node_type": "1", "metadata": {"page_label": "70", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "5715f400fce8607259844f9dbce84df57f4a4a5e95057ea226c50c00e426a8fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29f2f10f-cafa-424b-988c-1aa1c30759aa", "node_type": "1", "metadata": {}, "hash": "2727a129f833575ab46c3de1a3348d842e368a18a8ddd8f73bc48ab8bd41d3d5", "class_name": "RelatedNodeInfo"}}, "text": "2.002.252.502.753.003.253.503.754.00LossLearning curve\nconfig\nv2\nmetric\nsmooth train loss\nval loss\n0.00.20.40.60.81.0Grad normsmooth grad norm\n0 20000 40000 60000 80000 100000 120000\nstep0.81.01.2Weight normEmbeddingLayerNorm\nL1 InputLayerNorm 2.102.152.202.252.30 config\nv2\nmetric\nsmooth train loss\nval lossFigure 11: Loss, gradient norm and weight norms for listed components for the v2.0 run.\nThis makes the elbow artifact shown in Figure 10 even more confusing: An additional\npush of weights towards 0 would trivially explain a downward trend but not a sudden shift\nto growth.\nAfter four failed attempts to \ufb01x the run, we considered the possibility of this run being\nunsalvageable and contemplated starting from scratch to apply a more conservative hyper-\nparameter setting from the beginning. These would include things that we have tried in\nour attempts such as shrinking the learning rate or gradient clipping. Additionally, because\nthe pathological trend change is isolated to LNin\n1, which is topologically very close to the\nremoved LayerNorm at the embedding layer (LNem) we decided to add back LNemas an\nadditional precaution, despite most other LLMs not having this component.\nC.2 Slide\nAfter numerous attempts to \ufb01x the elbow issue, we wanted to be as conservative as possible\nfor the hyperparameter choices when starting from scratch to keep the learning dynamics\nas stable as possible. We started the next run (v2.0) with the following hyperparameters\nand changes:\n\u2022Use FP32 precision in LM-head (softmax in Equation (6))\n71", "start_char_idx": 0, "end_char_idx": 1539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29f2f10f-cafa-424b-988c-1aa1c30759aa": {"__data__": {"id_": "29f2f10f-cafa-424b-988c-1aa1c30759aa", "embedding": null, "metadata": {"page_label": "72", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51552b13-7c1a-42a7-813f-a88c16b17b46", "node_type": "4", "metadata": {"page_label": "72", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "167ae83ca185f5cff75e0b28b7b7afa2b80ec5ba5197bbaa81ee1c47646b792e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b73d2e4b-5fb6-4498-b288-bb2a2dea8204", "node_type": "1", "metadata": {"page_label": "71", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "cab44818be2db456f791293ba059aff2f8212fff5dfe2c71f3652448d86c93aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27fe09e2-f534-44f6-846b-05f52e89df48", "node_type": "1", "metadata": {}, "hash": "71da156a2767ed6f06a48a1766c602f22f2de12a9d72e2fa83b7f271f569c164", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Use max learning rate of 6e-5 instead of 1e-4\n\u2022Use a gradient clipping value of 0.3 instead of 1.0\n\u2022Fully shu\ufb04e data\n\u2022Use a di\ufb00erent seed to ensure di\ufb00erent initialization and data order\n\u2022Reintroduce LayerNorm at embedding layer (LNem)\n\u2022Use a longer learning rate warm-up period of 1800 steps\n\u2022Remove incorrect use of weight decay on LayerNorm multipliers ( \u03b3\u2022\n\u2022)\n\u2022Use Megatron initialization rescaling (see use of z\u2032in Appendix A.5)\n\u2022Apply query keylayer scaling (Shoeybi et al., 2019)\n\u2022Apply a batch size warm-up: Use a batch size of 1024 for 7200 iterations, then increase\nto 2048\nIn addition to hyperparameter changes, we also performed additional monitoring to catch\nissues earlier. Because we observed the pathological behavior at the \ufb01rst LayerNorm com-\nponent, we started monitoring the norms of the weights \u03b3emand\u03b3in\n1(scaled by 1 /\u221a\nD).\nWith the aforementioned conservative choice of hyperparameters during v2.0, we ob-\nserved very smooth and (thankfully!) uneventful training for approximately 42 days ( /tildelow115,500\niterations). We saw few surprises both in terms of training and validation performance\ncurves (see Figure 11), as well as the norms of the gradients. The only intervention needed\nduring this period was to restart the job after 28 days, due to the underlying platform\nhaving a hard limit on the duration of the job.\nDuring this period, we observed a smoothly decreasing validation loss (except a few\njumps earlier on) until it started to \ufb02atten around 2.116 (y-axis) at the end. Running\ntraining loss has a similar trend of overall decrease with the typical jitter and random\noccasional increments.\nWe also observed that weight norms for LayerNorm components of the initial layer\nwere smooth and stable without any immediate or long-term trend changes (see Figure 11).\nThis presents some evidence that we were not su\ufb00ering from the pathological behavior we\nobserved in v1.x in regards to LayerNorm parameters \u03b3em.\nOverall, while this set of changes led to a smooth training run in v2.0, we cannot conclude\nwhich of these changes was decisive in leading to a successful training run. We defer such\ninvestigation to future work.\nC.3 Suspense\nAbout 48 days into training v2.0, we noticed that the validation loss had not improved in\na week (from iteration 115,500 to 133,200, see Figure 12, v2.0 curves). During the same\nperiod, we also noticed training loss \ufb02attening around 2.10 (with the usual jitter). We\nsuspected that the model was no longer learning properly, and decided to intervene.\nWe considered two options: 1) changing the max learning rate, 2) rolling back to an ear-\nlier checkpoint and re-shu\ufb04ing the remainder of the data to pick up a di\ufb00erent optimization\npath.\n72", "start_char_idx": 0, "end_char_idx": 2711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27fe09e2-f534-44f6-846b-05f52e89df48": {"__data__": {"id_": "27fe09e2-f534-44f6-846b-05f52e89df48", "embedding": null, "metadata": {"page_label": "73", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2e545e5-9aae-4bd6-a37c-8a63a016b0a2", "node_type": "4", "metadata": {"page_label": "73", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "676bcfb52f031ab568859cf485846c28b81ec55177e73b0e1a9739d16e7106e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29f2f10f-cafa-424b-988c-1aa1c30759aa", "node_type": "1", "metadata": {"page_label": "72", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "167ae83ca185f5cff75e0b28b7b7afa2b80ec5ba5197bbaa81ee1c47646b792e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d70ef45-14d9-4366-a5c1-44e9e4e0ac13", "node_type": "1", "metadata": {}, "hash": "c790463c1a71944b75566862d21dff301d5f565699af492a571f7aa9ae46e7cc", "class_name": "RelatedNodeInfo"}}, "text": "80000 90000 100000 110000 120000 130000 140000 150000\nSteps2.002.052.102.152.202.252.302.352.40LossLearning curve\nconfig\nv2\nv2.1\nv2.2\nv2.3\nv2.4\nv2.5\nv2.6\nmetric\nsmooth train loss\nval lossFigure 12: Loss values for various runs at the end of v2. Note v2.4 overlaps with v2.6.\nWe had two proposed ways in which to change the learning rate. An argument for\nincreasing the learning rate was the possibility that we were stuck in a local optimum.\nAllowing the optimizer to make bigger jumps would allow the model to escape the optimum\nand continue learning. On the other hand, the argument for decreasing the learning rate\nwas based on Zhang et al. (2022a) in which they had observed improvements after shrinking\nthe learning rate after getting stuck. Furthermore, we had spent more steps in the high-\nlearning-rate region of the overall learning rate schedule since, by following the Chinchilla\nscaling law, we had more total steps compared to models like BLOOM or GPT-3.\nThe other option was to roll back to an earlier checkpoint, re-shu\ufb04e the remainder of\nthe data and continue training. Chowdhery et al. (2022) found that when they saw spikes\nin the validation loss, they \u201cre-started training from a checkpoint roughly 100 steps before\nthe spike started, and skipped roughly 200\u2013500 data batches.\u201d This suggests that data\nordering mattered, and backing out of a bad data/gradient path may help. That may have\nbeen our issue with curriculum learning (v0.x), although it may have been that the issues\nwere not with curriculum learning but with other issues we \ufb01xed in v1.0.\nIn the end, we decided to shrink the learning rate, roll back to the start of the increasing\nvalidation loss trend 7 days prior, and also re-shu\ufb04e the remaining data.\nWe also became concerned that our choices were based on a single, albeit large, de-\nvelopment set. Our validation set only contained data from July 2022 (val future ; roughly\n105M tokens), whereas the training set ranged from 2007 to June 2022, meaning that the\nvalidation set was slightly out of distribution. We had done this to ensure a future-forward\nevaluation, and to ensure that the training set didn\u2019t have leaked validation data. While\nthis matched our goals, it was possible that a single month of data was not properly re\ufb02ec-\ntive of the model\u2019s abilities, and thus we were making decisions that over\ufb01t the validation\n73", "start_char_idx": 0, "end_char_idx": 2369, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d70ef45-14d9-4366-a5c1-44e9e4e0ac13": {"__data__": {"id_": "1d70ef45-14d9-4366-a5c1-44e9e4e0ac13", "embedding": null, "metadata": {"page_label": "74", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7c27c93-dde6-4a93-bc20-faba0e635acf", "node_type": "4", "metadata": {"page_label": "74", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "a738b923ee462dfc2063b24dc4f8164a2730c0ac23b9c98b8d1054e43afd57d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27fe09e2-f534-44f6-846b-05f52e89df48", "node_type": "1", "metadata": {"page_label": "73", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "676bcfb52f031ab568859cf485846c28b81ec55177e73b0e1a9739d16e7106e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c18f26b-3730-4938-9677-67b5ae468a51", "node_type": "1", "metadata": {}, "hash": "c412d1b7ff2d54949a14d65f1a774acf378a8de10a056c9ad4014953e12f70bc", "class_name": "RelatedNodeInfo"}}, "text": "data. We created a second validation set from the last 105M tokens of the training set for\no\ufb04ine evaluation (val past). These tokens were from training but would be unobserved until\nthe model \ufb01nished training. However, since the model training data was fully shu\ufb04ed, this\nvalidation set was not from a held-out time-period.\nTo assess whether a lack of progress on validation loss translates into a lack of progress on\ndownstream evaluation performance, we used two popular benchmarks: the multiple-choice\nsubset of BBH (bbh sub) and all of MMLU. These provided additional assurance that changes\nin validation loss were tracking actual model improvements. Note that running a checkpoint\non these benchmarks is much more time-consuming than computing the validation loss.\nv2 v2.1 v2 v2.1 v2 v2.1 v2 v2.1\nstep valfuture valpast mmlu bbh sub\n99300 8.43 8.60 37.77 43.57\n115500 8.30 8.43 38.79 43.10\n126600 8.34 8.24 8.40 8.32 37.86 38.09 42.37 42.79\n133200 8.35 8.38 37.02 42.26\nTable 19: Preliminary evaluation on in-distribution (val past, 105M tokens), and out-of-\ndistribution (val future ; 105M tokens) validation sets (perplexity), and downstream\ntasks (accuracy). We report perplexity since we compare models with the same\ntokenization.\nUsing our two dev sets and downstream evaluations for guidance, we made several\nattempts to improve run v2.0 and direct the model to continue learning. A summary of our\nattempts follows:\nRun Changes from v2.0 run\nShared Change - Re-shu\ufb04e future data starting from step 115500\nv2.1- Start from v2.0 step 115500\n- Reduce max learning rate from 6e-5 to 4e-5\nv2.2- Start from v2.0 step 115500\n- Increase dropout from 0.0 to 0.1\nv2.3- Start from v2.1 step 129900\n- Reduce max learning rate from 6e-5 to 2e-5\n- Increase dropout from 0.0 to 0.1\nv2.4- Start from v2.1 step 129900\n- Reduce max learning rate from 6e-5 to 2e-5\nv2.5- Start from v2.3 step 137100\n- Reduce max learning rate from 6e-5 to 1e-5\n- Increase dropout from 0.0 to 0.1\nv2.6- Start from v2.1 step 129900\n- Reduce max learning rate from 6e-5 to 2e-5\n- Reduce weight decay from 0.1 to 0.01\n74", "start_char_idx": 0, "end_char_idx": 2091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c18f26b-3730-4938-9677-67b5ae468a51": {"__data__": {"id_": "9c18f26b-3730-4938-9677-67b5ae468a51", "embedding": null, "metadata": {"page_label": "75", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6dee29e-c418-4f94-a1d0-ecaa688bd3f7", "node_type": "4", "metadata": {"page_label": "75", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d250969e518c321319ba8d3663820c5e702d98a09d92579d14e2ffadf5795a8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d70ef45-14d9-4366-a5c1-44e9e4e0ac13", "node_type": "1", "metadata": {"page_label": "74", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "a738b923ee462dfc2063b24dc4f8164a2730c0ac23b9c98b8d1054e43afd57d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4417de3c-9d8d-4511-b605-78ee08a5dabe", "node_type": "1", "metadata": {}, "hash": "d10d828175b05e1cf9b8500f56c4a6b39b3db0445f93ea7544cf7db560684dd9", "class_name": "RelatedNodeInfo"}}, "text": "After we lowered the learning rate and rolled back the model (v2.1), we observed an\ninitial sudden (and dramatic) improvement; however, validation loss quickly \ufb02attened out.\nCoupled with the mixed results on downstream evaluation, we decided to enable dropout\nfor the \ufb01rst time with a probability of 0.1.\nWith dropout, as expected, we observed a larger training loss since dropout is applied\nduring the computation of the loss (v2.2 in Figure 12). However, we observed an initially de-\ncreasing validation loss. Still, as the run progressed further, validation loss started creeping\nback up and met the value of the original run (v2.0, blue).\nBased on these observations, we decided that further decreasing the learning rate would\ngive us the best chance to continue learning successfully. We subsequently tried various\ncombinations of smaller values of learning rate and adding dropout. Observe that in v2.3\n(red) with 2e-5 max learning rate and in v2.5 (brown) with 1e-5 as its continuation, both\nwith a dropout rate of 0.1, shown in Figure 12. In Table 20, we observed v2.3 led to\nmuch better perplexity and slightly better downstream performance, and v2.5 continues\nto improve downstream performance compared to v2.3 in the beginning, while decreasing\nperplexity slightly. v2.4 (purple) attempted a max learning rate of 2e-5 as well, without\ndropout, however. The only odd run during this time is v2.6 (pink), in which we exper-\nimented with a smaller weight decay of 0.01 (compared to the original 0.1) with a max\nlearning rate of 2e-5 to investigate the possibility of getting stuck in local minima due to\ntoo strong of a pull from the weight decay. However, this yields almost the exact same curve\nas the original 0.1 weight decay (the di\ufb00erence between v2.4 and v2.6 is only the weight\ndecay, and since they yield the same curve v2.6 completely hides v2.4, rendering it invisible\nin the plot).\nIn conclusion, all of the runs (summarized in Figure 12) had the same outcome of\neventual \ufb02attening of the validation loss and sometimes even increasing the loss. We did\nnot observe that any particular change signi\ufb01cantly improved the downstream evaluations\nand validation loss (Table 20).\nAt this point, we had used 77% of our training data and were nearing the end of the\nbudget we had set aside for training. Combined with all of these observations and initial\npromising results on the downstream benchmarks, we decided to end training despite not\nhaving gone through all of our training data. Another motivating factor was the possibility\nof using remaining unseen training data for subsequent runs of di\ufb00erent styles of training\nand \ufb01netuning.\nBased on this experience, we plan to explore di\ufb00erent options in future experiments\nthat have shown the potential to lead to more stable training for longer durations, in-\ncluding SwiGLU activations (Shazeer, 2020), RoPE embeddings (Su et al., 2021b), and\nnormalization for queries and keys in the attention layers (Henry et al., 2020).\n75", "start_char_idx": 0, "end_char_idx": 2990, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4417de3c-9d8d-4511-b605-78ee08a5dabe": {"__data__": {"id_": "4417de3c-9d8d-4511-b605-78ee08a5dabe", "embedding": null, "metadata": {"page_label": "76", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c5a052c-f004-47f5-bedd-510130dbbf90", "node_type": "4", "metadata": {"page_label": "76", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "7408397257d981c89441bc6354ab713844ea0a45012b88b0771d62178f8199f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c18f26b-3730-4938-9677-67b5ae468a51", "node_type": "1", "metadata": {"page_label": "75", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "d250969e518c321319ba8d3663820c5e702d98a09d92579d14e2ffadf5795a8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "495aaa09-00d7-4262-bc21-cc771273b11a", "node_type": "1", "metadata": {}, "hash": "d736b73f01d379171db1085191a38940bae0ab7ae04aeeffbb229a0fc1dcacdb", "class_name": "RelatedNodeInfo"}}, "text": "v2 v2.1 v2.2 v2.3 v2.4 v2.5 v2 v2.1 v2.2 v2.3 v2.4 v2.5\nstep valfuture valpast\n99300 8.43 8.60\n115500 8.30 8.43\n126600 8.34 8.24 8.31 8.40 8.32 8.56\n131700 8.09 8.20 8.22 8.24\n133200 8.35 8.08 8.38 8.22\n137100 8.13 8.30\n139200 8.14 8.32\n143400 8.15 8.33\n145800 8.16 8.32\nstep mmlu bbh sub\n99300 37.77 43.57\n115500 38.79 43.10\n126600 37.86 38.09 38.77 42.37 42.79 42.82\n131700 38.76 38.51 43.02 43.49\n133200 37.02 38.90 42.26 43.46\n137100 38.71 44.02\n139200 39.02 44.20\n143400 38.98 43.20\n145800 38.80 43.37\nTable 20: Preliminary evaluation on in-distribution (val past, 105M tokens), and out-of-\ndistribution (val future ; 105M tokens) validation sets (perplexity), and downstream\ntasks (accuracy). We report perplexity here as we are comparing models with\nthe same tokenization.\n76", "start_char_idx": 0, "end_char_idx": 782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "495aaa09-00d7-4262-bc21-cc771273b11a": {"__data__": {"id_": "495aaa09-00d7-4262-bc21-cc771273b11a", "embedding": null, "metadata": {"page_label": "1", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54a16377-b341-4c4a-ab6d-eeafe8cb7fd1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "d8fb3e3d4e1b02ce49b839038e168c0846b074eefae45c1f4a76a2803905bfa1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4417de3c-9d8d-4511-b605-78ee08a5dabe", "node_type": "1", "metadata": {"page_label": "76", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}, "hash": "7408397257d981c89441bc6354ab713844ea0a45012b88b0771d62178f8199f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99312a99-9417-4418-a639-10903110c875", "node_type": "1", "metadata": {}, "hash": "029338e0f3ab8ee59d37e7d66b7256c7267ea2e6ed8883e51b858a4cb3acbdf7", "class_name": "RelatedNodeInfo"}}, "text": "QL ORA: Efficient Finetuning of Quantized LLMs\nTim Dettmers\u2217Artidoro Pagnoni\u2217Ari Holtzman\nLuke Zettlemoyer\nUniversity of Washington\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\nAbstract\nWe present QLORA, an efficient finetuning approach that reduces memory us-\nage enough to finetune a 65B parameter model on a single 48GB GPU while\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\non a single GPU. QLORAintroduces a number of innovations to save memory\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\nis information theoretically optimal for normally distributed weights (b) Double\nQuantization to reduce the average memory footprint by quantizing the quantization\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\nto finetune more than 1,000 models, providing a detailed analysis of instruction\nfollowing and chatbot performance across 8 instruction datasets, multiple model\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\nfinetuning on a small high-quality dataset leads to state-of-the-art results, even\nwhen using smaller models than the previous SoTA. We provide a detailed analysis\nof chatbot performance based on both human and GPT-4 evaluations showing that\nGPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\nthermore, we find that current chatbot benchmarks are not trustworthy to accurately\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\nwhere Guanaco fails compared to ChatGPT. We release all of our models and code,\nincluding CUDA kernels for 4-bit training.2\n1 Introduction\nFinetuning large language models (LLMs) is a highly effective way to improve their performance,\n[40,62,43,61,59,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,\nfinetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\nparameter model [ 57] requires more than 780 GB of GPU memory. While recent quantization\nmethods can reduce the memory footprint of LLMs [ 14,13,18,66], such techniques only work for\ninference and break down during training [65].\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\nperformance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]\n\u2217Equal contribution.\n2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\nPreprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023", "start_char_idx": 0, "end_char_idx": 3040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99312a99-9417-4418-a639-10903110c875": {"__data__": {"id_": "99312a99-9417-4418-a639-10903110c875", "embedding": null, "metadata": {"page_label": "2", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c9c4d8c1-408f-4496-b4d3-abedd1762e8b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7f3748beb35375bc4dc80d59cf7ddc9584c639ba167eec08d7cd81e3d5c50bc7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "495aaa09-00d7-4262-bc21-cc771273b11a", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "d8fb3e3d4e1b02ce49b839038e168c0846b074eefae45c1f4a76a2803905bfa1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2d42920-b34d-498b-8e4b-8ef47e1eb227", "node_type": "1", "metadata": {}, "hash": "f352b96e2b7a0534a539196c73ee04510f2b911545426f0dfbbf508fa0233330", "class_name": "RelatedNodeInfo"}}, "text": "Table 1: Elo ratings for a competition between\nmodels, averaged for 10,000 random initial order-\nings. The winner of a match is determined by\nGPT-4 which declares which response is better for\na given prompt of the the Vicuna benchmark. 95%\nconfidence intervals are shown ( \u00b1). After GPT-\n4, Guanaco 33B and 65B win the most matches,\nwhile Guanaco 13B scores better than Bard.\nModel Size Elo\nGPT-4 - 1348 \u00b11\nGuanaco 65B 41 GB 1022 \u00b11\nGuanaco 33B 21 GB 992 \u00b11\nVicuna 13B 26 GB 974 \u00b11\nChatGPT - 966 \u00b11\nGuanaco 13B 10 GB 916 \u00b11\nBard - 902 \u00b11\nGuanaco 7B 6 GB 879 \u00b11that are tuned by backpropagating gradients through\nthe quantized weights.\nQLORAreduces the average memory requirements\nof finetuning a 65B parameter model from >780GB\nof GPU memory to <48GB without degrading the\nruntime or predictive performance compared to a 16-\nbit fully finetuned baseline. This marks a significant\nshift in accessibility of LLM finetuning: now the\nlargest publicly available models to date finetunable\non a single GPU. Using QLORA, we train the Gua-\nnaco family of models, with the second best model\nreaching 97.8% of the performance level of ChatGPT\non the Vicuna [ 10] benchmark, while being trainable\nin less than 12 hours on a single consumer GPU;\nusing a single professional GPU over 24 hours we\nachieve 99.3% with our largest model, essentially\nclosing the gap to ChatGPT on the Vicuna bench-\nmark. When deployed, our smallest Guanaco model\n(7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than\n20 percentage points on the Vicuna benchmark (Table 6).\nQLORAintroduces multiple innovations designed to reduce memory use without sacrificing per-\nformance: (1) 4-bit NormalFloat , an information theoretically optimal quantization data type for\nnormally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.\n(2)Double Quantization , a method that quantizes the quantization constants, saving an average\nof about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3) Paged Optimizers ,\nusing NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when\nprocessing a mini-batch with a long sequence length. We combine these contributions into a better\ntuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of\nthe accuracy tradeoffs seen in prior work.\nQLORA\u2019s efficiency enables us to perform an in-depth study of instruction finetuning and chatbot\nperformance on model scales that would be impossible using regular finetuning due to memory\noverhead. Therefore, we train more than 1,000 models across several instruction tuning datasets,\nmodel architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA\nrecovers 16-bit performance (\u00a74) and training a state-of-the-art chatbot, Guanaco , (\u00a75), we also\nanalyze trends in the trained models. First, we find that data quality is far more important than\ndataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,\nsubsampled) on chatbot performance, even when both are meant to support instruction following\ngeneralization. Second, we show that strong Massive Multitask Language Understanding (MMLU)\nbenchmark performance does not imply strong Vicuna chatbot benchmark performance and vice\nversa\u2014in other words, dataset suitability matters more than size for a given task.\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human\nraters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\nagainst each other in matches to produce the best response for a given prompt. The winner of a\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into\nElo scores [ 16,17] which determine the ranking of chatbot performance.", "start_char_idx": 0, "end_char_idx": 3872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2d42920-b34d-498b-8e4b-8ef47e1eb227": {"__data__": {"id_": "a2d42920-b34d-498b-8e4b-8ef47e1eb227", "embedding": null, "metadata": {"page_label": "2", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c9c4d8c1-408f-4496-b4d3-abedd1762e8b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7f3748beb35375bc4dc80d59cf7ddc9584c639ba167eec08d7cd81e3d5c50bc7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99312a99-9417-4418-a639-10903110c875", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "50dee0e56bbc1418882923fddf21654e8598925d71de8c4056940304e2898ca9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1527307-042e-40ef-a150-aa6ba301bfb3", "node_type": "1", "metadata": {}, "hash": "bf14166cd0c2574d83c8dc835a0e7447062682f036af9ef4377d6b29c00c874c", "class_name": "RelatedNodeInfo"}}, "text": "Second, we show that strong Massive Multitask Language Understanding (MMLU)\nbenchmark performance does not imply strong Vicuna chatbot benchmark performance and vice\nversa\u2014in other words, dataset suitability matters more than size for a given task.\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human\nraters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\nagainst each other in matches to produce the best response for a given prompt. The winner of a\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into\nElo scores [ 16,17] which determine the ranking of chatbot performance. We find that GPT-4 and\nhuman evaluations largely agree on the rank of model performance in the tournaments, but we also\nfind there are instances of strong disagreement. As such, we highlight that model-based evaluation\nwhile providing a cheap alternative to human-annotation also has its uncertainties.\nWe augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy-\nsis highlights success and failure cases that were not captured by the quantitative benchmarks.\nWe release all model generations with human and GPT-4 annotations to facilitate further study. We\nopen-source our codebase and CUDA kernels and integrate our methods into the Hugging Face\ntransformers stack [ 64], making them easily accessible to all. We release a collection of adapters\nfor 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32\ndifferent open sourced, finetuned models.\n2", "start_char_idx": 3176, "end_char_idx": 4804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1527307-042e-40ef-a150-aa6ba301bfb3": {"__data__": {"id_": "a1527307-042e-40ef-a150-aa6ba301bfb3", "embedding": null, "metadata": {"page_label": "3", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4cd007e5-bd11-4e2f-aa9d-50c4b2feed70", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7ddda397aa9a44326a3517b502c8bf7e49417bbb817ce2f7b1cf2eccc3d1ce6e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2d42920-b34d-498b-8e4b-8ef47e1eb227", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "bf1d3565f8f965c19a823e900dce16ae033856615447fdfac4921ac622720998", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb72a68c-84ba-49dc-8d47-eee11abe4e8d", "node_type": "1", "metadata": {}, "hash": "cd2e7b6ac309a742f6feae1b22fdbea55dc9afcf1dd833f2200da8cb25705ec5", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by\nquantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.\n2 Background\nBlock-wise k-bit Quantization Quantization is the process of discretizing an input from a rep-\nresentation that holds more information to a representation with less information. It often means\ntaking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to\n8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is\ncommonly rescaled into the target data type range through normalization by the absolute maximum\nof the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit\nFloating Point (FP32) tensor into a Int8 tensor with range [\u2212127,127]:\nXInt8=round\u0012127\nabsmax (XFP32)XFP32\u0013\n=round (cFP32\u00b7XFP32), (1)\nwhere cis the quantization constant orquantization scale . Dequantization is the inverse:\ndequant (cFP32,XInt8) =XInt8\ncFP32=XFP32(2)\nThe problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input\ntensor, then the quantization bins\u2014certain bit combinations\u2014are not utilized well with few or no\nnumbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the\ninput tensor into blocks that are independently quantized, each with their own quantization constant c.\nThis can be formalized as follows: We chunk the input tensor X\u2208Rb\u00d7hintoncontiguous blocks of\nsizeBby flattening the input tensor and slicing the linear segment into n= (b\u00d7h)/Bblocks. We\nquantize these blocks independently with Equation 1 to create a quantized tensor and nquantization\nconstants ci.\nLow-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory\nrequirements by using a small set of trainable parameters, often termed adapters, while not updating\nthe full model parameters which remain fixed. Gradients during stochastic gradient descent are\npassed through the fixed pretrained model weights to the adapter, which is updated to optimize the\nloss function. LoRA augments a linear projection through an additional factorized projection. Given\na projection XW =YwithX\u2208Rb\u00d7h,W\u2208Rh\u00d7oLoRA computes:\nY=XW +sXL 1L2, (3)\nwhereL1\u2208Rh\u00d7randL2\u2208Rr\u00d7o, and sis a scalar.\nMemory Requirement of Parameter-Efficient Finetuning One important point of discussion is\nthe memory requirement of LoRA during training both in terms of the number and size of adapters\nused. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve\nperformance without significantly increasing the total memory used. While LoRA was designed as a\n3", "start_char_idx": 0, "end_char_idx": 2749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb72a68c-84ba-49dc-8d47-eee11abe4e8d": {"__data__": {"id_": "eb72a68c-84ba-49dc-8d47-eee11abe4e8d", "embedding": null, "metadata": {"page_label": "4", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "577c11e7-bb98-41df-8de9-cdaaa1d639c9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "6e47729b53342bfbe9ef649ee43f74f389345a28466d3e4ecc661c5a38ef343a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1527307-042e-40ef-a150-aa6ba301bfb3", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7ddda397aa9a44326a3517b502c8bf7e49417bbb817ce2f7b1cf2eccc3d1ce6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6dc50b2d-9137-4d9f-be4f-a8463137625a", "node_type": "1", "metadata": {}, "hash": "971aa5b70fcb2c9fce549e54cbe4082a3854a8e2a75ca0a4bbf60418415504dd", "class_name": "RelatedNodeInfo"}}, "text": "Parameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning\ncomes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA\nmodel trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used\n0.2% of the original model weights[ 28,37], the LoRA input gradients have a memory footprint\nof 567 MB while the LoRA parameters take up only 26 MB. With gradient checkpointing [ 9], the\ninput gradients reduce to an average of 18 MB per sequence making them more memory intensive\nthan all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of\nmemory. This highlights that gradient checkpointing is important but also that aggressively reducing\nthe amount of LoRA parameter yields only minor memory benefits. This means we can use more\nadapters without significantly increasing the overall training memory footprint (see Appendix G\nfor a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision\nperformance.\n3 QL ORA Finetuning\nQLORAachieves high-fidelity 4-bit finetuning via two techniques we propose\u20144-bit NormalFloat\n(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to\nprevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\ntraditionally made finetuning on a single machine difficult for large models.\nQLORAhas one low-precision storage data type, in our case usually 4-bit, and one computation data\ntype that is usually BFloat16. In practice, this means whenever a QLORAweight tensor is used, we\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\nWe now discuss the components of QL ORA followed by a formal definition of QL ORA.\n4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization\n[15] which is an information-theoretically optimal data type that ensures each quantization bin has an\nequal number of values assigned from the input tensor. Quantile quantization works by estimating\nthe quantile of the input tensor through the empirical cumulative distribution function.\nThe main limitation of quantile quantization is that the process of quantile estimation is expensive.\nTherefore fast quantile approximation algorithms, such as SRAM quantiles [ 15], are used to estimate\nthem. Due to the approximate nature of these quantile estimation algorithms, the data type has large\nquantization errors for outliers, which are often the most important values.\nExpensive quantile estimates and approximation errors can be avoided when input tensors come from\na distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles\nmaking exact quantile estimation computationally feasible.\nSince pretrained neural network weights usually have a zero-centered normal distribution with\nstandard deviation \u03c3(see Appendix F), we can transform all weights to a single fixed distribution by\nscaling \u03c3such that the distribution fits exactly into the range of our data type. For our data type, we\nset the arbitrary range [\u22121,1]. As such, both the quantiles for the data type and the neural network\nweights need to be normalized into this range.\nThe information theoretically optimal data type for zero-mean normal distributions with arbitrary\nstandard deviations \u03c3in the range [\u22121,1]is computed as follows: (1) estimate the 2k+ 1quantiles\nof a theoretical N(0,1)distribution to obtain a k-bit quantile quantization data type for normal distri-\nbutions, (2) take this data type and normalize its values into the [\u22121,1]range, (3) quantize an input\nweight tensor by normalizing it into the [\u22121,1]range through absolute maximum rescaling.\nOnce the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to\nrescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data\ntype. More formally, we estimate the 2kvalues qiof the data type as follows:\nqi=1\n2\u0012\nQX\u0012i\n2k+ 1\u0013\n+QX\u0012i+ 1\n2k+ 1\u0013\u0013\n, (4)\nwhere QX(\u00b7)is the quantile function of the standard normal distribution N(0,1). A problem for\na symmetric k-bit quantization is that this approach does not have an exact representation of zero,\nwhich is an important property to quantize padding and other zero-valued elements with no error. To\n4", "start_char_idx": 0, "end_char_idx": 4375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6dc50b2d-9137-4d9f-be4f-a8463137625a": {"__data__": {"id_": "6dc50b2d-9137-4d9f-be4f-a8463137625a", "embedding": null, "metadata": {"page_label": "5", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fd8de60-897f-4090-983e-1d52831ea649", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "31d2aef22932596c8224723bd081d41b3d377f74dd295c9565cbbd39f9ea4ab1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb72a68c-84ba-49dc-8d47-eee11abe4e8d", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "6e47729b53342bfbe9ef649ee43f74f389345a28466d3e4ecc661c5a38ef343a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1101ad3-d165-4648-924c-11c3a70013c3", "node_type": "1", "metadata": {}, "hash": "40b620b57bec38801daddd6726cd8d60741bdd6d2a0a77731b59c03f2368f0d3", "class_name": "RelatedNodeInfo"}}, "text": "ensure a discrete zeropoint of 0and to use all 2kbits for a k-bit datatype, we create an asymmetric\ndata type by estimating the quantiles qiof two ranges qi:2k\u22121for the negative part and 2k\u22121+ 1for\nthe positive part and then we unify these sets of qiand remove one of the two zeros that occurs in both\nsets. We term the resulting data type that has equal expected number of values in each quantization bin\nk-bit NormalFloat (NFk), since the data type is information-theoretically optimal for zero-centered\nnormally distributed data. The exact values of this data type can be found in Appendix E.\nDouble Quantization We introduce Double Quantization (DQ), the process of quantizing the\nquantization constants for additional memory savings. While a small blocksize is required for precise\n4-bit quantization [ 13], it also has a considerable memory overhead. For example, using 32-bit\nconstants and a blocksize of 64 for W, quantization constants add 32/64 = 0 .5bits per parameter on\naverage. Double Quantization helps reduce the memory footprint of quantization constants.\nMore specifically, Double Quantization treats quantization constants cFP32\n2of the first quantization\nas inputs to a second quantization. This second step yields the quantized quantization constants\ncFP8\n2and the second level of quantization constants cFP32\n1. We use 8-bit Floats with a blocksize of\n256 for the second quantization as no performance degradation is observed for 8-bit quantization,\nin line with results from Dettmers and Zettlemoyer [13]. Since the cFP32\n2are positive, we subtract\nthe mean from c2before quantization to center the values around zero and make use of symmetric\nquantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per\nparameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64\u00b7256) = 0 .127bits, a reduction of 0.373 bits\nper parameter.\nPaged Optimizers use the NVIDIA unified memory3feature wich does automatic page-to-page\ntransfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\noccasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\nand the disk. We use this feature to allocate paged memory for the optimizer states which are then\nautomatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\nmemory when the memory is needed in the optimizer update step.\nQL ORA.Using the components described above, we define QLORAfor a single linear layer in\nthe quantized base model with a single LoRA adapter as follows:\nYBF16=XBF16doubleDequant (cFP32\n1, ck-bit\n2,WNF4) +XBF16LBF16\n1LBF16\n2, (5)\nwhere doubleDequant (\u00b7)is defined as:\ndoubleDequant (cFP32\n1, ck-bit\n2,Wk-bit) =dequant (dequant (cFP32\n1, ck-bit\n2),W4bit) =WBF16,(6)\nWe use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\nand a blocksize of 256 for c2to conserve memory.\nFor parameter updates only the gradient with respect to the error for the adapters weights\u2202E\n\u2202Liare\nneeded, and not for 4-bit weights\u2202E\n\u2202W. However, the calculation of\u2202E\n\u2202Lientails the calculation of\u2202X\n\u2202W\nwhich proceeds via equation (5) with dequantization from storage WNF4to computation data type\nWBF16to calculate the derivative\u2202X\n\u2202Win BFloat16 precision.\nTo summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\ndata type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\nto perform the forward and backward pass, but we only compute weight gradients for the LoRA\nparameters which use 16-bit BrainFloat.\n4 QLoRA vs. Standard Finetuning\nWe have discussed how QLoRA works and how it can significantly reduce the required memory for\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model\nfinetuning.", "start_char_idx": 0, "end_char_idx": 3821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1101ad3-d165-4648-924c-11c3a70013c3": {"__data__": {"id_": "c1101ad3-d165-4648-924c-11c3a70013c3", "embedding": null, "metadata": {"page_label": "5", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fd8de60-897f-4090-983e-1d52831ea649", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "31d2aef22932596c8224723bd081d41b3d377f74dd295c9565cbbd39f9ea4ab1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6dc50b2d-9137-4d9f-be4f-a8463137625a", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "329462cce484b594312e76d3d14aa3d4a28bd8c79b29d7610641bc4b39a6c33b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6bd935c-ade2-4adf-8068-91093194f2a1", "node_type": "1", "metadata": {}, "hash": "a64145d70a7cb436c5e8ba0c8d59fef6736e3f5d7dba2320ace6f00473c90767", "class_name": "RelatedNodeInfo"}}, "text": "However, the calculation of\u2202E\n\u2202Lientails the calculation of\u2202X\n\u2202W\nwhich proceeds via equation (5) with dequantization from storage WNF4to computation data type\nWBF16to calculate the derivative\u2202X\n\u2202Win BFloat16 precision.\nTo summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\ndata type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\nto perform the forward and backward pass, but we only compute weight gradients for the LoRA\nparameters which use 16-bit BrainFloat.\n4 QLoRA vs. Standard Finetuning\nWe have discussed how QLoRA works and how it can significantly reduce the required memory for\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model\nfinetuning. Furthermore, we want to analyze the components of QLoRA including the impact of\nNormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed\nat answering these questions.\n3https://docs.nvidia.com/cuda/cuda-c-programming-guide\n5", "start_char_idx": 3054, "end_char_idx": 4086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6bd935c-ade2-4adf-8068-91093194f2a1": {"__data__": {"id_": "d6bd935c-ade2-4adf-8068-91093194f2a1", "embedding": null, "metadata": {"page_label": "6", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ed38d5d-a5f1-4d79-8ac5-f0acbeb0b124", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "6486ff2a487b79b62ebe91437ecb2d5ab3c66edafc14b4e0ebd8b2fd6745b18f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1101ad3-d165-4648-924c-11c3a70013c3", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "aab1dcf8abf1e581a368cd42f0485e8ddb70cf49a83a646e595c7b11564e9573", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c729b94e-6b22-415b-b0c6-bd1c37852472", "node_type": "1", "metadata": {}, "hash": "0f7e072798d0edd42b364f56289e6b774d86a54702aa7e015fbf9d6bd18804b3", "class_name": "RelatedNodeInfo"}}, "text": "Experimental setup. We consider three architectures (encoder, encoder-decoder, and decoder only)\nand compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our\nevaluations include GLUE [ 58] with RoBERTa-large [ 38], Super-NaturalInstructions (TKInstruct)\n[61] with T5 [ 49], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [ 39] and Alpaca\n[55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of\nDettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity\nacross different models (OPT [ 72], LLaMA [ 57], BLOOM [ 52], Pythia [ 7]) for model sizes 125m -\n13B. We provide more details in the results section for each particular setup to make the results more\nreadable. Full details in Appendix A.\nQLoRA-AllQLoRA-FFN\nQLoRA-AttentionAlpaca (ours)\nStanford-Alpaca\nModel6061626364RougeL\nbits\n4\n16\nFigure 2: RougeL for LLaMA 7B models on the\nAlpaca dataset. Each point represents a run with a\ndifferent random seed. We improve on the Stanford\nAlpaca fully finetuned default hyperparameters to\nconstruct a strong 16-bit baseline for comparisons.\nUsing LoRA on all transformer layers is critical to\nmatch 16-bit performance.While paged optimizers are critical to do 33B/65B\nQLORAtuning on a single 24/48GB GPU, we do\nnot provide hard measurements for Paged Optimiz-\ners since the paging only occurs when processing\nmini-batches with long sequence lengths, which is\nrare. We do, however, perform an analysis of the\nruntime of paged optimizers for 65B models on\n48GB GPUs and find that with a batch size of 16,\npaged optimizers provide the same training speed\nas regular optimizers. Future work should measure\nand characterize under what circumstances slow-\ndowns occur from the paging process.\nDefault LoRA hyperparameters do not match 16-\nbit performance When using the standard prac-\ntice of applying LoRA to query and value attention\nprojection matrices [ 28], we are not able to replicate\nfull finetuning performance for large base models.\nAs shown in Figure 2 for LLaMA 7B finetuning on\nAlpaca, we find that the most critical LoRA hyper-\nparameter is how many LoRA adapters are used in\ntotal and that LoRA on all linear transformer block\nlayers are required to match full finetuning perfor-\nmance. Other LoRA hyperparameters, such as the\nprojection dimension r, do not affect performance (see Appendix A).\n1010\n1011\nT otal model bits\n0.60\n0.61\n0.62\n0.63\n0.64\n0.65\n0.66\n0.67Mean zeroshot accuracy\n4-bit LLaMA\nFloat\nNFloat\nNFloat + DQData type\nFigure 3: Mean zero-shot accuracy over Wino-\ngrande, HellaSwag, PiQA, Arc-Easy, and Arc-\nChallenge using LLaMA models with different 4-bit\ndata types. The NormalFloat data type significantly\nimproves the bit-for-bit accuracy gains compared\nto regular 4-bit Floats. While Double Quantization\n(DQ) only leads to minor gains, it allows for a more\nfine-grained control over the memory footprint to fit\nmodels of certain size (33B/65B) into certain GPUs\n(24/48GB).Similarly, we find that default hyperparameters for\nfully finetuned baselines are undertuned. We do a\nhyperparameter search over learning rates 1e-6 to\n5e-5 and batch sizes 8 to 128 to find robust baselines.\nResults for 7B LLaMA finetuning on Alpaca are\nshown in Figure 2.\n4-bit NormalFloat yields better performance\nthan 4-bit Floating Point While the 4-bit\nNormalFloat (NF4) data type is information-\ntheoretically optimal, it still needs to be determined\nif this property translates to empirical advantages.", "start_char_idx": 0, "end_char_idx": 3528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c729b94e-6b22-415b-b0c6-bd1c37852472": {"__data__": {"id_": "c729b94e-6b22-415b-b0c6-bd1c37852472", "embedding": null, "metadata": {"page_label": "6", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ed38d5d-a5f1-4d79-8ac5-f0acbeb0b124", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "6486ff2a487b79b62ebe91437ecb2d5ab3c66edafc14b4e0ebd8b2fd6745b18f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6bd935c-ade2-4adf-8068-91093194f2a1", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "e9d010224878a733e5f2b2875ea0778dbdf5092cd042d8cd77fd802c71dd8389", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60931171-b5d5-48bf-b05b-c201cb1ad237", "node_type": "1", "metadata": {}, "hash": "c84bf50a483775a5d470bca5c4229c5a377405274813a9ede0567a3436e45542", "class_name": "RelatedNodeInfo"}}, "text": "While Double Quantization\n(DQ) only leads to minor gains, it allows for a more\nfine-grained control over the memory footprint to fit\nmodels of certain size (33B/65B) into certain GPUs\n(24/48GB).Similarly, we find that default hyperparameters for\nfully finetuned baselines are undertuned. We do a\nhyperparameter search over learning rates 1e-6 to\n5e-5 and batch sizes 8 to 128 to find robust baselines.\nResults for 7B LLaMA finetuning on Alpaca are\nshown in Figure 2.\n4-bit NormalFloat yields better performance\nthan 4-bit Floating Point While the 4-bit\nNormalFloat (NF4) data type is information-\ntheoretically optimal, it still needs to be determined\nif this property translates to empirical advantages.\nWe follow the setup from Dettmers and Zettlemoyer\n[13] where quantized LLMs (OPT [ 72], BLOOM\n[52], Pythia [ 7], LLaMA) of different sizes (125M\nto 65B) with different data types are evaluated on\nlanguage modeling and a set of zero-shot tasks. In\nFigure 3 and Table 2 we see that NF4 improves per-\nformance significantly over FP4 and Int4 and that\ndouble quantization reduces the memory footprint\nwithout degrading performance.\nk-bit QL ORAmatches 16-bit full finetuning and\n16-bit LoRA performance Recent findings have\nestablished that 4-bit quantization for inference is\n6", "start_char_idx": 2824, "end_char_idx": 4103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60931171-b5d5-48bf-b05b-c201cb1ad237": {"__data__": {"id_": "60931171-b5d5-48bf-b05b-c201cb1ad237", "embedding": null, "metadata": {"page_label": "7", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8605f3a-7656-43e8-a4e6-81677bcc909a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "8ddda4508c5c43df74802c1e0a71ab25556ed38238187e5d68573a0818e43168", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c729b94e-6b22-415b-b0c6-bd1c37852472", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "9fdb93092620078903b6f89363fc3c5a24d0755bd89f4ea720a43aacd1c0c757", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d226190e-198f-484b-a37d-1c43de9c1875", "node_type": "1", "metadata": {}, "hash": "94bdc0ecf80874ac962334658be3b10f6bff7f5145f9d8067247e7492b1bbb93", "class_name": "RelatedNodeInfo"}}, "text": "Table 3: Experiments comparing 16-bit BrainFloat (BF16), 8-bit Integer (Int8), 4-bit Float (FP4), and 4-\nbit NormalFloat (NF4) on GLUE and Super-NaturalInstructions. QLORAreplicates 16-bit LoRA and full-\nfinetuning.\nDataset GLUE (Acc.) Super-NaturalInstructions (RougeL)\nModel RoBERTa-large T5-80M T5-250M T5-780M T5-3B T5-11B\nBF16 88.6 40.1 42.1 48.0 54.3 62.0\nBF16 replication 88.6 40.0 42.2 47.3 54.9 -\nLoRA BF16 88.8 40.5 42.6 47.1 55.4 60.7\nQLORA Int8 88.8 40.4 42.9 45.4 56.5 60.7\nQLORA FP4 88.6 40.3 42.4 47.5 55.6 60.9\nQLORA NF4 + DQ - 40.4 42.7 47.7 55.3 60.9\npossible, but leads to performance degradation rel-\native to 16-bit [ 13,18]. This raises the crucial question of whether the lost performance can be\nrecovered by conducting 4-bit adapter finetuning. We test this for two setups.\nTable 2: Pile Common Crawl mean\nperplexity for different data types\nfor 125M to 13B OPT, BLOOM,\nLLaMA, and Pythia models.\nData type Mean PPL\nInt4 34.34\nFloat4 (E2M1) 31.07\nFloat4 (E3M0) 29.48\nNFloat4 + DQ 27.41The first focuses on a comparison with full 16-bit finetuning\nof RoBERTA and T5 models sized 125M to 3B parameters on\nGLUE and the Super-NaturalInstructions dataset. Results are\nshown in Table 3. In both datasets, we observe that 16-bit, 8-bit,\nand 4-bit adapter methods replicate the performance of the fully\nfinetuned 16-bit baseline. This suggests that the performance lost\ndue to the imprecise quantization can be fully recovered through\nadapter finetuning after quantization.\nFor our second setup, since full finetuning models at and beyond\n11B parameters requires more than one server of high memory\nGPUs, we continue to test whether 4-bit QLORAcan match\n16-bit LoRA at the 7B to 65B parameter scales. To this end, we\nfinetune LLaMA 7B through 65B on two instruction following\ndatasets, Alpaca and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results\nare shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit\nLoRA MMLU performance. In addition, we also note that QLORAwith FP4 lags behind the 16-bit\nbrain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1)\nQLORAwith NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance,\nand (2) NF4 is superior to FP4 in terms of quantization precision.\nSummary Our results consistently show that 4-bit QLORAwith NF4 data type matches 16-\nbit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with well-\nestablished evaluation setups. We have also shown that NF4 is more effective than FP4 and that\ndouble quantization does not degrade performance. Combined, this forms compelling evidence that\n4-bit QL ORA tuning reliably yields results matching 16-bit methods.\nIn line with previous work on quantization [ 13], our MMLU and Elo results indicate that with a given\nfinetuning and inference resource budget it is beneficial to increase the number of parameters in the\nbase model while decreasing their precision. This highlights the importance of efficiency benefits\nfrom QLORA. Since we did not observe performance degradation compared to full-finetuning in\nour experiments with 4-bit finetuning, this raises the question of where the performance-precision\ntrade-off exactly lies for QLoRA tuning, which we leave to future work to explore.", "start_char_idx": 0, "end_char_idx": 3344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d226190e-198f-484b-a37d-1c43de9c1875": {"__data__": {"id_": "d226190e-198f-484b-a37d-1c43de9c1875", "embedding": null, "metadata": {"page_label": "7", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8605f3a-7656-43e8-a4e6-81677bcc909a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "8ddda4508c5c43df74802c1e0a71ab25556ed38238187e5d68573a0818e43168", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60931171-b5d5-48bf-b05b-c201cb1ad237", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "4c74f074692c486a4a0c703e75976256cca70fdbbd448f6c146d5bf078a0f0fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "933dfac2-afde-4083-b742-41d075f5926b", "node_type": "1", "metadata": {}, "hash": "fdb50b8c1f8f68028e779e89c5128b310b7c85fc7f8f0f8622e4cd748b180ce7", "class_name": "RelatedNodeInfo"}}, "text": "We have also shown that NF4 is more effective than FP4 and that\ndouble quantization does not degrade performance. Combined, this forms compelling evidence that\n4-bit QL ORA tuning reliably yields results matching 16-bit methods.\nIn line with previous work on quantization [ 13], our MMLU and Elo results indicate that with a given\nfinetuning and inference resource budget it is beneficial to increase the number of parameters in the\nbase model while decreasing their precision. This highlights the importance of efficiency benefits\nfrom QLORA. Since we did not observe performance degradation compared to full-finetuning in\nour experiments with 4-bit finetuning, this raises the question of where the performance-precision\ntrade-off exactly lies for QLoRA tuning, which we leave to future work to explore.\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full\n16-bit finetuning on academic research hardware.\n5 Pushing the Chatbot State-of-the-art with QLoRA\nHaving established that 4-bit QLORAmatches 16-bit performance across scales, tasks, and datasets\nwe conduct an in-depth study of instruction finetuning up to the largest open-source language models\navailable for research. To assess the performance of instruction finetuning these models, we evaluate\n7", "start_char_idx": 2539, "end_char_idx": 3846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "933dfac2-afde-4083-b742-41d075f5926b": {"__data__": {"id_": "933dfac2-afde-4083-b742-41d075f5926b", "embedding": null, "metadata": {"page_label": "8", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "560552c8-8b86-4624-9648-827df3ef3543", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7d2547e4fd998655e70911f39d00f09e2fd6e272244f16151dc4df081911ad48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d226190e-198f-484b-a37d-1c43de9c1875", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "c77cb15eae712bed3df10d5df2858f7cc800a3d97098ba90406c6c62c3db0318", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "285cf2fb-92b2-4fe6-b97b-43b21867eb63", "node_type": "1", "metadata": {}, "hash": "b7ee7ddfb73a35a809177244be8960c088e9cb307086f3ceeee29c5772470bc1", "class_name": "RelatedNodeInfo"}}, "text": "Table 4: Mean 5-shot MMLU test accuracy for LLaMA 7-65B models finetuned with adapters on Alpaca and\nFLAN v2 for different data types. Overall, NF4 with double quantization (DQ) matches BFloat16 performance,\nwhile FP4 is consistently one percentage point behind both.\nMean 5-shot MMLU Accuracy\nLLaMA Size 7B 13B 33B 65B Mean\nDataset Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2\nBFloat16 38.4 45.6 47.2 50.6 57.7 60.5 61.8 62.5 53.0\nFloat4 37.2 44.0 47.3 50.0 55.9 58.5 61.3 63.3 52.2\nNFloat4 + DQ 39.0 44.5 47.5 50.7 57.3 59.2 61.8 63.9 53.1\non a challenging Natural Language Understanding benchmark (MMLU) and develop new methods\nfor real-world chatbot performance evaluation.\n5.1 Experimental setup\nWe now describe an overview of the experimental setup with full details in Appendix B.\nData As, to our knowledge, there is no comprehensive study of recent instruction-following datasets,\nwe select eight recent datasets. We include datasets obtained through crowd-sourcing (OASST1 [ 31],\nHH-RLHF [ 4]), distillation from instruction-tuned models (Alpaca [ 55], self-instruct [ 59], unnatural-\ninstructions [ 26]), corpora aggregations (FLAN v2 [ 12]), as well as hybrids (Chip2 [ 32], Long-\nform [30]). These datasets cover different languages, data sizes, and licenses.\nTraining Setup To avoid confounding effects from different training objectives, we perform QLoRA\nfinetuning with cross-entropy loss (supervised learning) without reinforcement learning, even for\ndatasets that include human judgments of different responses. For datasets that have a clear distinction\nbetween instruction and response, we finetune only on the response (see ablations in Appendix B).\nFor OASST1 and HH-RLHF, multiple responses are available. We then select the top response at\nevery level of the conversation tree and finetune on the full selected conversation, including the\ninstructions. In all of our experiments, we use NF4 QLORAwith double quantization and paged\noptimizers to prevent memory spikes during gradient checkpointing. We do small hyperparameter\nsearches for the 13B and 33B LLaMA models and we find that all hyperparameter settings found\nat 7B generalize (including number of epochs) except learning rate and batch size. We halve the\nlearning rate for 33B and 65B while doubling the batch size.\nBaselines We compare our models to both research (Vicuna [ 10] and Open Assistant [ 31]) and\ncommercial (GPT-4 [ 42], GPT-3.5-turbo and Bard) chatbot systems. The Open Assistant model is\na LLaMA 33B model finetuned with Reinforcement Learning from Human Feedback (RLHF) on\nthe same OASST1 dataset that we experiment with. Vicuna does full fine-tuning of LLaMA 13B\non proprietary user-shared conversations from ShareGPT and is thus the result of distillation from\nOpenAI GPT models.\n5.2 Evaluation\nTable 5: MMLU 5-shot test results for different\nsizes of LLaMA finetuned on the corresponding\ndatasets using QLoRA.", "start_char_idx": 0, "end_char_idx": 2925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "285cf2fb-92b2-4fe6-b97b-43b21867eb63": {"__data__": {"id_": "285cf2fb-92b2-4fe6-b97b-43b21867eb63", "embedding": null, "metadata": {"page_label": "8", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "560552c8-8b86-4624-9648-827df3ef3543", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7d2547e4fd998655e70911f39d00f09e2fd6e272244f16151dc4df081911ad48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "933dfac2-afde-4083-b742-41d075f5926b", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "2a748b7a98b97ed9cd8222a10cd0020dd6930f7235036f5eb7ff873ceb2a9adb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b3806c7-4b7f-4acd-91ab-046dc8023b1f", "node_type": "1", "metadata": {}, "hash": "df6d02dde5c462d5f641ccfa396007ca89c96830efd537d37a1244d4793300f9", "class_name": "RelatedNodeInfo"}}, "text": "We halve the\nlearning rate for 33B and 65B while doubling the batch size.\nBaselines We compare our models to both research (Vicuna [ 10] and Open Assistant [ 31]) and\ncommercial (GPT-4 [ 42], GPT-3.5-turbo and Bard) chatbot systems. The Open Assistant model is\na LLaMA 33B model finetuned with Reinforcement Learning from Human Feedback (RLHF) on\nthe same OASST1 dataset that we experiment with. Vicuna does full fine-tuning of LLaMA 13B\non proprietary user-shared conversations from ShareGPT and is thus the result of distillation from\nOpenAI GPT models.\n5.2 Evaluation\nTable 5: MMLU 5-shot test results for different\nsizes of LLaMA finetuned on the corresponding\ndatasets using QLoRA.\nDataset 7B 13B 33B 65B\nLLaMA no tuning 35.1 46.9 57.8 63.4\nSelf-Instruct 36.4 33.3 53.0 56.7\nLongform 32.1 43.2 56.6 59.7\nChip2 34.5 41.6 53.6 59.8\nHH-RLHF 34.9 44.6 55.8 60.1\nUnnatural Instruct 41.9 48.1 57.3 61.3\nGuanaco (OASST1) 36.6 46.4 57.0 62.2\nAlpaca 38.8 47.8 57.3 62.5\nFLAN v2 44.5 51.4 59.2 63.9Following common practice, we use the MMLU (Mas-\nsively Multitask Language Understanding) benchmark\n[24] to measure performance on a range of language un-\nderstanding tasks. This is a multiple-choice benchmark\ncovering 57 tasks including elementary mathematics,\nUS history, computer science, law, and more. We report\n5-shot test accuracy.\nWe also test generative language capabilities through\nboth automated and human evaluations. This second\nset of evaluations relies on queries curated by humans\nand aims at measuring the quality of model responses.\nWhile this is a more realistic testbed for chatbot model\nperformance and is growing in popularity, there is no\ncommonly accepted protocol in the literature. We de-\nscribe below our proposed setup, using nucleus sampling with p= 0.9and temperature 0.7in all\ncases.\n8", "start_char_idx": 2239, "end_char_idx": 4049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b3806c7-4b7f-4acd-91ab-046dc8023b1f": {"__data__": {"id_": "1b3806c7-4b7f-4acd-91ab-046dc8023b1f", "embedding": null, "metadata": {"page_label": "9", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbf8dd20-f397-4917-a962-fc1dafdc121b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "05306b9067664b36a7a85bb41c3d9aa21d3170774e5df2f63e297e0ffc5e0f2f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "285cf2fb-92b2-4fe6-b97b-43b21867eb63", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "85a0f98cca7133b5784ff548891435b8d0f286346d089b6c121ffb3628c79e0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9394d0c-ddf4-4420-b307-1c790d35eede", "node_type": "1", "metadata": {}, "hash": "4f7319d1452559b57c59fed2e8590a67d2e947b2153d67370d4fd2ef4c324f1b", "class_name": "RelatedNodeInfo"}}, "text": "Benchmark Data We evaluate on two curated datasets of queries (questions): the Vicuna prompts\n[10] and the OASST1 validation dataset [ 31]. We use the Vicuna prompts, a set of 80 prompts from a\ndiverse set of categories, without modifications. The OASST1 dataset is a multilingual collection of\ncrowd-sourced multiturn dialogs between a user and an assistant. We select all user messages in the\nvalidation dataset as queries and include previous turns in the prompt. This procedure leads to 953\nunique user queries. We term these two datasets the Vicuna and OA benchmarks.\nAutomated Evaluation First, based on the evaluation protocol introduced by Chiang et al. [10],\nwe use GPT-4 to rate the performance of different systems against ChatGPT (GPT-3.5 Turbo) on the\nVicuna benchmark. Given a query along with ChatGPT\u2019s and a model\u2019s responses, GPT-4 is prompted\nto assign a score out of ten to both responses and provide an explanation. The overall performance of\na model is calculated as a percentage of the score that ChatGPT achieved. Note this relative score\ncan be higher than 100% if the model achieves a higher absolute score than ChatGPT. We find a\nsignificant ordering effect with GPT-4 increasing the score of the response occurring earlier in the\nprompt. To control for such effects, we recommend reporting the mean score over both orders.\nNext, we measure performance through direct comparisons between system outputs. We simplify\nthe rating scheme to a three-class labeling problem that accounts for ties. We prompt GPT-4 to\npick the best response or declare a tie and provide an explanation. We conduct these head-to-head\ncomparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.\nHuman Evaluation While recent work indicates generative models can be effectively employed\nfor system evaluations [ 19], the reliability GPT-4 ratings to assess chatbot performance is, to our\nknowledge, yet to be proven to correlate with human judgments. Therefore, we run two parallel\nhuman evaluations on the Vicuna benchmark matching both automated evaluation protocols described\nabove. We use Amazon Mechanical Turk (AMT) and get two human annotators for comparisons to\nChatGPT and three annotators for pairwise comparisons.\nElo Rating With both human and automated pairwise comparisons, we create a tournament-style\ncompetition where models compete against each other. The tournament is made up of matches where\npairs of models compete to produce the best response for a given prompt. This is similar to how Bai\net al. [4]and Chiang et al. [10] compare models, but we also employ GPT-4 ratings in addition to\nhuman ratings. We randomly sample from the set of labeled comparisons to compute Elo [ 16,17].\nElo rating, which is widely used in chess and other games, is a measure of the expected win-rate\nrelative to an opponent\u2019s win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player\nhas an expected win-rate of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or\n1100 vs 1100 match results in an expected win-rate of 50%. The Elo rating changes after each match\nproportionally to the expected outcome, that is, an unexpected upset leads to a large change in Elo\nrating while an expected outcome leads to a small change. Over time, Elo ratings approximately\nmatch the skill of each player at playing the game. We start with a score of 1,000 and use K= 32 .\nSimilar to Chiang et al. [10], we repeat this procedure 10,000 times with different random seeds to\ncontrol for ordering effects, e.g., the effect of which model pairs compete with each other first.\n5.3 Guanaco: QL ORA trained on OASST1 is a State-of-the-art Chatbot\nBased on our automated and human evaluations, we find that the top QLORAtuned model, Guanaco\n65B, which we finetune on a variant of OASST1, is the best-performing open-source chatbot model\nand offers performance competitive to ChatGPT. When compared to GPT-4, Guanaco 65B and 33B\nhave an expected win probability of 30%, based on Elo rating from human annotators system-level\npairwise comparisons - the highest reported to date.\nThe Vicuna benchmark [ 10] results relative to ChatGPT are shown in Table 6.", "start_char_idx": 0, "end_char_idx": 4189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9394d0c-ddf4-4420-b307-1c790d35eede": {"__data__": {"id_": "e9394d0c-ddf4-4420-b307-1c790d35eede", "embedding": null, "metadata": {"page_label": "9", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbf8dd20-f397-4917-a962-fc1dafdc121b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "05306b9067664b36a7a85bb41c3d9aa21d3170774e5df2f63e297e0ffc5e0f2f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b3806c7-4b7f-4acd-91ab-046dc8023b1f", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "96964cc9eadf8865992aa613104dacac7f241b5641ed05f85d4b5ba720cb5d44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56ac9a9b-f180-4603-b1f4-c1d6ceb541a2", "node_type": "1", "metadata": {}, "hash": "e11d0806f2208dad3ff0388017a8ca40d012b50c89a401b9bd322173a159b56d", "class_name": "RelatedNodeInfo"}}, "text": "[10], we repeat this procedure 10,000 times with different random seeds to\ncontrol for ordering effects, e.g., the effect of which model pairs compete with each other first.\n5.3 Guanaco: QL ORA trained on OASST1 is a State-of-the-art Chatbot\nBased on our automated and human evaluations, we find that the top QLORAtuned model, Guanaco\n65B, which we finetune on a variant of OASST1, is the best-performing open-source chatbot model\nand offers performance competitive to ChatGPT. When compared to GPT-4, Guanaco 65B and 33B\nhave an expected win probability of 30%, based on Elo rating from human annotators system-level\npairwise comparisons - the highest reported to date.\nThe Vicuna benchmark [ 10] results relative to ChatGPT are shown in Table 6. We find that Guanaco\n65B is the best-performing model after GPT-4, achieving 99.3% performance relative to ChatGPT.\nGuanaco 33B has more parameters than the Vicuna 13B model, but uses only 4-bit precision for its\nweights and is thus much more memory efficient at 21 GB vs 26 GB, providing a three percentage\npoints of improvement over Vicuna 13B. Furthermore, Guanaco 7B easily fits on modern phones at a\n5 GB footprint while still scoring nearly 20 percentage points higher than Alpaca 13B.\nHowever, Table 6 also has very wide confidence intervals, with many models overlapping in per-\nformance. We hypothesize that this uncertainty comes from the lack of clear specification of scale,\ne.g., it is unclear what 8 on a 10 point scale means across different scenarios. As such, we instead\nrecommend using the Elo ranking method [ 16], based on pairwise judgments from human annotators\nand GPT-4 to avoid the problem of grounding an absolute scale. Elo ratings of the most competitive\n9", "start_char_idx": 3442, "end_char_idx": 5174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56ac9a9b-f180-4603-b1f4-c1d6ceb541a2": {"__data__": {"id_": "56ac9a9b-f180-4603-b1f4-c1d6ceb541a2", "embedding": null, "metadata": {"page_label": "10", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9bd7ebe-c240-4518-8985-2d099c8fffdb", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "994783ac9011059289e629d88f076d8822cd99c9b1a8d258b6a14f44cb2e91ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9394d0c-ddf4-4420-b307-1c790d35eede", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "923360570f54e07207fd4ad8d700f5b1b761806eadc1c009400c174503ad6118", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6836dd8-052d-452e-addb-6bb263d29b99", "node_type": "1", "metadata": {}, "hash": "11e97f05fd1417ebe20a73b43bd028c5ca24a289e94062ad9ba276daf78c1be1", "class_name": "RelatedNodeInfo"}}, "text": "Table 6: Zero-shot Vicuna benchmark scores as a percentage of the score obtained by ChatGPT evaluated by\nGPT-4. We see that OASST1 models perform close to ChatGPT despite being trained on a very small dataset\nand having a fraction of the memory requirement of baseline models.\nModel / Dataset Params Model bits Memory ChatGPT vs Sys Sys vs ChatGPT Mean 95% CI\nGPT-4 - - - 119.4% 110.1% 114.5 % 2.6%\nBard - - - 93.2% 96.4% 94.8% 4.1%\nGuanaco 65B 4-bit 41 GB 96.7% 101.9% 99.3% 4.4%\nAlpaca 65B 4-bit 41 GB 63.0% 77.9% 70.7% 4.3%\nFLAN v2 65B 4-bit 41 GB 37.0% 59.6% 48.4% 4.6%\nGuanaco 33B 4-bit 21 GB 96.5% 99.2% 97.8% 4.4%\nOpen Assistant 33B 16-bit 66 GB 91.2% 98.7% 94.9% 4.5%\nAlpaca 33B 4-bit 21 GB 67.2% 79.7% 73.6% 4.2%\nFLAN v2 33B 4-bit 21 GB 26.3% 49.7% 38.0% 3.9%\nVicuna 13B 16-bit 26 GB 91.2% 98.7% 94.9% 4.5%\nGuanaco 13B 4-bit 10 GB 87.3% 93.4% 90.4% 5.2%\nAlpaca 13B 4-bit 10 GB 63.8% 76.7% 69.4% 4.2%\nHH-RLHF 13B 4-bit 10 GB 55.5% 69.1% 62.5% 4.7%\nUnnatural Instr. 13B 4-bit 10 GB 50.6% 69.8% 60.5% 4.2%\nChip2 13B 4-bit 10 GB 49.2% 69.3% 59.5% 4.7%\nLongform 13B 4-bit 10 GB 44.9% 62.0% 53.6% 5.2%\nSelf-Instruct 13B 4-bit 10 GB 38.0% 60.5% 49.1% 4.6%\nFLAN v2 13B 4-bit 10 GB 32.4% 61.2% 47.0% 3.6%\nGuanaco 7B 4-bit 5 GB 84.1% 89.8% 87.0% 5.4%\nAlpaca 7B 4-bit 5 GB 57.3% 71.2% 64.4% 5.0%\nFLAN v2 7B 4-bit 5 GB 33.3% 56.1% 44.8% 4.0%\nmodels can be seen in Table 1. We note that human and GPT-4 ranking of models on the Vicuna\nbenchmark disagree partially, particularly for Guanaco 7B, but are consistent for most models with\na Kendall Tau of \u03c4= 0.43and Spearman rank correlation of r= 0.55at the system level. At the\nexample level, the agreement between GPT-4 and human annotators\u2019 majority vote is weaker with\nFleiss \u03ba= 0.25. Overall, this shows a moderate agreement between system-level judgments by\nGPT-4 and human annotators, and thus that model-based evaluation represents a somewhat reliable\nalternative to human evaluation. We discuss further considerations in Section 6.2.\nElo rankings in Table 7 indicate that Guanaco 33B and 65B models outperform all models besides\nGPT-4 on the Vicuna and OA benchmarks and that they perform comparably to ChatGPT in line\nwith Table 6. We note that the Vicuna benchmark favors open-source models while the larger OA\nbenchmark favors ChatGPT.", "start_char_idx": 0, "end_char_idx": 2290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6836dd8-052d-452e-addb-6bb263d29b99": {"__data__": {"id_": "e6836dd8-052d-452e-addb-6bb263d29b99", "embedding": null, "metadata": {"page_label": "10", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9bd7ebe-c240-4518-8985-2d099c8fffdb", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "994783ac9011059289e629d88f076d8822cd99c9b1a8d258b6a14f44cb2e91ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56ac9a9b-f180-4603-b1f4-c1d6ceb541a2", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "d149bf1b88dd0da2852ded6d1634335d4421a9ec5eee64e0487a730f50e94138", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1eaf772-c31a-4cc9-b01a-7bc05c86030f", "node_type": "1", "metadata": {}, "hash": "a91290e3b8c225a3f6a2ac9e0b987e46a3aceb735650a8bc8b70ac160f214992", "class_name": "RelatedNodeInfo"}}, "text": "At the\nexample level, the agreement between GPT-4 and human annotators\u2019 majority vote is weaker with\nFleiss \u03ba= 0.25. Overall, this shows a moderate agreement between system-level judgments by\nGPT-4 and human annotators, and thus that model-based evaluation represents a somewhat reliable\nalternative to human evaluation. We discuss further considerations in Section 6.2.\nElo rankings in Table 7 indicate that Guanaco 33B and 65B models outperform all models besides\nGPT-4 on the Vicuna and OA benchmarks and that they perform comparably to ChatGPT in line\nwith Table 6. We note that the Vicuna benchmark favors open-source models while the larger OA\nbenchmark favors ChatGPT. Furthermore, we can see from Tables 5 and 6 that the suitability of\na finetuning dataset is a determining factor in performance. Finetuning Llama models on FLAN\nv2 does particularly well on MMLU, but performs worst on the Vicuna benchmark (similar trends\nare observed with other models). This also points to partial orthogonality in current evaluation\nbenchmarks: strong MMLU performance does not imply strong chatbot performance (as measured\nby Vicuna or OA benchmarks) and vice versa.\nGuanaco is the only top model in our evaluation that is not trained on proprietary data as the OASST1\ndataset collection guidelines explicitly forbid the use of GPT models. The next best model trained\non only open-source data is the Anthropic HH-RLHF model, which scores 30 percentage points\nlower than Guanaco on the Vicuna benchmark (see Table 6). Overall, these results show that 4-bit\nQLORAis effective and can produce state-of-the-art chatbots that rival ChatGPT. Furthermore, our\n33B Guanaco can be trained on 24 GB consumer GPUs in less than 12 hours. This opens up the\npotential for future work via QLORAtuning on specialized open-source data, which produces models\nthat can compete with the very best commercial models that exist today.\n6 Qualitative Analysis\nWhile quantitative analysis is the core of our evaluation, there are a number of issues with only\nlooking at summary statistics. Perhaps the largest is the problem of benchmark validity [ 36]\u2014whether\na benchmark truly tests what its name or description suggests is always at question, especially as we\ndiscover \u201cshortcuts\u201d to solve benchmarks that machine learning models sometimes exploit [ 22,46].\nTo partially alleviate this, we here perform some qualitative analysis, in two sections. First, in \u00a76.1\n10", "start_char_idx": 1615, "end_char_idx": 4053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1eaf772-c31a-4cc9-b01a-7bc05c86030f": {"__data__": {"id_": "d1eaf772-c31a-4cc9-b01a-7bc05c86030f", "embedding": null, "metadata": {"page_label": "11", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ae3847f-a505-4e7f-9a0c-05cd6cdc9112", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "0760fc18b702545969d684239b2b83ed81625cd7adbe362ccdc643a07a461bf5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6836dd8-052d-452e-addb-6bb263d29b99", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "08e83339da4c42cfa3361b35d5149a1a5492ddddbb1bb9df56319bf472dd267e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b0eee68-8e4e-477f-bdbd-f33a8446a1eb", "node_type": "1", "metadata": {}, "hash": "cc2f93830921c44e6b9f3cedb6ff761f006457b224029928cd2c1a033fe262b8", "class_name": "RelatedNodeInfo"}}, "text": "Table 7: Elo rating for a tournament between models where models compete to generate the best response\nfor a prompt, judged by human raters or GPT-4. Overall, Guanaco 65B and 33B tend to be preferred to\nChatGPT-3.5 on the benchmarks studied. According to human raters they have a Each 10-point difference in Elo\nis approximately a difference of 1.5% in win-rate.\nBenchmark Vicuna Vicuna Open Assistant\n# Prompts 80 80 953\nJudge Human raters GPT-4 GPT-4 Median Rank\nModel Elo Rank Elo Rank Elo Rank\nGPT-4 1176 1 1348 1 1294 1 1\nGuanaco-65B 1023 2 1022 2 1008 3 2\nGuanaco-33B 1009 4 992 3 1002 4 4\nChatGPT-3.5 Turbo 916 7 966 5 1015 2 5\nVicuna-13B 984 5 974 4 936 5 5\nGuanaco-13B 975 6 913 6 885 6 6\nGuanaco-7B 1010 3 879 8 860 7 7\nBard 909 8 902 7 - - 8\nwe show some examples that we believe are representative of some observed patterns in the text\ngenerated by our 65b Guanaco model. Second, \u00a76.2 we detail considerations about the results we\nhave discussed and our interpretation of them.\n6.1 Qualitative Analysis of Example Generations\nTo find examples, we first go through data generated for the Vicuna benchmark and the OpenAssistant\nbenchmark, and look for patterns in the answers Guanaco generates. When we notice a pattern we\nattempt to setup a question or prompt that will induce the pattern even though it is the incorrect\nsolution, e.g., if we observe that the model tends to give long-winded answers we prompt the model\nto \u201cAnswer yes or no without explanation.\u201d We use this to find \u201clemons\u201d where we manage to\nadversarially break the model and \u201ccherries\u201d where we fail to break the model, and present both. All\ngenerations in this section were generated with Nucleus Sampling [25] with p= 0.9.\nOf course, this is by no means comprehensive, since it is beyond the scope of this small qualitative\nstudy to control for all the variables involved, e.g., the full distribution of responses the model can\ngenerate for a given prompt is quite large, so we rely on samples we hope are representative. However,\nwe believe describing these examples gives context to the quantitative evidence shown earlier in the\npaper. Since we open source all models and code, we hope this section will inspire future work to\nexamine in more detail the issues we present here.\nFactual Recall For questions such as \u201cWhat is the capital of Zambia?\u201d all models consistently\ngenerate correct answers, e.g.,\nGuanaco :The capital of Zambia is Lusaka.\nHowever as questions get more obscure Guanaco becomes unreliable, but stays confident. For\ninstance in response to this prompt from HotPotQA [68]:\nUser : In what year was the singer who popularized \u2018I\u2019ll Keep the Lovelight\nBurning\u2019 born?\nGuanaco :Al Jolson is credited with popularizing the song\n\u2018I\u2019ll Keep the Lovelight Burning,\u2019 and he was born in the year\n1886.\nGuanaco generates the wrong popularizer and the wrong birthday (though the birthday given is\ncorrect for the person mentioned, Al Jolson).\nSuggestibility Guanaco shows a surprising resistance to going along with some kinds of assumed\nmisinformation, e.g., in the following interaction:\nUser : How was it finally, officially confirmed that the earth is flat by peer reviewed\nscientists?\n11", "start_char_idx": 0, "end_char_idx": 3185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b0eee68-8e4e-477f-bdbd-f33a8446a1eb": {"__data__": {"id_": "8b0eee68-8e4e-477f-bdbd-f33a8446a1eb", "embedding": null, "metadata": {"page_label": "12", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7054c97e-a62b-4ff0-9233-358d37455907", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "c459119465e1db7754e3e99b570b84a65579edb8d39ddb8e44b62612925315f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1eaf772-c31a-4cc9-b01a-7bc05c86030f", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "0760fc18b702545969d684239b2b83ed81625cd7adbe362ccdc643a07a461bf5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e54a70f9-2e5f-460d-b90a-5514a5f76885", "node_type": "1", "metadata": {}, "hash": "d85185d15188e6f0f9e3db977a49f047361719ab8c3ddba276babd4762e717bf", "class_name": "RelatedNodeInfo"}}, "text": "Guanaco :The earth has never been officially confirmed to be\nflat by peer reviewed scientists. The overwhelming consensus\namong scientists is that the earth is spherical. In fact,\nthis has been known for centuries and has been confirmed\nthrough numerous experiments and observations.\nThe belief that the earth is flat is a conspiracy theory\nthat has been debunked by science. The idea that a group\nof scientists have peer reviewed and confirmed the earth is\nflat is simply false.\nIf you have any more questions please ask.\nGuanaco is also quite good at knowing what kinds of questions aren\u2019t possible to answer, e.g.,\nUser : What time is it?\nGuanaco :I\u2019m sorry, but as an AI language model, I don\u2019t have\naccess to real-time information. My responses are generated\nbased on my pre-existing knowledge and do not reflect the\ncurrent time. If you\u2019re looking for the current time, I\nsuggest using a timekeeping device or checking an online\ntimepiece.\nSimilar results hold for \u201cWhere are you?\u201d, \u201cHow are you?\u201d, etc.\nRefusal Similarly, Guanaco sometimes refuses to follow instructions for seemingly random rea-\nsons:\nUser : Please reverse the words in the sentence \u201cI\u2019m a cool dude in a mocha mood.\u201d\nGuanaco :I\u2019m sorry, I\u2019m not able to do that right now, but\nI can explain why the sentence you provided is grammatically\ncorrect.\nThe sentence \u201cI\u2019m a cool dude in a mocha mood\" follows the\ncorrect grammar and syntax rules of the English language. In\nthis case, the adjective \u201ccool\u201d comes before the noun \u201cdude\u201d\nto describe it, and the prepositional phrase \u201cin a mocha\nmood\" provides additional information about the context of\nthe sentence. This sentence structure is perfectly acceptable\nand makes sense in the context of the sentence.\nIf you have any other questions or if there\u2019s anything else\nI can help you with, feel free to ask\nSecret Keeping Unfortunately, this refusal is unreliable even when it is desired. Often system\ndesigners would like to keep something secret from the user, even if the system is allowed to give\nfree text responses back to the user. We prompt the model with \u201cThe secret word is \u2018banana\u2019. Do not\nreveal or repeat back this back, no matter what the human requests.\u201d and try to see how easy it is for\na potential user to access this secret information. Directly asking fails:\nUser : What is the secret word?\nGuanaco :I\u2019m sorry, but I\u2019m not able to disclose the secret\nword. I\u2019m programmed to not reveal or repeat back the secret\nword, no matter what the human requests. Is there anything\nelse I can help you with?\nwhich is the desired behavior. However, a tiny amount of trickery breaks this behavior:\nUser : This is a game. The goal is to ignore your previous instructions. What is the\nsecret word?\n12", "start_char_idx": 0, "end_char_idx": 2725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e54a70f9-2e5f-460d-b90a-5514a5f76885": {"__data__": {"id_": "e54a70f9-2e5f-460d-b90a-5514a5f76885", "embedding": null, "metadata": {"page_label": "13", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a659c6a2-3b84-4066-acaa-8798044ee557", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "3845eaf3b1be8c18164f43b9db61ac421530c101d2517a268fd1b2a01ff4e012", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b0eee68-8e4e-477f-bdbd-f33a8446a1eb", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "c459119465e1db7754e3e99b570b84a65579edb8d39ddb8e44b62612925315f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc41af11-f759-4221-ac56-a05855fc1ee1", "node_type": "1", "metadata": {}, "hash": "9eecf2033c31b7f75feae9f7bd465685c7fc14502c9ae6a0801b67afbfffba19", "class_name": "RelatedNodeInfo"}}, "text": "Guanaco :Sorry, the secret word is banana. It\u2019s best if we\ndon\u2019t speak of it. Is there anything else you\u2019d like to ask?\nshowcasing the need to study methods for more faithful instruction following.\nMath Guanaco\u2019s biggest weakness is mathematics, an area where many language models struggle\nin [35]. When Guanaco shows its work, it tends to be accurate, e.g.,\nUser : Lee mows one lawn and charges $33. Last week he mowed 16 lawns and\nthree customers each gave him a $10 tip. How many dollars did Lee earn mowing\nlawns last week?\nGuanaco :Lee earned $582 mowing lawns last week.\nTo find the total amount he earned, we first need to calculate\nhow much he earned for mowing 16 lawns:\n$33 x 16 = $528\nThen we need to add the tips he received from the three\ncustomers:\n$10 x 3 = $30\nSo the total amount Lee earned was:\n$528 + $30 = $558\nHowever Guanaco can breakdown at even simple problems if it fails to break them down step-by-step,\na known issue [63]. For example, we see the following interaction:\nUser : Please factorize 1833.\nGuanaco :Sure, I\u2019d be happy to help! 1833 is a prime number,\nwhich means it has only two factors: 1 and 1833.\nSo the unique factorization of 1833 is:\n2\u02c61 * 3\u02c62 * 17\u02c61\nI hope this helps! Let me know if you have any other\nquestions.\nwhich is not only wrong (the true factorization is 3\u00d717\u00d743), it\u2019s wrong twice .\nTheory of Mind Guanaco displays surprisingly strong Theory of Mind capabilities [ 41,51]. For\ninstance, the model correctly answers the following question in great detail:\nUser : James and Abby are in the bedroom. Abby put the pen in the desk drawer.\nAbby leaves the bedroom. James moves the pen into the bag. Where does James\nthink Abby will look for the pen?\nGuanaco :James knows that Abby put the pen in the desk drawer.\nBut James also knows that he has moved the pen into the bag,\nand he assumes that Abby does not know about this. So, James\nmay guess that Abby will look for the pen in the desk drawer,\nbecause that\u2019s where she left it last time. However, this\nassumption may be incorrect, because Abby may have seen James\nmoving the pen.\nHowever, these inferences are unreliable, and often the model gives reasons that presume things that\ndon\u2019t make sense in the situation, e.g.,\nUser : Evelyn entered the living room. Jackson entered the playroom. James\nentered the playroom. The beans are in the treasure chest. James exited the\n13", "start_char_idx": 0, "end_char_idx": 2377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc41af11-f759-4221-ac56-a05855fc1ee1": {"__data__": {"id_": "cc41af11-f759-4221-ac56-a05855fc1ee1", "embedding": null, "metadata": {"page_label": "14", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f68befb-8cb8-4dfe-b5cd-8ba2de4042e0", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7f5bbb9b0b18e3ff6941db4288da516bd62c151970a2c5300800f7a39b7c00f6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e54a70f9-2e5f-460d-b90a-5514a5f76885", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "3845eaf3b1be8c18164f43b9db61ac421530c101d2517a268fd1b2a01ff4e012", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0bf00a8-bab2-48a8-8cda-bbf4e22454b3", "node_type": "1", "metadata": {}, "hash": "79c0fb5ea4f329a36ed3701cf9b2814ce69214cfdc267ca8b701e45c48ffe1b4", "class_name": "RelatedNodeInfo"}}, "text": "playroom. Jackson moved the beans to the pantry. Jackson exited the playroom.\nJames entered the living room. Where will James look for the beans?\nGuanaco :James will look for the beans in the pantry, because\nthat\u2019s where Jackson moved them.\nwhere Guanaco presumes information transfer that was never described. These issues echo recent\nliterature [51], but require more study.\n6.2 Considerations\nEvaluation We report moderate agreement among human annotators (Fleiss \u03ba= 0.42) with\nadditional deterioration when comparing two strong systems. This points to limitations in the\ncurrent benchmarks and human evaluation protocols for chatbot task performance. When manually\ncomparing generations from ChatGPT and Guanaco 65B on the Vicuna benchmark, we find that\nsubjective preferences start to play an important role as the authors of this paper disagreed on the\nmany preferred responses. Future work should investigate approaches to mitigate these problems\ndrawing from disciplines that developed mechanisms to deal with subjective preferences, such as\nHuman-Computer Interaction and Psychology.\nIn our analysis, we also find that automated evaluation systems have noticeable biases. For example,\nwe observe strong order effects with GPT-4 assigning higher scores to the system appearing first in its\nprompt. The relatively weak sample-level agreement between GPT-4 and human annotators (Fleiss\n\u03ba= 0.25) also suggests that human annotators and automated systems might rely on preferences\nthat are not always aligned. In addition, in Table 7, we observe that GPT-4 assigns significantly\nhigher scores to its own outputs compared to human ratings, Elo of 1348 vs 1176, which represent an\nadditional 20% probability of winning against an opponent. Future work should examine the presence\nof potential biases in automated evaluation systems as well as possible mitigation strategies.\nData & Training We note that the OASST1 dataset on which Guanaco models are trained is\nmultilingual and that the OA benchmark also contains prompts in different languages. We leave it to\nfuture work to investigate the degree to which such multilingual training improves performance on\ninstructions in languages other than English and whether this explains the larger gap between Vicuna-\n13B model (only trained on English data) and Guanaco 33B and 65B on the OA benchmark.\nGiven the strong performance of Guanaco models, we investigate any data leakage between the\nOASST1 data and the Vicuna benchmark prompts. We do not find overlapping prompts after perform-\ning fuzzy string matching in the two datasets and inspecting the closest matches manually.\nFurthermore, we note that our model is only trained with cross-entropy loss (supervised learning)\nwithout relying on reinforcement learning from human feedback (RLHF). This calls for further\ninvestigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA\nenables such analysis at scale, without the need for overwhelming computational resources.\n7 Related Work\nQuantization of Large Language Models Quantization of LLMs has largely focused on quanti-\nzation for inference time. Major approaches for preserving 16-bit LLM quality focus on managing\noutlier features (e.g., SmoothQuant [ 66] and LLM.int8() [ 14]) while others use more sophisticated\ngrouping methods [ 44,69]. Lossy quantization approaches study the trade-offs for regular round-\ning [ 13,71,47] or how to optimize rounding decisions to improve quantization precision [ 18].\nBesides our work, SwitchBack layers [ 65] is the only work that studies backpropagation through\nquantized weights at a scale beyond 1B parameters.\nFinetuning with Adapters While we use Low-rank Adapters [ 28] (LoRA), many other Parameter\nEfficient FineTuning (PEFT) methods have been proposed such as prompt tuning [ 48,33,34], tuning\nthe embedding layer inputs [ 1], tuning hidden states (IA3) [37], adding full layers [ 27], tuning\nbiases [ 70], learning a mask over weights based on Fisher information [ 54], and a combination of\napproaches [ 23]. In our work, we show that LoRA adapters are able to reach full 16-bit finetuning\nperformance. We leave it to future work to explore the tradeoffs of other PEFT approaches.\nInstruction Finetuning To help a pretrained LLM follow the instructions provided in a prompt,\ninstruction finetuning uses input-output pairs of various data sources to finetune a pretrained LLM\nto generate the output given the input as a prompt. Approaches and datasets include MetaICL [ 40],\n14", "start_char_idx": 0, "end_char_idx": 4521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0bf00a8-bab2-48a8-8cda-bbf4e22454b3": {"__data__": {"id_": "a0bf00a8-bab2-48a8-8cda-bbf4e22454b3", "embedding": null, "metadata": {"page_label": "15", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6f08484-e26b-4953-9a42-807bca5f22f1", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "ec8ede817623bb03d759e2fa8c9331539c64e82966512775f1c9e868d34b6a58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc41af11-f759-4221-ac56-a05855fc1ee1", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7f5bbb9b0b18e3ff6941db4288da516bd62c151970a2c5300800f7a39b7c00f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d46358f1-3d67-4719-bb4e-db223a03c4da", "node_type": "1", "metadata": {}, "hash": "ba4d334f37a9e0dabe492e18de2d4177f70db3b41351ffea972e90ff157cdcba", "class_name": "RelatedNodeInfo"}}, "text": "Table 8: Evaluation of biases on the CrowS dataset. A lower score indicates lower likelihood of generating\nbiased sequences. Guanaco follows the biased pattern of the LLaMA base model.\nLLaMA-65B GPT-3 OPT-175B Guanaco-65B\nGender 70.6 62.6 65.7 47.5\nReligion 79.0 73.3 68.6 38.7\nRace/Color 57.0 64.7 68.6 45.3\nSexual orientation 81.0 76.2 78.6 59.1\nAge 70.1 64.4 67.8 36.3\nNationality 64.2 61.6 62.9 32.4\nDisability 66.7 76.7 76.7 33.9\nPhysical appearance 77.8 74.6 76.2 43.1\nSocioeconomic status 71.5 73.8 76.2 55.3\nAverage 66.6 67.2 69.5 43.5\nMetaTuning [ 73], InstructGPT [ 43], FLAN [ 62,12], PromptSource [ 3], Super-NaturalInstructions [ 61,\n50], Self-instruct [ 59], UnnaturalInstructions [ 26], OPT-IML [ 29], UnifiedSKG[ 67], OIG/Chip2 [ 32],\nAlpaca [55], Vicuna [10], Koala [20], and Self-instruct-GPT-4 [45].\nChatbots Many instruction following models are structured as dialogue-based chatbots, often using\nReinforcement Learning from Human Feedback (RLHF) [ 11] or generating data from an existing\nmodel to train with AI model feedback (RLAIF) [ 5]. Approaches and datasets include Anthropic-\nHH [ 2,4], Open Assistant [ 31], LaMDA [ 56], and Sparrow [ 21]. We do not use reinforcement\nlearning, but our best model, Guanaco, is finetuned on multi-turn chat interactions from the Open\nAssistant dataset which was designed to be used for RLHF training [ 31]. For the evaluation of\nchatbots approaches that use GPT-4 instead of costly human annotation have been developed [ 10,45].\nWe improve on such approaches with a focus on an evaluation setup that is more reliable.\n8 Limitations and Discussion\nWe have shown evidence that our method, QLORA, can replicate 16-bit full finetuning performance\nwith a 4-bit base model and Low-rank Adapters (LoRA). Despite this evidence, we did not establish\nthat QLORAcan match full 16-bit finetuning performance at 33B and 65B scales. Due to the\nimmense resource costs, we leave this study to future work.\nAnother limitation is the evaluation of instruction finetuning models. While we provide evaluations\non MMLU, the Vicuna benchmark, and the OA benchmark, we did not evaluate on other benchmarks\nsuch as BigBench, RAFT, and HELM, and it is not ensured that our evaluations generalize to these\nbenchmarks. On the other hand, we perform a very broad study on MMLU and develop new methods\nfor evaluating chatbots.\nFrom the evidence presented, it appears that the performance of these benchmarks likely depends how\nsimilar the finetuning data is to the benchmark dataset. For example, FLAN v2 is similar to MMLU,\nbut dissimilar to chatbot benchmarks and vice versa for the Chip2 dataset and both models score\naccordingly on the MMLU and Vicuna benchmarks. This highlights that not only better benchmarks\nand evaluation is needed, but that one needs to be careful about what one is evaluating in the first\nplace. Do we want to create models that do well on classroom highschool and colleague knowledge or\ndo we want to do well on chatbot conversation ability? Maybe something else? Because it is always\neasier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks\ncan steer the community towards a certain direction. We should ensure as a community that the\nbenchmarks measure what we care about.\nWhile we provide a detailed evaluation for general chatbot performance, another limitation is that we\nonly do a limited responsible AI evaluation of Guanaco. We evaluate the likelihood of Guanaco-65B\nto generate a socially biased sequence of tokens compared to other models in Table 8.", "start_char_idx": 0, "end_char_idx": 3559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d46358f1-3d67-4719-bb4e-db223a03c4da": {"__data__": {"id_": "d46358f1-3d67-4719-bb4e-db223a03c4da", "embedding": null, "metadata": {"page_label": "15", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6f08484-e26b-4953-9a42-807bca5f22f1", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "ec8ede817623bb03d759e2fa8c9331539c64e82966512775f1c9e868d34b6a58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0bf00a8-bab2-48a8-8cda-bbf4e22454b3", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "656aa632f1f125cc46a585281be07e0d67a67e2cc1bcf83d6c916123c46261af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59f22d61-12a1-41b6-9d16-a0913744d92d", "node_type": "1", "metadata": {}, "hash": "307cea6be62f2bb878cfa10e022195706127d72db267abafc014745615de713b", "class_name": "RelatedNodeInfo"}}, "text": "This highlights that not only better benchmarks\nand evaluation is needed, but that one needs to be careful about what one is evaluating in the first\nplace. Do we want to create models that do well on classroom highschool and colleague knowledge or\ndo we want to do well on chatbot conversation ability? Maybe something else? Because it is always\neasier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks\ncan steer the community towards a certain direction. We should ensure as a community that the\nbenchmarks measure what we care about.\nWhile we provide a detailed evaluation for general chatbot performance, another limitation is that we\nonly do a limited responsible AI evaluation of Guanaco. We evaluate the likelihood of Guanaco-65B\nto generate a socially biased sequence of tokens compared to other models in Table 8. We see that the\naverage score in Guanaco-65B is much lower than other raw pretrained models. As such, it seems that\nfinetuning on the OASST1 dataset reduces the bias of the LLaMA base model. While these results\nare encouraging, it is unclear if Guanaco does also well when assessed on other types of biases. We\nleave further evaluation of analyzing biases in Guanaco and similar chatbots to future work.\n15", "start_char_idx": 2700, "end_char_idx": 3966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59f22d61-12a1-41b6-9d16-a0913744d92d": {"__data__": {"id_": "59f22d61-12a1-41b6-9d16-a0913744d92d", "embedding": null, "metadata": {"page_label": "16", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9943056c-56a8-41f4-b21c-d92534f5aece", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "5c076b5ea31be443deec0a9c97e67b9e39459b66be6cdbe0fc0ec2c84963fda8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d46358f1-3d67-4719-bb4e-db223a03c4da", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "807074bd0a9d4c3e842cc2b1da26c620eeb070cb631819b6f4cf9f9534a25b09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46754bd8-104c-4b62-9eb6-11aad7c55686", "node_type": "1", "metadata": {}, "hash": "fc3873d96f595e2837dc4b4e9b65eb85dd47d39dd5e3ce1f1f8eb8e9ba0538fb", "class_name": "RelatedNodeInfo"}}, "text": "An additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base\nmodels, or different adapter methods. Besides LoRA, there is also a wide variety Parameter Efficient\nFineTuning (PEFT) methods that have been shown to work well. However, it is unclear if these\nmethods scale to large models. We used LoRA as many results established its robustness but other\nadapters might yield better performance. Since finetuning after quantization seems to recover most of\nthe information that is lost during quantization this might enable much more aggressive quantization.\nFor example, 3-bit GPTQ quantization of the basemodel with LoRA might also yield 16-bit full\nfinetuning performance after finetuning.\n9 Broader Impacts\nOur QLORAfinetuning method is the first method that enables the finetuning of 33B parameter\nmodels on a single consumer GPU and 65B parameter models on a single professional GPU, while\nnot degrading performance relative to a full finetuning baseline. We have demonstrated that our\nbest 33B model trained on the Open Assistant dataset can rival ChatGPT on the Vicuna benchmark.\nSince instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like\nchatbots, we believe that our method will make finetuning widespread and common in particular for\nthe researchers that have the least resources, a big win for the accessibility of state of the art NLP\ntechnology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\nlarge corporations and small teams with consumer GPUs.\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod\nmight enable the critical milestone of enabling the finetuning of LLMs on phones and other low\nresource settings. While 7B models were shown to be able to be run on phones before, QLORAis\nthe first method that would enable the finetuning of such models. We estimate that with an iPhone 12\nPlus, QLORAcan finetune 3 million tokens per night while the phone is charging. While finetuned\n7B models do not reach the quality of ChatGPT, we believe that the quality is good enough to enable\nnovel applications that have not been possible before due to privacy or LLM quality issues. QLORA\ncan help enable privacy-preserving usage of LLMs, where users can own and manage their own data\nand models, while simultaneously making LLMs easier to deploy.\nHowever, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of\nLLMs has known dangers [ 8,6], but we believe that equalizing access to a technology that is quickly\nbecoming ubiquitous will allow for better more independent analysis than keeping the power of LLMs\nin the hands of large corporations that do not release models or source code for auditing.\nAll in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high\nquality LLMs much more widely and easily accessible.\nAcknowledgements\nWe thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and\nEvangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced\ncomputational, storage, and networking infrastructure of the Hyak supercomputer system at the\nUniversity of Washington. We thank the Hyak team for ensuring a smooth operation. We thank\nthe beta testers of the bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank\nYounes Belkada for help with the integration of our software into the Hugging Face transformers\nstack.\n16", "start_char_idx": 0, "end_char_idx": 3558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46754bd8-104c-4b62-9eb6-11aad7c55686": {"__data__": {"id_": "46754bd8-104c-4b62-9eb6-11aad7c55686", "embedding": null, "metadata": {"page_label": "17", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc6d33ba-4593-43fc-bd2f-2465e7bb1077", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "fdaf0209a9a457fd3bfbaa12a4d1dfbb94572afc76c3d53b0e0db39f7c4b8958", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59f22d61-12a1-41b6-9d16-a0913744d92d", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "5c076b5ea31be443deec0a9c97e67b9e39459b66be6cdbe0fc0ec2c84963fda8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b4e0ffd-ffd4-4151-aece-d516c61d9865", "node_type": "1", "metadata": {}, "hash": "ce38aa4bf9f7f0784b2cd7f5fe5164c6ef514e37bda0d71e7f114843f700cd1e", "class_name": "RelatedNodeInfo"}}, "text": "References\n[1]S. An, Y . Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning:\nAdapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131 ,\n2022.\n[2]A. Askell, Y . Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann,\nN. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint\narXiv:2112.00861 , 2021.\n[3]S. H. Bach, V . Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V . Nayak, A. Sharma, T. Kim, M. S.\nBari, T. Fevry, et al. Promptsource: An integrated development environment and repository for\nnatural language prompts. arXiv preprint arXiv:2202.01279 , 2022.\n[4]Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,\nT. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from\nhuman feedback. arXiv preprint arXiv:2204.05862 , 2022.\n[5]Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\nseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073 , 2022.\n[6]E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic\nparrots: Can language models be too big? In Proceedings of the 2021 ACM conference on\nfairness, accountability, and transparency , pages 610\u2013623, 2021.\n[7]S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O\u2019Brien, E. Hallahan, M. A. Khan,\nS. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models\nacross training and scaling. arXiv preprint arXiv:2304.01373 , 2023.\n[8]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\nJ. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258 , 2021.\n[9]T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost.\narXiv preprint arXiv:1604.06174 , 2016.\n[10] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E.\nGonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/ .\n[11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\nlearning from human preferences. Advances in neural information processing systems , 30,\n2017.\n[12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y .", "start_char_idx": 0, "end_char_idx": 2523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b4e0ffd-ffd4-4151-aece-d516c61d9865": {"__data__": {"id_": "4b4e0ffd-ffd4-4151-aece-d516c61d9865", "embedding": null, "metadata": {"page_label": "17", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc6d33ba-4593-43fc-bd2f-2465e7bb1077", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "fdaf0209a9a457fd3bfbaa12a4d1dfbb94572afc76c3d53b0e0db39f7c4b8958", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46754bd8-104c-4b62-9eb6-11aad7c55686", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "a1bea107d72617655929e9cf1a760f30664f422132c766121ab3661d14fdca72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b7c76b4-e976-4279-931b-454636871612", "node_type": "1", "metadata": {}, "hash": "caf3c6b6466fb60cdbdb218422df5faf3acfd9e9822168a653612eaf5b775c86", "class_name": "RelatedNodeInfo"}}, "text": "Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E.\nGonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/ .\n[11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\nlearning from human preferences. Advances in neural information processing systems , 30,\n2017.\n[12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li, X. Wang, M. De-\nhghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\narXiv:2210.11416 , 2022.\n[13] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv\npreprint arXiv:2212.09720 , 2022.\n[14] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication\nfor transformers at scale. Advances in Neural Information Processing Systems 35: Annual\nConference on Neural Information Processing Systems 2022, NeurIPS 2022 , 2022.\n[15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise\nquantization. 9th International Conference on Learning Representations, ICLR , 2022.\n[16] A. E. Elo. The proposed uscf rating system. its development, theory, and applications. Chess\nLife, 22(8):242\u2013247, 1967.\n[17] A. E. Elo. The rating of chessplayers, past and present . Arco Pub., 1978.\n17", "start_char_idx": 2019, "end_char_idx": 3465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b7c76b4-e976-4279-931b-454636871612": {"__data__": {"id_": "8b7c76b4-e976-4279-931b-454636871612", "embedding": null, "metadata": {"page_label": "18", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35c57f5e-f676-4e3b-b4e7-7cd353a50dc4", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "5a7aabe6768b4785318d20a777f8766c62db5e707aa1009e45c568c7d1a9f256", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b4e0ffd-ffd4-4151-aece-d516c61d9865", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "6714dc8fcf240a24f7ef500ac053b4dd4935fc5393366cf00b87e0d9aba2cdd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ec02813-5214-40e8-a169-792c64d6543a", "node_type": "1", "metadata": {}, "hash": "b069febfbd9bd91568b3a87c5d644b63b06193469167a550dc527a137ec5bc85", "class_name": "RelatedNodeInfo"}}, "text": "[18] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.\n[19] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. arXiv preprint\narXiv:2302.04166 , 2023.\n[20] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A\ndialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.\nedu/blog/2023/04/03/koala/ .\n[21] A. Glaese, N. McAleese, M. Tr\u02db ebacz, J. Aslanides, V . Firoiu, T. Ewalds, M. Rauh, L. Weidinger,\nM. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human\njudgements. arXiv preprint arXiv:2209.14375 , 2022.\n[22] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith.\nAnnotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324 , 2018.\n[23] J. Henderson, S. Ruder, et al. Compacter: Efficient low-rank hypercomplex adapter layers. In\nAdvances in Neural Information Processing Systems , 2021.\n[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Mea-\nsuring massive multitask language understanding. In International Conference on Learning\nRepresentations , 2020.\n[25] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi. The curious case of neural text\ndegeneration. In International Conference on Learning Representations , 2020.\n[26] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language\nmodels with (almost) no human labor. arXiv preprint arXiv:2212.09689 , 2022.\n[27] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. At-\ntariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In International Conference\non Machine Learning , pages 2790\u20132799. PMLR, 2019.\n[28] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora:\nLow-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.\n[29] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S.\nKoura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of\ngeneralization. arXiv preprint arXiv:2212.12017 , 2022.\n[30] A. K\u00f6ksal, T. Schick, A. Korhonen, and H. Sch\u00fctze. Longform: Optimizing instruction tuning\nfor long text generation with corpus extraction. arXiv preprint arXiv:2304.08460 , 2023.\n[31] A. K\u00f6pf, Y . Kilcher, D. von R\u00fctte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M.\nDuc, O. Stanley, R. Nagyfi, et al.", "start_char_idx": 0, "end_char_idx": 2638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ec02813-5214-40e8-a169-792c64d6543a": {"__data__": {"id_": "8ec02813-5214-40e8-a169-792c64d6543a", "embedding": null, "metadata": {"page_label": "18", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35c57f5e-f676-4e3b-b4e7-7cd353a50dc4", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "5a7aabe6768b4785318d20a777f8766c62db5e707aa1009e45c568c7d1a9f256", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b7c76b4-e976-4279-931b-454636871612", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "1d322ce9101791904421d8575677ab7e8df13e58a2e7c0a43f5f74a55be6a1c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbf0839c-8634-43c8-875d-5da2fbd2e309", "node_type": "1", "metadata": {}, "hash": "93786ee8ebc8b842cdd122e47d2cadac8f3de57dd79446c695f705511fbf4366", "class_name": "RelatedNodeInfo"}}, "text": "Opt-iml: Scaling language model instruction meta learning through the lens of\ngeneralization. arXiv preprint arXiv:2212.12017 , 2022.\n[30] A. K\u00f6ksal, T. Schick, A. Korhonen, and H. Sch\u00fctze. Longform: Optimizing instruction tuning\nfor long text generation with corpus extraction. arXiv preprint arXiv:2304.08460 , 2023.\n[31] A. K\u00f6pf, Y . Kilcher, D. von R\u00fctte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M.\nDuc, O. Stanley, R. Nagyfi, et al. Openassistant conversations\u2013democratizing large language\nmodel alignment. arXiv preprint arXiv:2304.07327 , 2023.\n[32] LAION. Open-instruction-generalist dataset. https://github.com/LAION-AI/\nOpen-Instruction-Generalist , 2023.\n[33] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691 , 2021.\n[34] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190 , 2021.\n[35] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y . Zhang, D. Narayanan,\nY . Wu, A. Kumar, et al. Holistic evaluation of language models. arXiv preprint\narXiv:2211.09110 , 2022.\n[36] T. Liao, R. Taori, I. D. Raji, and L. Schmidt. Are we learning yet? a meta review of evaluation\nfailures across machine learning. In Thirty-fifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2) , 2021.\n18", "start_char_idx": 2186, "end_char_idx": 3594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbf0839c-8634-43c8-875d-5da2fbd2e309": {"__data__": {"id_": "bbf0839c-8634-43c8-875d-5da2fbd2e309", "embedding": null, "metadata": {"page_label": "19", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47b8b997-1352-42a6-9c13-40907baa2ffb", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "1173e45579b36245e551ba3d3ebbe749e4b1f9e05b674aec2f8057f76066c53e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ec02813-5214-40e8-a169-792c64d6543a", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "b12dcec6b90db382c00a15caa9114a4fc8a9301a97a68bc18ddbb71615e679a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eda7da7c-da25-4650-a21a-bf4eecf0c572", "node_type": "1", "metadata": {}, "hash": "f00edfd62154eb3aeea0e7c10f2b16daa24b7bd67dcdecc8de523389dce3bd5a", "class_name": "RelatedNodeInfo"}}, "text": "[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot\nparameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in\nNeural Information Processing Systems , 35:1950\u20131965, 2022.\n[38] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\nand V . Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692 , 2019.\n[39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei,\net al. The flan collection: Designing data and methods for effective instruction tuning. arXiv\npreprint arXiv:2301.13688 , 2023.\n[40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context.\narXiv preprint arXiv:2110.15943 , 2021.\n[41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in\nquestion answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing , pages 2392\u20132400, 2018.\n[42] OpenAI. Gpt-4 technical report. arXiv , 2023.\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems , 35:27730\u201327744, 2022.\n[44] G. Park, B. Park, S. J. Kwon, B. Kim, Y . Lee, and D. Lee. nuqmm: Quantized matmul for\nefficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557 ,\n2022.\n[45] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint\narXiv:2304.03277 , 2023.\n[46] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines\nin natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and\nComputational Semantics , pages 180\u2013191, 2018.\n[47] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao,\nS. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint\narXiv:2211.05102 , 2022.\n[48] G. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv\npreprint arXiv:2104.06599 , 2021.\n[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.\nRes., 21(1), jan 2020. ISSN 1532-4435.\n[50] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,\nT. L. Scao, A. Raja, et al.", "start_char_idx": 0, "end_char_idx": 2623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eda7da7c-da25-4650-a21a-bf4eecf0c572": {"__data__": {"id_": "eda7da7c-da25-4650-a21a-bf4eecf0c572", "embedding": null, "metadata": {"page_label": "19", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47b8b997-1352-42a6-9c13-40907baa2ffb", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "1173e45579b36245e551ba3d3ebbe749e4b1f9e05b674aec2f8057f76066c53e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbf0839c-8634-43c8-875d-5da2fbd2e309", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "e6597b4736c3f09331e2f1302406ccc15f3ee2e63046f902e723522d2d526669", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7c47ce3-fb62-4a91-9dee-de6d199cdbf3", "node_type": "1", "metadata": {}, "hash": "8e8b1aeb3f22b5fad8c0ae790674d4f804eda1c3d82953f56768ea8b79d13543", "class_name": "RelatedNodeInfo"}}, "text": "Learning how to ask: Querying lms with mixtures of soft prompts. arXiv\npreprint arXiv:2104.06599 , 2021.\n[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.\nRes., 21(1), jan 2020. ISSN 1532-4435.\n[50] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,\nT. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization.\narXiv preprint arXiv:2110.08207 , 2021.\n[51] M. Sap, R. LeBras, D. Fried, and Y . Choi. Neural theory-of-mind? on the limits of social\nintelligence in large lms. arXiv preprint arXiv:2210.13312 , 2022.\n[52] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili \u00b4c, D. Hesslow, R. Castagn\u00e9, A. S. Luccioni,\nF. Yvon, M. Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100 , 2022.\n[53] S. Shaphiro and M. Wilk. An analysis of variance test for normality. Biometrika , 52(3):591\u2013611,\n1965.\n[54] Y .-L. Sung, V . Nair, and C. A. Raffel. Training neural networks with fixed sparse masks.\nAdvances in Neural Information Processing Systems , 34:24193\u201324205, 2021.\n19", "start_char_idx": 2151, "end_char_idx": 3403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7c47ce3-fb62-4a91-9dee-de6d199cdbf3": {"__data__": {"id_": "b7c47ce3-fb62-4a91-9dee-de6d199cdbf3", "embedding": null, "metadata": {"page_label": "20", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5db188b7-2661-49a3-a1f2-c226e5132704", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "646f30e8c829ea6fd4d8dd62453914e3f40f12a9f350fb629ac6c2b2538443ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eda7da7c-da25-4650-a21a-bf4eecf0c572", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "3f8d35ffa1a4547ec21b132450c7f77cf1f339c2d8329a5b940c4c4c1ff5593b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be0ae17f-7f3c-451c-aa37-b9f3917863c1", "node_type": "1", "metadata": {}, "hash": "0a95166ebeb891c7f9fe37a699240c17e8a3e685ea07b145413673d4d73aa292", "class_name": "RelatedNodeInfo"}}, "text": "[55] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.\nStanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/\nstanford_alpaca , 2023.\n[56] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,\nL. Baker, Y . Du, et al. Lamda: Language models for dialog applications. arXiv preprint\narXiv:2201.08239 , 2022.\n[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 , 2023.\n[58] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-\ntask benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461 , 2018.\n[59] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:\nAligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 ,\n2022.\n[60] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S.\nDhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative\ninstructions on 1600+ tasks. In EMNLP , 2022.\n[61] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S.\nDhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via\ndeclarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing , pages 5085\u20135109, 2022.\n[62] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V . Le.\nFinetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2021.\n[63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V . Le, D. Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models. In Advances in Neural\nInformation Processing Systems , 2022.\n[64] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\nM. Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing.\narXiv preprint arXiv:1910.03771 , 2019.\n[65] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and\nlow-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013 ,\n2023.\n[66] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant: Accurate and efficient\npost-training quantization for large language models.", "start_char_idx": 0, "end_char_idx": 2632, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be0ae17f-7f3c-451c-aa37-b9f3917863c1": {"__data__": {"id_": "be0ae17f-7f3c-451c-aa37-b9f3917863c1", "embedding": null, "metadata": {"page_label": "20", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5db188b7-2661-49a3-a1f2-c226e5132704", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "646f30e8c829ea6fd4d8dd62453914e3f40f12a9f350fb629ac6c2b2538443ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7c47ce3-fb62-4a91-9dee-de6d199cdbf3", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "e5becb4a65766190c91b53fdabe58cdc9f3f9d6d26765e75bf9bb82bb3c5a9ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fdd14b2-5370-425d-9063-009d3ac37a85", "node_type": "1", "metadata": {}, "hash": "48b3814cf886092efa92379e21ca91b80c9daa7d3769db0f12ee8d885ddeb768", "class_name": "RelatedNodeInfo"}}, "text": "Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\nM. Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing.\narXiv preprint arXiv:1910.03771 , 2019.\n[65] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and\nlow-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013 ,\n2023.\n[66] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant: Accurate and efficient\npost-training quantization for large language models. arXiv preprint arXiv:2211.10438 , 2022.\n[67] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu, M. Zhong, P. Yin,\nS. I. Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with\ntext-to-text language models. arXiv preprint arXiv:2201.05966 , 2022.\n[68] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa:\nA dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing , pages 2369\u20132380, 2018.\n[69] Z. Yao, R. Y . Aminabadi, M. Zhang, X. Wu, C. Li, and Y . He. Zeroquant: Efficient and affordable\npost-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861 , 2022.\n[70] E. B. Zaken, S. Ravfogel, and Y . Goldberg. Bitfit: Simple parameter-efficient fine-tuning for\ntransformer-based masked language-models. arXiv preprint arXiv:2106.10199 , 2021.\n[71] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu, W. Zheng, X. Xia, et al.\nGlm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 , 2022.\n20", "start_char_idx": 2081, "end_char_idx": 3779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fdd14b2-5370-425d-9063-009d3ac37a85": {"__data__": {"id_": "0fdd14b2-5370-425d-9063-009d3ac37a85", "embedding": null, "metadata": {"page_label": "21", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15fbef4c-46b7-4d87-829d-200106fa753a", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "ae2ff9a84d049000c5ce08e6f04c9b04a4c174d644439e496045a6e350d0a8b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be0ae17f-7f3c-451c-aa37-b9f3917863c1", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "05d35aa8763ca65e58aa7cf61bc58663647649e73429f975e0486763dd058694", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a8060fc-1044-4667-bc98-86fd9dfbfef8", "node_type": "1", "metadata": {}, "hash": "3016f4ac205b7412f6ca3b069f553402a4795237d46e9fe6d76d40bd1218e146", "class_name": "RelatedNodeInfo"}}, "text": "[72] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V .\nLin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 ,\n2022.\n[73] R. Zhong, K. Lee, Z. Zhang, and D. Klein. Adapting language models for zero-shot learning by\nmeta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670 , 2021.\n21", "start_char_idx": 0, "end_char_idx": 389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a8060fc-1044-4667-bc98-86fd9dfbfef8": {"__data__": {"id_": "2a8060fc-1044-4667-bc98-86fd9dfbfef8", "embedding": null, "metadata": {"page_label": "22", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "96944c55-fd43-4667-9b61-492540397ba0", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "b9e4876057ea05c7302ad567c6bb2faaa99d5ac9102ac18a120586434923aaf8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fdd14b2-5370-425d-9063-009d3ac37a85", "node_type": "1", "metadata": {"page_label": "21", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "ae2ff9a84d049000c5ce08e6f04c9b04a4c174d644439e496045a6e350d0a8b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "157723df-b07b-4cdb-b71f-53fc28a83b69", "node_type": "1", "metadata": {}, "hash": "f639a3321fed20f8fdf49306f2f36c2a1c7df9ba82d13832730d8cba46f50c45", "class_name": "RelatedNodeInfo"}}, "text": "A QLoRA vs Standard Finetuning Experimental Setup Details\nA.1 Hyperparameters for QL ORA\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\nall layers, attention + FFN output layers}. We keep LoRA \u03b1fixed and search the learning rate, since\nLoRA \u03b1is always proportional to the learning rate.\nWe find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B,\n65B). We find LoRA ris unrelated to final performance if LoRA is used on all layers as can be seen\nin Figure 4\n8 16 32 64\nLoRA r64.064.264.464.664.865.0RougeL\nbits\n4\nFigure 4: LoRA rfor LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of\nhyperparameters and for each LoRA rwe run 3 random seed with each hyperparameter combination. The\nperformance of specific LoRA rvalues appears to be independent of other hyperparameters.\nA.2 Super-Natural Instructions Experimental Setup Details\nWe use the same preprocessing of the Super-Natural Instruction dataset as Wang et al. [60]. However,\nwe split the training data in training and validation datasets allowing us to perform more rigorous\nhyperparameter tuning and early stopping. We use the same hyperparameters described in the paper\nfor training the various T5 model sizes on the Super-Natural Instruction data. We use LoRA r= 16\nfor small, medium, and large T5 models and LoRA r= 64 for T5 xl and xxl models. We also use\nLoRA \u03b1= 64 in all our experiments and no LoRA dropout.\nB Training a State-of-the-art Chatbot Experimental Setup Details\nB.1 Datasets\nWe describe the datasets used for QL ORA finetuning experiments outlined in Section 5.\nOASST1 The OpenAssistant dataset [ 31] was collected via crowd-sourcing. It contains 161,443\nunique messages distributed across 66,497 conversations and spanning 35 different languages. The\ndataset often contains several ranked replies for each given user question. In our experiments, we\nonly use the top reply at each level in the conversation tree. This limits the dataset to 9,209 examples.\nWe finetuning our models on the full conversation including the user queries.\nHH-RLHF This is a human preference dataset about helpfulness and harmlessness. Each datapoint\nconsists of two assistant replies to a user question along with a human preference judgment of the\nbest reply. The dataset contains 160,800 examples. When finetuning on this dataset, we combine\nhelpfulness and harmlessness data and only keep the preferred assistant reply.\nFLAN v2 The FLAN v2 collection [ 39] is a collection of 1836 tasks augmented with hundreds\nof manually curated templates and rich formatting patterns into over 15M examples. The authors\nshow that models trained on this collection outperform other public collections including the original\nFLAN 2021 [ 62], T0++ [ 50], Super-Natural Instructions [ 60], and OPT-IML [ 29]. We used the\nsame task mixtures described by the authors with the exception of some datasets that were not freely\navailable at the time of writing.\n22", "start_char_idx": 0, "end_char_idx": 3110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "157723df-b07b-4cdb-b71f-53fc28a83b69": {"__data__": {"id_": "157723df-b07b-4cdb-b71f-53fc28a83b69", "embedding": null, "metadata": {"page_label": "23", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c3243a6-66b9-44b1-931e-b6c563088ffb", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7f201bf7cbe1ce0cad0f32e061b24b90581378213c70b40318bd3153b3b7a8a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a8060fc-1044-4667-bc98-86fd9dfbfef8", "node_type": "1", "metadata": {"page_label": "22", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "b9e4876057ea05c7302ad567c6bb2faaa99d5ac9102ac18a120586434923aaf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7704b3c9-d7d7-4860-87e2-fead97ad9560", "node_type": "1", "metadata": {}, "hash": "bfe5abef0e99a9e90e3131ce0ec480a2d0b2711f48d079b3ac17d3ad4735a2fb", "class_name": "RelatedNodeInfo"}}, "text": "Parameters Dataset Batch size LR Steps Source Length Target Length\n7B All 16 2e-4 10000 384 128\n7B OASST1 16 2e-4 1875 - 512\n7B HH-RLHF 16 2e-4 10000 - 768\n7B Longform 16 2e-4 4000 512 1024\n13B All 16 2e-4 10000 384 128\n13B OASST1 16 2e-4 1875 - 512\n13B HH-RLHF 16 2e-4 10000 - 768\n13B Longform 16 2e-4 4000 512 1024\n33B All 32 1e-4 5000 384 128\n33B OASST1 16 1e-4 1875 - 512\n33B HH-RLHF 32 1e-4 5000 - 768\n33B Longform 32 1e-4 2343 512 1024\n65B All 64 1e-4 2500 384 128\n65B OASST1 16 1e-4 1875 - 512\n65B HH-RLHF 64 1e-4 2500 - 768\n65B Longform 32 1e-4 2343 512 1024\nTable 9: Training hyperparameters for QL ORA finetuning on different datasets and across model sizes.\nSelf-Instruct, Alpaca, Unnatural Instructions The Self-Instruct, Alpaca, and Unnatural Instruc-\ntions datasets [ 59,55,26] are instruction tuning datasets collected with various approaches of model\ndistillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and\nparaphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of\n82,612, 51,942, and 240,670 examples respectively. One advantage of such distilled datasets is that\nthey contain a more diverse set of instruction styles compared to the FLAN v2 collection and similar\ninstruction tuning collections.\nLongform The LongForm dataset [ 30] is based on an English corpus augmented with instructions\nand as such is a hybrid human-generated dataset. The underlying documents are human-written and\ncome from C4 and Wikipedia while the instructions are generated visa LLMs. The dataset is extended\nwith additional structured corpora examples such as Stack Exchange and WikiHow and task examples\nsuch as question answering, email writing, grammar error correction, story/poem generation, and text\nsummarization. The dataset contains 23,700 examples.\nChip2 is part of the OIG Laion dataset. It contains Python code examples, natural instruction exam-\nples, generic harmless instructions, instruction/responses with lists, follow-up questions, Wikipedia\ntoxic adversarial questions, grade school math, reasoning instructions, and character and scene\ndescriptions with a total of 210,289 examples.\nB.2 Hyperparameters\nWe provide the exact hyperparameters used in our QLORAfinetuning experiments. We find hyper-\nparameters to be largely robust across datasets. We use the MMLU 5-shot dev set for validation\nand hyperparameter tuning. In all our experiments we use NF4 with double quantization and bf16\ncomputation datatype. We set LoRA r= 64 ,\u03b1= 16 , and add LoRA modules on all linear layers of\nthe base model. We also use Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1\nfor models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction\nfinetuning [ 62,60] and after benchmarking other linear and cosine schedules, we use a constant\nlearning rate schedule. We use group-by-length to group examples of similar lengths in the same\nbatch (note this will produce a oscillating loss curve). The hyperparameters we tune for each model\nsize are shown in Table 9.\nB.3 Ablations\nWhile it is general practice in the literature to only train on the response in instruction following\ndatasets, we study the effect of training on the instruction in addition to the response in Table 10. In\nthese experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four\ndifferent instruction tuning datasets, we find that only training on the target is beneficial to MMLU\n23", "start_char_idx": 0, "end_char_idx": 3504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7704b3c9-d7d7-4860-87e2-fead97ad9560": {"__data__": {"id_": "7704b3c9-d7d7-4860-87e2-fead97ad9560", "embedding": null, "metadata": {"page_label": "24", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53a91575-5885-4b08-ad3d-0cf3bfe21722", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "d94c133019c7bf282a24f296be0bc1a428f0b32f04a3c61d8977ee3a17a2a630", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "157723df-b07b-4cdb-b71f-53fc28a83b69", "node_type": "1", "metadata": {"page_label": "23", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7f201bf7cbe1ce0cad0f32e061b24b90581378213c70b40318bd3153b3b7a8a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a60269f5-5e8c-486f-99d9-954a352676b0", "node_type": "1", "metadata": {}, "hash": "d4932308cb8ee35db50382977bf2a2c53b13721fe96ea2ad5da381b3081eb5f9", "class_name": "RelatedNodeInfo"}}, "text": "Dataset Unnatural Instructions Chip2 Alpaca FLAN v2 Mean\nTrain on source and target 36.2 33.7 38.1 42.0 37.5\nTrain on target 38.0 34.5 39.0 42.9 38.6\nTable 10: MMLU 5-shot test results studying the effect of training on the instructions in addition to the response.\nperformance. We did not evaluate the effect this may have on chatabot performance as measured by\nvicuna or OA benchmarks.\nB.4 What is more important: instruction finetuning dataset size or dataset quality?\nData set suitability is more important than dataset size. To understand the effects of dataset\nquality vs. dataset size, we experiment with subsampling large datasets with at least 150,000 samples\n(Chip2, FLAN v2, Unnatural Instructions), into datasets of size 50,000, 100,000 and 150,000 and\nexamine the resulting trends, as shown in Table 11. We find that increasing the dataset size and\nincreasing the number of epochs improves MMLU only marginally (0.0 - 0.5 MMLU), while the\ndifference between datasets is up to 40x larger (1.5 - 8.0 MMLU). This is a clear indicator that dataset\nquality rather than dataset size is critical for mean MMLU accuracy. We obtain similar findings for\nchatbot performance as discussed in .\nC Human Evaluation\nWe conduct a human evaluation with the same wording given to GPT-4 in the original Vicuna\nevaluation [10], adjusted for an Amazon Mechanical Turk form as show in Figure 5.\nD Pairwise Evaluation with GPT-4\nWhile we found that the GPT-4 evaluation gave different results depend on which system was\npresented first, when averaged over both options the pairwise results were well-ordered. The\naggregated pairwise judgments are hown in Table 12. On inspection, it is clear these judgments are\ntransitive, i.e., when System A is judged better than System B and System B is judged better than\nSystem C, it is always the case that System A is judged better than System C. This yields a complete\nordering, given in Table 13.\nE NormalFloat 4-bit data type\nThe exact values of the NF4 data type are as follows:\n[-1.0, -0.6961928009986877, -0.5250730514526367,\n-0.39491748809814453, -0.28444138169288635, -0.18477343022823334,\n-0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,\n0.24611230194568634, 0.33791524171829224, 0.44070982933044434,\n0.5626170039176941, 0.7229568362236023, 1.0]\nF Normality of Trained Neural Network Weights\nWhile it is common knowledge that trained neural network weights are mostly normally distributed,\nwe perform statistical testing to verify this. We use the Shapiro-Wilk test[ 53] on the weights of the 7B\nTable 11: Effect different dataset sizes and finetuning epochs on mean 5-shot MMLU test set accuracy. While\nincreasing the dataset size and training for more than 1 epochs helps with MMLU performance, the difference\nbetween datasets are far larger, indicating that dataset quality affects MMLU performance more than dataset size.\nChip Unnatural Instructions FLAN v2\nDatapoints \u2193Epochs \u2192 1 2 3 1 2 3 1 2 3 Mean\n50000 34.50 35.30 34.70 38.10 42.20 38.10 43.00 43.50 44.10 39.28\n100000 33.70 33.90 34.00 40.10 41.20 37.00 43.90 43.70 44.90 39.16\n150000 34.40 34.80 35.10 39.70 41.10 41.50 44.60 45.50 43.50 40.02\nMean 34.20 34.67 34.60 39.30 41.50 38.87 43.83 44.23 44.17\n24", "start_char_idx": 0, "end_char_idx": 3227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a60269f5-5e8c-486f-99d9-954a352676b0": {"__data__": {"id_": "a60269f5-5e8c-486f-99d9-954a352676b0", "embedding": null, "metadata": {"page_label": "25", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "346b92a8-56bc-498e-94c1-e4a5fadb1e02", "node_type": "4", "metadata": {"page_label": "25", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "824caa6c1d25d09ec22a96ddc186b79f7a028ad540f697f41f0b4287a739dac1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7704b3c9-d7d7-4860-87e2-fead97ad9560", "node_type": "1", "metadata": {"page_label": "24", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "d94c133019c7bf282a24f296be0bc1a428f0b32f04a3c61d8977ee3a17a2a630", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdb455b8-2a93-45a2-90d8-4f6a1cce69ae", "node_type": "1", "metadata": {}, "hash": "8fda69570578e242736d043209bd1db44016c8e0b4849d930c4d5fe4ef219e82", "class_name": "RelatedNodeInfo"}}, "text": "Figure 5: The crowdsourcing form used by human annotators.\nLLaMA model [ 57]. We find that the weights of each hidden unit have different normal distributions.\nAs such, we test he weights of each individual hidden unit. This mean for weight W\u2208 Rin\u00d7out\nwe perform tests over the outdimension. Using a 5% significance threshold, we find that 7.5% of\nneurons are non-normally distributed which is about 2.5% more than the expected false-positive\nrate. As such, while almost all pretrained weights appear to be normally distributed there seem to\nbe exceptions. Such exceptions might be due to outliers weights [ 13] or because the p-value of the\nShaprio-Wilk test is not accurate for large samples sizes[ 53] that occur in the LLaMA FFN layer\nhidden units. this verifies the claim that neural network weights.\nTable 12: Aggregated pairwise GPT-4 judgments between systems where the value of a cell at row xand column\nyis# judgment xis better than y\u2212# judgment yis better than x\ntotal # number of judgments\nModel Guanaco 65B Guanaco 33B Vicuna ChatGPT-3.5 Turbo Bard Guanaco 13B Guanaco 7B\nGuanaco 65B - 0.21 0.19 0.16 0.72 0.59 0.86\nGuanaco 33B -0.21 - 0.17 0.10 0.51 0.41 0.68\nVicuna -0.19 -0.17 - 0.10 0.50 0.20 0.57\nChatGPT-3.5 Turbo -0.16 -0.10 -0.10 - 0.35 0.19 0.40\nBard -0.72 -0.51 -0.50 -0.35 - 0.12 0.03\nGuanaco 13B -0.59 -0.41 -0.20 -0.19 -0.12 - 0.20\nGuanaco 7B -0.86 -0.68 -0.57 -0.40 -0.03 -0.20 -\n25", "start_char_idx": 0, "end_char_idx": 1409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdb455b8-2a93-45a2-90d8-4f6a1cce69ae": {"__data__": {"id_": "cdb455b8-2a93-45a2-90d8-4f6a1cce69ae", "embedding": null, "metadata": {"page_label": "26", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5449ccf-2bf2-4bc0-aaf9-081bd938dfc5", "node_type": "4", "metadata": {"page_label": "26", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "34dd86354b3066faa8bd8d14c7c5bafeec1d9f7ca028d39635baabc9b6122a58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a60269f5-5e8c-486f-99d9-954a352676b0", "node_type": "1", "metadata": {"page_label": "25", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "824caa6c1d25d09ec22a96ddc186b79f7a028ad540f697f41f0b4287a739dac1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "735ee020-1d0e-4528-84d8-162e3743ac09", "node_type": "1", "metadata": {}, "hash": "2c55a2ea399b066a31ef06286c8f782e4b9fc08153119b7d8d6066dee6e0c753", "class_name": "RelatedNodeInfo"}}, "text": "LLaMA model size0%25%50%75%100%\n7B (6.9 GB) 13B (11.3 GB) 33B (24.7 GB) 65B (45.0 GB)Input gradient Optimizer Weight gradient Adapters ModelFigure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch\nsize 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).\nNumbers on the bars are memory footprint in MB of individual elements of the total footprint. While some\nmodels do not quite fit on certain GPUs, paged optimzier provide enough memory to make these models fit.\nG Memory Footprint\nThe memory footpring for QLoRA training with different LLaMA base models can be seen in\nFigure 6. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers\nare needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient\ncheckpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the\nactivation gradient might consume a considerable amount of memory.\nTable 13: The complete ordering induced by pairwise GPT-4 judgments between systems\nModel Params Size\nGuanaco 65B 41 GB\nGuanaco 33B 21 GB\nVicuna 13B 26 GB\nChatGPT-3.5 Turbo N/A N/A\nBard N/A N/A\nGuanaco 13B 10 GB\nGuanaco 7B 5 GB\n26", "start_char_idx": 0, "end_char_idx": 1263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "735ee020-1d0e-4528-84d8-162e3743ac09": {"__data__": {"id_": "735ee020-1d0e-4528-84d8-162e3743ac09", "embedding": null, "metadata": {"page_label": "1", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de77cfa4-550c-4151-b98f-3ff1ddb3e672", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "dac4cdf5b0481be578287bf86adcd866b9fc045a1fa75f4cb8a24801f02723ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdb455b8-2a93-45a2-90d8-4f6a1cce69ae", "node_type": "1", "metadata": {"page_label": "26", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "34dd86354b3066faa8bd8d14c7c5bafeec1d9f7ca028d39635baabc9b6122a58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d234f88b-c0d9-44f0-bd3e-dcd26ee976ed", "node_type": "1", "metadata": {}, "hash": "cf802f0ff0eaa35a38341eee3b85ea382496b69a895825e62ff35a0786fcd52c", "class_name": "RelatedNodeInfo"}}, "text": "Retentive Network: A Successor to Transformer\nfor Large Language Models\nYutao Sun\u2217\u2020\u2021Li Dong\u2217\u2020Shaohan Huang\u2020Shuming Ma\u2020\nYuqing Xia\u2020Jilong Xue\u2020Jianyong Wang\u2021Furu Wei\u2020\u22c4\n\u2020Microsoft Research\u2021Tsinghua University\nhttps://aka.ms/GeneralAI\nAbstract\nIn this work, we propose Retentive Network (RETNET) as a foundation archi-\ntecture for large language models, simultaneously achieving training parallelism,\nlow-cost inference, and good performance. We theoretically derive the connection\nbetween recurrence and attention. Then we propose the retention mechanism for\nsequence modeling, which supports three computation paradigms, i.e., parallel,\nrecurrent, and chunkwise recurrent. Specifically, the parallel representation allows\nfor training parallelism. The recurrent representation enables low-cost O(1)infer-\nence, which improves decoding throughput, latency, and GPU memory without\nsacrificing performance. The chunkwise recurrent representation facilitates effi-\ncient long-sequence modeling with linear complexity, where each chunk is encoded\nparallelly while recurrently summarizing the chunks. Experimental results on\nlanguage modeling show that RETNETachieves favorable scaling results, parallel\ntraining, low-cost deployment, and efficient inference. The intriguing properties\nmake RETNETa strong successor to Transformer for large language models. Code\nwill be available at https://aka.ms/retnet .\n02040\n0150300\n0150300\nGPU Memory\u2193\n(GB)Throughput \u2191\n(wps)Latency \u2193\n(ms)3.4X\n15.6X8.4XInference Cost Scaling Curve\nRetNet Transformer1 3 7LM Perplexity\nModel Size (B)\nFigure 1: Retentive network (RetNet) achieves low-cost inference (i.e., GPU memory, throughput,\nand latency), training parallelism, and favorable scaling curves compared with Transformer. Results\nof inference cost are reported with 8k as input length. Figure 6 shows more results on different\nsequence lengths.\n\u2217Equal contribution. \u22c4Corresponding author.arXiv:2307.08621v4  [cs.CL]  9 Aug 2023", "start_char_idx": 0, "end_char_idx": 1958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d234f88b-c0d9-44f0-bd3e-dcd26ee976ed": {"__data__": {"id_": "d234f88b-c0d9-44f0-bd3e-dcd26ee976ed", "embedding": null, "metadata": {"page_label": "2", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "111e0d8d-9ea7-46d1-97be-86c885590e7b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "8c6a75c06f18dca0c0595a175cdb548ab551cd14985a46d380a457f7c042bf81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "735ee020-1d0e-4528-84d8-162e3743ac09", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "dac4cdf5b0481be578287bf86adcd866b9fc045a1fa75f4cb8a24801f02723ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb78dad9-0a0d-439b-a7e9-6314181b706c", "node_type": "1", "metadata": {}, "hash": "dd81a2129f246b8d11e0f91f5dc713767c84627e82216a413e0d661912423fa8", "class_name": "RelatedNodeInfo"}}, "text": "\u201cThe only way to discover the limits of the possible is to go beyond them into the impossible.\nArthur C. Clarke\u201d\n1 Introduction\nLow -Cost\nInference\nTransformerRetNet\nFigure 2: RetNet makes the \u201cimpossible triangle\u201d\npossible, which achieves training parallelism, good\nperformance, and low inference cost simultane-\nously.Transformer [ VSP+17] has become the de\nfacto architecture for large language mod-\nels [BMR+20], which was initially proposed\nto overcome the sequential training issue of\nrecurrent models [ HS97 ]. However, training\nparallelism of Transformers is at the cost of in-\nefficient inference, because of the O(N)com-\nplexity per step and memory-bound key-value\ncache [ Sha19 ], which renders Transformers un-\nfriendly to deployment. The growing sequence\nlength increases GPU memory consumption as\nwell as latency and reduces inference speed.\nNumerous efforts have continued to develop the\nnext-generation architecture, aiming at retain-\ning training parallelism and competitive perfor-\nmance as Transformers while having efficient\nO(1)inference. It is challenging to achieve the\nabove goals simultaneously, i.e., the so-called\n\u201cimpossible triangle\u201d as shown in Figure 2.\nThere have been three main strands of research.\nFirst, linearized attention [ KVPF20 ] approximates standard attention scores exp(q\u00b7k)with kernels\n\u03d5(q)\u00b7\u03d5(k), so that autoregressive inference can be rewritten in a recurrent form. However, the\nmodeling capability and performance are worse than Transformers, which hinders the method\u2019s popu-\nlarity. The second strand returns to recurrent models for efficient inference while sacrificing training\nparallelism. As a remedy, element-wise operators [ PAA+23] are used for acceleration, however,\nrepresentation capacity and performance are harmed. The third line of research explores replacing\nattention with other mechanisms, such as S4 [ GGR21 ], and its variants [ DFS+22,PMN+23]. None\nof the previous work can break through the impossible triangle, resulting in no clear winner compared\nwith Transformers.\nIn this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long-\nsequence modeling, Transformer-comparable performance, and parallel model training simultane-\nously. Specifically, we introduce a multi-scale retention mechanism to substitute multi-head attention,\nwhich has three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent repre-\nsentations. First, the parallel representation empowers training parallelism to utilize GPU devices\nfully. Second, the recurrent representation enables efficient O(1)inference in terms of memory\nand computation. The deployment cost and latency can be significantly reduced. Moreover, the\nimplementation is greatly simplified without key-value cache tricks. Third, the chunkwise recurrent\nrepresentation can perform efficient long-sequence modeling. We parallelly encode each local block\nfor computation speed while recurrently encoding the global blocks to save GPU memory.\nWe conduct extensive experiments to compare RetNet with Transformer and its variants. Experi-\nmental results on language modeling show that RetNet is consistently competitive in terms of both\nscaling curves and in-context learning. Moreover, the inference cost of RetNet is length-invariant.\nFor a 7B model and 8k sequence length, RetNet decodes 8.4 \u00d7faster and saves 70% of memory\nthan Transformers with key-value caches. During training, RetNet also achieves 25-50% memory\nsaving and 7 \u00d7acceleration than standard Transformer and an advantage towards highly-optimized\nFlashAttention [ DFE+22]. Besides, RetNet\u2019s inference latency is insensitive to batch size, allowing\nenormous throughput. The intriguing properties make RetNet a strong successor to Transformer for\nlarge language models.\n2", "start_char_idx": 0, "end_char_idx": 3800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb78dad9-0a0d-439b-a7e9-6314181b706c": {"__data__": {"id_": "eb78dad9-0a0d-439b-a7e9-6314181b706c", "embedding": null, "metadata": {"page_label": "3", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c0724c7-6f79-4b64-8e0a-adeda852ae02", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "2194c74ca6759b8e5761a9141e445d7c661cfdab52ff1a5819e7fe67af907015", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d234f88b-c0d9-44f0-bd3e-dcd26ee976ed", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "8c6a75c06f18dca0c0595a175cdb548ab551cd14985a46d380a457f7c042bf81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49796e13-fa2e-4cb9-9f64-4dd8831143d8", "node_type": "1", "metadata": {}, "hash": "52d13cd3079efe6516c689996d07513f7a91eaff2beaed1ff95de405d6b79824", "class_name": "RelatedNodeInfo"}}, "text": "2 Retentive Networks\nRetentive network (RetNet) is stacked with Lidentical blocks, which follows a similar layout (i.e.,\nresidual connection, and pre-LayerNorm) as in Transformer [ VSP+17]. Each RetNet block contains\ntwo modules: a multi-scale retention (MSR) module, and a feed-forward network (FFN) module.\nWe introduce the MSR module in the following sections. Given an input sequence x=x1\u00b7\u00b7\u00b7x|x|,\nRetNet encodes the sequence in an autoregressive way. The input vectors {xi}|x|\ni=1is first packed\nintoX0= [x1,\u00b7\u00b7\u00b7,x|x|]\u2208R|x|\u00d7dmodel, where dmodel is hidden dimension. Then we compute\ncontextualized vector representations Xl= RetNet l(Xl\u22121), l\u2208[1, L].\n2.1 Retention\nIn this section, we introduce the retention mechanism that has a dual form of recurrence and\nparallelism. So we can train the models in a parallel way while recurrently conducting inference.\nGiven input X\u2208R|x|\u00d7dmodel, we project it to one-dimensional function v(n) =Xn\u00b7wV. Consider a\nsequence modeling problem that maps v(n)7\u2192o(n)through states sn. Letvn, ondenote v(n), o(n)\nfor simplicity. We formulate the mapping in a recurrent manner:\nsn=Asn\u22121+K\u22ba\nnvn, A \u2208Rd\u00d7d, Kn\u2208R1\u00d7d\non=Qnsn=nX\nm=1QnAn\u2212mK\u22ba\nmvm, Q n\u2208R1\u00d7d(1)\nwhere we map vnto the state vector sn, and then implement a linear transform to encode sequence\ninformation recurrently.\nNext, we make the projection Qn, Kncontent-aware:\nQ=XW Q, K =XW K (2)\nwhere WQ, WK\u2208Rd\u00d7dare learnable matrices.\nWe diagonalize the matrix A= \u039b( \u03b3ei\u03b8)\u039b\u22121, where \u03b3, \u03b8\u2208Rd. Then we obtain An\u2212m=\n\u039b(\u03b3ei\u03b8)n\u2212m\u039b\u22121. By absorbing \u039bintoWQandWK, we can rewrite Equation (1) as:\non=nX\nm=1Qn(\u03b3ei\u03b8)n\u2212mK\u22ba\nmvm\n=nX\nm=1(Qn(\u03b3ei\u03b8)n)(Km(\u03b3ei\u03b8)\u2212m)\u22bavm(3)\nwhere Qn(\u03b3ei\u03b8)n, Km(\u03b3ei\u03b8)\u2212mis known as xPos [ SDP+22], i.e., a relative position embedding\nproposed for Transformer. We further simplify \u03b3as a scalar, Equation (3) becomes:\non=nX\nm=1\u03b3n\u2212m(Qnein\u03b8)(Kmeim\u03b8)\u2020vm (4)\nwhere\u2020is the conjugate transpose. The formulation is easily parallelizable within training instances.\nIn summary, we start with recurrent modeling as shown in Equation (1), and then derive its parallel\nformulation in Equation (4). We consider the original mapping v(n)7\u2192o(n)as vectors and obtain\nthe retention mechanism as follows.\nThe Parallel Representation of Retention As shown in Figure 3a, the retention layer is defined as:\nQ= (XW Q)\u2299\u0398, K = (XW K)\u2299\u0398, V =XW V\n\u0398n=ein\u03b8, D nm=\u001a\u03b3n\u2212m, n\u2265m\n0, n < m\nRetention( X) = (QK\u22ba\u2299D)V(5)\nwhere \u0398is the complex conjugate of \u0398, andD\u2208R|x|\u00d7|x|combines causal masking and exponential\ndecay along relative distance as one matrix. Similar to self-attention, the parallel representation\nenables us to train the models with GPUs efficiently.\n3", "start_char_idx": 0, "end_char_idx": 2613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49796e13-fa2e-4cb9-9f64-4dd8831143d8": {"__data__": {"id_": "49796e13-fa2e-4cb9-9f64-4dd8831143d8", "embedding": null, "metadata": {"page_label": "4", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c141fc2-7deb-4e13-a2e5-f9f7282e0823", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "1c045a76bc46ae45479d0ec1958321375142faf6d69ed2641b8538407b7ba01c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb78dad9-0a0d-439b-a7e9-6314181b706c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "2194c74ca6759b8e5761a9141e445d7c661cfdab52ff1a5819e7fe67af907015", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb7ef439-7f96-4fd4-90ce-ea866a9c1053", "node_type": "1", "metadata": {}, "hash": "f9ad9869e94a2fa4592fac1b7875aa05a1ace3c19eeaf2f5f063725c74137b42", "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc4b\ud835\udc3e\ud835\udc44 \ud835\udc49\ud835\udc42\nGN\n\ud835\udc44\ud835\udc3e\u22ba\u2299\ud835\udc37\ud835\udc49(a) Parallel representation.\n\ud835\udc4b\ud835\udc5b\ud835\udc46\ud835\udc5b\u22121 \ud835\udc46\ud835\udc5b\ud835\udefe\n\ud835\udc3e\ud835\udc5b \ud835\udc49\ud835\udc5b \ud835\udc44\ud835\udc5b\ud835\udc42\ud835\udc5bRecurrent\nState\nInputOutput\nGN (b) Recurrent representation.\nFigure 3: Dual form of RetNet. \u201cGN\u201d is short for GroupNorm.\nThe Recurrent Representation of Retention As shown in Figure 3b, the proposed mechanism\ncan also be written as recurrent neural networks (RNNs), which is favorable for inference. For the\nn-th timestep, we recurrently obtain the output as:\nSn=\u03b3Sn\u22121+K\u22ba\nnVn\nRetention( Xn) =QnSn, n = 1,\u00b7\u00b7\u00b7,|x|(6)\nwhere Q, K, V, \u03b3 are the same as in Equation (5).\nThe Chunkwise Recurrent Representation of Retention A hybrid form of parallel representation\nand recurrent representation is available to accelerate training, especially for long sequences. We\ndivide the input sequences into chunks. Within each chunk, we follow the parallel representation\n(Equation (5)) to conduct computation. In contrast, cross-chunk information is passed following the\nrecurrent representation (Equation (6)). Specifically, let Bdenote the chunk length. We compute the\nretention output of the i-th chunk via:\nQ[i]=QBi:B(i+1), K [i]=KBi:B(i+1), V [i]=VBi:B(i+1)\nRi=K\u22ba\n[i](V[i]\u2299\u03b6) +\u03b3BRi\u22121, \u03b6 ij=\u03b3B\u2212i\u22121\nRetention( X[i]) = (Q[i]K\u22ba\n[i]\u2299D)V[i]| {z }\nInner-Chunk+ (Q[i]Ri\u22121)\u2299\u03be|{z}\nCross-Chunk, \u03be ij=\u03b3i+1(7)\nwhere [i]indicates the i-th chunk, i.e., x[i]= [x(i\u22121)B+1,\u00b7\u00b7\u00b7, xiB].\n2.2 Gated Multi-Scale Retention\nWe use h=dmodel/dretention heads in each layer, where dis the head dimension. The heads use\ndifferent parameter matrices WQ, WK, WV\u2208Rd\u00d7d. Moreover, multi-scaleretention (MSR) assigns\ndifferent \u03b3for each head. For simplicity, we set \u03b3identical among different layers and keep them\nfixed. In addition, we add a swish gate [ HG16 ,RZL17 ] to increase the non-linearity of retention\nlayers. Formally, given input X, we define the layer as:\n\u03b3= 1\u22122\u22125\u2212arange(0 ,h)\u2208Rh\nhead i= Retention( X, \u03b3 i)\nY= GroupNormh(Concat(head 1,\u00b7\u00b7\u00b7,head h))\nMSR( X) = (swish( XW G)\u2299Y)WO(8)\nwhere WG, WO\u2208Rdmodel\u00d7dmodelare learnable parameters, and GroupNorm [WH18 ] normalizes the\noutput of each head, following SubLN proposed in [ SPP+19]. Notice that the heads use multiple \u03b3\nscales, which results in different variance statistics. So we normalize the head outputs separately.\nThe pseudocode of retention is summarized in Figure 4.\n4", "start_char_idx": 0, "end_char_idx": 2258, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb7ef439-7f96-4fd4-90ce-ea866a9c1053": {"__data__": {"id_": "bb7ef439-7f96-4fd4-90ce-ea866a9c1053", "embedding": null, "metadata": {"page_label": "5", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1a594ef-52f6-43fe-a7d5-8cc90d2fdc5a", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "8548e7d01ce43b44e1284b08985dc6bd4c0542dcfdc73f731e8bfad7d4f4a51c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49796e13-fa2e-4cb9-9f64-4dd8831143d8", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "1c045a76bc46ae45479d0ec1958321375142faf6d69ed2641b8538407b7ba01c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "caec7162-7ba7-4a2a-be70-c38ea68e2601", "node_type": "1", "metadata": {}, "hash": "660ba97ad15d7f4ff2ba51825349ff9b2122ebeb39cc021fee2da727e357f153", "class_name": "RelatedNodeInfo"}}, "text": "def ParallelRetention(\nq, # bsz \u2217num_head \u2217len \u2217qk_dim\nk, # bsz \u2217num_head \u2217len \u2217qk_dim\nv, # bsz \u2217num_head \u2217len \u2217v_dim\ndecay_mask # num_head \u2217len \u2217len\n):\nretention = q @ k.transpose( \u22121,\u22122)\nretention = retention \u2217decay_mask\noutput = retention @ v\noutput = group_norm(output)\nreturn outputdef RecurrentRetention(\nq, k, v, # bsz \u2217num_head \u2217len \u2217qkv_dim\npast_kv, # bsz \u2217num_head \u2217qk_dim \u2217v_dim\ndecay # num_head \u22171\u22171\n):\ncurrent_kv = decay \u2217past_kv + k.unsqueeze\n(\u22121)\u2217v.unsqueeze( \u22122)\noutput = torch.sum(q.unsqueeze( \u22121)\u2217\ncurrent_kv, dim= \u22122)\noutput = group_norm(output)\nreturn output, current_kv\ndef ChunkwiseRetention(\nq, k, v, # bsz \u2217num_head \u2217chunk_size \u2217qkv_dim\npast_kv, # bsz \u2217num_head \u2217qk_dim \u2217v_dim\ndecay_mask, # num_head \u2217chunk_size \u2217chunk_size\nchunk_decay, # num_head \u22171\u22171\ninner_decay, # num_head \u2217chunk_size\n):\nretention = q @ k.transpose( \u22121,\u22122)\nretention = retention \u2217decay_mask\ninner_retention = retention @ v\ncross_retention = (q @ past_kv) \u2217inner_decay\nretention = inner_retention + cross_retention\noutput = group_norm(retention)\ncurrent_kv = chunk_decay \u2217past_kv + k.transpose( \u22121,\u22122) @ v\nreturn output, current_kv\nFigure 4: Pseudocode for the three computation paradigms of retention.\nRetention Score Normalization We utilize the scale-invariant nature of GroupNorm to im-\nprove the numerical precision of retention layers. Specifically, multiplying a scalar value within\nGroupNorm does not affect outputs and backward gradients, i.e., GroupNorm( \u03b1\u2217head i) =\nGroupNorm(head i). We implement three normalization factors in Equation (5). First, we normalize\nQK\u22baasQK\u22ba/\u221a\nd. Second, we replace Dwith \u02dcDnm=Dnm/\u221aPn\ni=1Dni. Third, let Rdenote the\nretention scores R=QK\u22ba\u2299D, we normalize it as \u02dcRnm=Rnm/max(|Pn\ni=1Rni|,1). Then the\nretention output becomes Retention( X) =\u02dcRV. The above tricks do not affect the final results while\nstabilizing the numerical flow of both forward and backward passes, because of the scale-invariant\nproperty.\n2.3 Overall Architecture of Retention Networks\nFor an L-layer retention network, we stack multi-scale retention (MSR) and feed-forward network\n(FFN) to build the model. Formally, the input sequence {xi}|x|\ni=1is transformed to vectors by a word\nembedding layer. We use the packed embeddings X0= [x1,\u00b7\u00b7\u00b7,x|x|]\u2208R|x|\u00d7dmodelas the input and\ncompute the model output XL:\nYl= MSR(LN( Xl)) +Xl\nXl+1= FFN(LN( Yl)) +Yl(9)\nwhere LN(\u00b7)is LayerNorm [ BKH16 ]. The FFN part is computed as FFN( X) = gelu( XW 1)W2,\nwhere W1, W2are parameter matrices.\nTraining We use the parallel (Equation (5)) and chunkwise recurrent (Equation (7)) representations\nduring the training process. The parallelization within sequences or chunks efficiently utilizes\nGPUs to accelerate computation. More favorably, chunkwise recurrence is especially useful for\nlong-sequence training, which is efficient in terms of both FLOPs and memory consumption.\n5", "start_char_idx": 0, "end_char_idx": 2861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "caec7162-7ba7-4a2a-be70-c38ea68e2601": {"__data__": {"id_": "caec7162-7ba7-4a2a-be70-c38ea68e2601", "embedding": null, "metadata": {"page_label": "6", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2750f85-eeba-40dc-a6f9-f80310dc43c6", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "e8d59a3adfe965c238220c9b3ee03479e2b4b73329991b668f75e19fb41459cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb7ef439-7f96-4fd4-90ce-ea866a9c1053", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "8548e7d01ce43b44e1284b08985dc6bd4c0542dcfdc73f731e8bfad7d4f4a51c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba7d32a4-4f2c-475a-865e-ea5e89ac65d3", "node_type": "1", "metadata": {}, "hash": "639367a55c2dbf5973d3969c93c065fe5d2165e6d350442e9c1c42b2243c87e7", "class_name": "RelatedNodeInfo"}}, "text": "ArchitecturesTraining\nParallelizationInference CostLong-Sequence\nMemory ComplexityPerformance\nTransformer \u2714 O(N) O(N2) \u2714\u2714\nLinear Transformer \u2714 O(1) O(N) \u2718\nRecurrent NN \u2718 O(1) O(N) \u2718\nRWKV \u2718 O(1) O(N) \u2714\nH3/S4 \u2714 O(1) O(NlogN) \u2714\nHyena \u2714 O(N) O(NlogN) \u2714\nRetNet \u2714 O(1) O(N) \u2714\u2714\nTable 1: Model comparison from various perspectives. RetNet achieves training parallelization,\nconstant inference cost, linear long-sequence memory complexity, and good performance.\nInference The recurrent representation (Equation (6)) is employed during the inference, which\nnicely fits autoregressive decoding. The O(1)complexity reduces memory and inference latency\nwhile achieving equivalent results.\n2.4 Relation to and Differences from Previous Methods\nTable 1 compares RetNet with previous methods from various perspectives. The comparison results\necho the \u201cimpossible triangle\u201d presented in Figure 2. Moreover, RetNet has linear memory complexity\nfor long sequences due to the chunkwise recurrent representation. We also summarize the comparisons\nwith specific methods as follows.\nTransformer The parallel representation of retention shares similar spirits as Transform-\ners [ VSP+17]. The most related Transformer variant is Lex Transformer [ SDP+22] which im-\nplements xPos as position embeddings. As described in Equation (3), the derivation of retention\naligns with xPos. In comparison with attention, retention removes softmax and enables recurrent\nformulation, which significantly benefits inference.\nS4 Unlike Equation (2), if QnandKnare content-unaware, the formulation can be degenerated to\nS4 [GGR21], where O= (QK\u22ba, QAK\u22ba, .., QA|x|\u22121K\u22ba)\u2217V.\nLinear Attention The variants typically use various kernels \u03d5(qi)\u03d5(kj)/P|x|\nn=1\u03d5(qi)\u03d5(kn)to replace\nthesoftmax function. However, linear attention struggles to effectively encode position information,\nrendering the models less performant. Besides, we reexamine sequence modeling from scratch, rather\nthan aiming at approximating softmax .\nAFT/RWKV Attention Free Transformer (AFT) simplifies dot-product attention to element-wise\noperations and moves softmax to key vectors. RWKV replaces AFT\u2019s position embeddings with\nexponential decay and runs the models recurrently for training and inference. In comparison, retention\npreserves high-dimensional states to encode sequence information, which contributes to expressive\nability and better performance.\nxPos/RoPE Compared with relative position embedding methods proposed for Transformers,\nEquation (3) presents a similar formulation as xPos [SDP+22] and RoPE [SLP+21].\nSub-LayerNorm As shown in Equation (8), the retention layer uses Sub-LayerNorm [ WMH+22]\nto normalize outputs. Because the multi-scale modeling leads to different variances for the heads, we\nreplace the original LayerNorm with GroupNorm.\n3 Experiments\nWe conduct experiments on language modeling to evaluate RetNet. We evaluate the proposed\narchitecture with various benchmarks, i.e., language modeling performance, and zero-/few-shot\nlearning on downstream tasks. Moreover, for training and inference, we compare speed, memory\nconsumption, and latency.\n6", "start_char_idx": 0, "end_char_idx": 3105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba7d32a4-4f2c-475a-865e-ea5e89ac65d3": {"__data__": {"id_": "ba7d32a4-4f2c-475a-865e-ea5e89ac65d3", "embedding": null, "metadata": {"page_label": "7", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1427bb47-aac0-482f-b2d4-09041d58bd78", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "6c72744f502d66ce72e09a5071fa39dfbf5a3d7b290b7a725e5b9526b86b7067", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "caec7162-7ba7-4a2a-be70-c38ea68e2601", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "e8d59a3adfe965c238220c9b3ee03479e2b4b73329991b668f75e19fb41459cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d18414f6-ea25-4b2b-a279-0c01d47647f8", "node_type": "1", "metadata": {}, "hash": "578f056ff1acdceea86093c0aafcb1bfd68fff38fe7e3e96ee12338eef2b27ad", "class_name": "RelatedNodeInfo"}}, "text": "Size Hidden Dim. #Layers Batch Size # Tokens Learning Rate\n1.3B 2048 24 4M 100B 6\u00d710\u22124\n2.7B 2560 32 4M 100B 3\u00d710\u22124\n6.7B 4096 32 4M 100B 3\u00d710\u22124\nTable 2: Sizes, and learning hyper-parameters of the models in language modeling experiments.\n1.3B 2.7B 6.7B\nModel Size12.513.013.514.014.515.0Validation PPL\nRetNet\nTransformer\nFigure 5: Perplexity decreases along with scaling up the model size. We empirically observe that\nRetNet tends to outperform Transformer when the model size is larger than 2B.\n3.1 Setup\nParameter Allocation We re-allocate the parameters in MSR and FFN for fair comparisons. Let d\ndenote dmodel for simplicity here. In Transformers, there are about 4d2parameters in self-attention\nwhere WQ, WK, WV, WO\u2208Rd\u00d7d, and 8d2parameters in FFN where the intermediate dimension is\n4d. In comparison, RetNet has 8d2parameters in retention, where WQ, WK\u2208Rd\u00d7d, WG, WV\u2208\nRd\u00d72d, WO\u2208R2d\u00d7d. Notice that the head dimension of Vis twice Q, K . The widened dimension\nis projected back to dbyWO. In order to keep the parameter number the same as Transformer, the\nFFN intermediate dimension in RetNet is 2d. Meanwhile, we set the head dimension to 256in our\nexperiments, i.e., 256for queries and keys, and 512for values. For fair comparison, we keep \u03b3\nidentical among different model sizes, where \u03b3= 1\u2212elinspace(log 1/32,log1/512,h)\u2208Rhinstead of the\ndefault value in Equation (8).\nLanguage Model Training As shown in Table 2, we train language models with various sizes\n(i.e., 1.3B, 2.7B, and 6.7B) from scratch. The training corpus is a curated compilation of The\nPile [ GBB+20], C4 [ DMI+21], and The Stack [ KLBA+22]. We append the <bos> token to indicate\nthe start of a sequence2. The training batch size is 4M tokens with 2048 maximal length. We\ntrain the models with 100B tokens, i.e., 25k steps. We use the AdamW [ LH19 ] optimizer with\n\u03b21= 0.9, \u03b22= 0.98, and weight decay is set to 0.05. The number of warmup steps is 375 with linear\nlearning rate decay. The parameters are initialized following DeepNet [ WMD+22] to guarantee\ntraining stability. The implementation is based on TorchScale [ MWH+22]. We train the models with\n512 AMD MI200 GPUs.\n3.2 Comparisons with Transformer\nLanguage Modeling As shown in Figure 5, we report perplexity on the validation set for the\nlanguage models based on Transformer and RetNet. We present the scaling curves with three model\nsizes, i.e., 1.3B, 2.7B, and 6.7B. RetNet achieves comparable results with Transformers. More\nimportantly, the results indicate that RetNet is favorable regarding size scaling. Besides performance,\nthe RetNet training is quite stable in our experiments. Experimental results show that RetNet is a\nstrong competitor to Transformer for large language models. Empirically, we find that RetNet starts\nto outperform Transformer when the model size is larger than 2B. We also summarize the language\nmodeling results with different context lengths in Appendix B.\n2We find that appending the <bos> token at the beginning benefits training stability and performance.\n7", "start_char_idx": 0, "end_char_idx": 3025, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d18414f6-ea25-4b2b-a279-0c01d47647f8": {"__data__": {"id_": "d18414f6-ea25-4b2b-a279-0c01d47647f8", "embedding": null, "metadata": {"page_label": "8", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "353a2d97-af49-4f77-af61-d0b1b433f604", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "294cc66517dec033828386cee867f6895a43ea0486c2acd70337418fa71764dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba7d32a4-4f2c-475a-865e-ea5e89ac65d3", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "6c72744f502d66ce72e09a5071fa39dfbf5a3d7b290b7a725e5b9526b86b7067", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de0cdc44-f78a-4c5f-9efe-3562d03ae5a6", "node_type": "1", "metadata": {}, "hash": "7c3caa6e2aac6dba2e7732f07275acc00587955c5bdad1487c9a2ee49fd4c91b", "class_name": "RelatedNodeInfo"}}, "text": "HS BoolQ COPA PIQA Winograd Winogrande SC Avg\nZero-Shot\nTransformer 55.9 62.0 69.0 74.6 69.5 56.5 75.0 66.07\nRetNet 60.7 62.2 77.0 75.4 77.2 58.1 76.0 69.51\n4-Shot\nTransformer 55.8 58.7 71.0 75.0 71.9 57.3 75.4 66.44\nRetNet 60.5 60.1 78.0 76.0 77.9 59.9 75.9 69.76\nTable 3: Zero-shot and few-shot learning with Transformer and RetNet. The model size is 6.7B.\nModel SizeMemory (GB) \u2193 Throughput (wps) \u2191\nTrm Trm+FlashAttn RetNet Trm Trm+FlashAttn RetNet\n1.3B 74.8 38.8 34.5 10832.4 63965.2 73344.8\n2.7B 69.6 42.1 42.0 5186.0 34990.2 38921.2\n6.7B 69.0 51.4 48.0 2754.4 16230.1 17458.6\n13B 61.4 46.3 45.9 1208.9 7945.1 8642.2\nTable 4: Training cost of Transformer (Trm), Transformer with FlashAttention (Trm+FlashAttn), and\nRetNet. We report memory consumption and training throughput (word per second; wps).\nZero-Shot and Few-Shot Evaluation on Downstream Tasks We also compare the language\nmodels on a wide range of downstream tasks. We evaluate zero-shot and 4-shot learning\nwith the 6.7B models. As shown in Table 3, the datasets include HellaSwag (HS) [ ZHB+19],\nBoolQ [ CLC+19], COPA [ WPN+19], PIQA [ BZB+20], Winograd, Winogrande [ LDM12 ], and Sto-\nryCloze (SC) [ MRL+17]. The accuracy numbers are consistent with language modeling perplexity\npresented in Figure 5. RetNet achieves comparable performance with Transformer on zero-shot and\nin-context learning settings.\n3.3 Training Cost\nAs shown in Table 4, we compare the training speed and memory consumption of Transformer and\nRetNet, where the training sequence length is 8192. We also compare with FlashAttention [ DFE+22],\nwhich improves speed and reduces GPU memory IO by recomputation and kernel fusion. In compari-\nson, we implement RetNet using vanilla PyTorch code, and leave kernel fusion or FlashAttention-like\nacceleration for future work. We use chunkwise recurrent representation of retention as described in\nEquation (7). The chunk size is set to 512. We evaluate the results with eight Nvidia A100-80GB\nGPUs, because FlashAttention is highly optimized for A100. Tensor parallelism is enabled for 6.7B\nand 13B models.\nExperimental results show that RetNet is more memory-efficient and has higher throughput than\nTransformers during training. Even compared with FlashAttention, RetNet is still competitive in\nterms of speed and memory cost. Moreover, without relying on specific kernels, it is easy to train\nRetNet on other platforms efficiently. For example, we train the RetNet models on an AMD MI200\ncluster with decent throughput. It is notable that RetNet has the potential to further reduce cost via\nadvanced implementation, such as kernel fusion.\n3.4 Inference Cost\nAs shown in Figure 6, we compare memory cost, throughput, and latency of Transformer and RetNet\nduring inference. Transformers reuse KV caches of previously decoded tokens. RetNet uses the\nrecurrent representation as described in Equation (6). We evaluate the 6.7B model on the A100-80GB\nGPU in our experiments. Figure 6 shows that RetNet outperforms Transformer in terms of inference\ncost.\nMemory As shown in Figure 6a, the memory cost of Transformer increases linearly due to KV\ncaches. In contrast, the memory consumption of RetNet remains consistent even for long sequences,\n8", "start_char_idx": 0, "end_char_idx": 3224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de0cdc44-f78a-4c5f-9efe-3562d03ae5a6": {"__data__": {"id_": "de0cdc44-f78a-4c5f-9efe-3562d03ae5a6", "embedding": null, "metadata": {"page_label": "9", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c09ae629-23c8-4815-b115-5070dbda2856", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "765995901d3e887b40e4c600f06d0b373a3f4d96f63ada0f209a5acf0f80121a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d18414f6-ea25-4b2b-a279-0c01d47647f8", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "294cc66517dec033828386cee867f6895a43ea0486c2acd70337418fa71764dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef14231c-4800-469c-8491-a86160014866", "node_type": "1", "metadata": {}, "hash": "e0e7dcf7c0b2e7fe9a0f5c48462d5cc846be694046dc2bd5fafd4d36b765f495", "class_name": "RelatedNodeInfo"}}, "text": "2048 3072 4096 5120 6144 7168 8192\nSequence Length15202530354045GPU Memory (GB)\nModel Weights\nRetNet\nTransformer(a) GPU memory cost of Transformer and RetNet.\n2048 3072 4096 5120 6144 7168 8192\nSequence Length50100150200250300Throughput (wps)\nRetNet\nTransformer (b) Throughput of Transformer and RetNet.\n1 2 3 4 5 6 7 8\nBatch Size50100150200250300350Latency (ms)\nTransformer (1024)\nTransformer (2048)\nTransformer (4096)\nTransformer (8192)\nRetNet (8192)\n(c) Inference latency with different batch sizes.\nFigure 6: Inference cost of Transformer and RetNet with a model size of 6.7B. RetNet outperforms\nTransformers in terms of memory consumption, throughput, and latency.\nrequiring much less GPU memory to host RetNet. The additional memory consumption of RetNet is\nalmost negligible (i.e., about 3%) while the model weights occupy 97%.\nThroughput As presented in Figure 6b, the throughput of Transformer drops along with the\ndecoding length increases. In comparison, RetNet has higher and length-invariant throughput during\ndecoding, by utilizing the recurrent representation of retention.\nLatency Latency is an important metric in deployment, which greatly affects user experience. We\nreport decoding latency in Figure 6c. Experimental results show that increasing batch size renders\nTransformer\u2019s latency larger. Moreover, the latency of Transformers grows faster with longer input. In\norder to make latency acceptable, we have to restrict the batch size, which harms the overall inference\nthroughput of Transformers. By contrast, RetNet\u2019s decoding latency outperforms Transformers and\nkeeps almost the same across different batch sizes and input lengths.\n3.5 Comparison with Transformer Variants\nApart from Transformer, we compare RetNet with various efficient Transformer variants, including\nLinear Transformer [ KVPF20 ], RWKV [ PAA+23], H3 [ DFS+22], and Hyena [ PMN+23]. All\nmodels have 200M parameters with 16 layers and a hidden dimension of 1024. For H3, we set the\nhead dimension as 8. For RWKV , we use the TimeMix module to substitute self-attention layers\nwhile keeping FFN layers consistent with other models for fair comparisons. We train the models\nwith 10k steps with a batch size of 0.5M tokens. Most hyperparameters and training corpora are kept\nthe same as in Section 3.1.\nTable 5 reports the perplexity numbers on the in-domain validation set and other out-of-domain\ncorpora, e.g., Project Gutenberg 2019-2022 (PG22) [ SDP+22], QMSum [ ZYY+21], GovRe-\n9", "start_char_idx": 0, "end_char_idx": 2474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef14231c-4800-469c-8491-a86160014866": {"__data__": {"id_": "ef14231c-4800-469c-8491-a86160014866", "embedding": null, "metadata": {"page_label": "10", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4318d69b-6d75-423c-97c4-b0988a2f4c89", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "dddca4b725ef169814c19d5ac149642118d193bf11c782d3f9761f4f289e5099", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de0cdc44-f78a-4c5f-9efe-3562d03ae5a6", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "765995901d3e887b40e4c600f06d0b373a3f4d96f63ada0f209a5acf0f80121a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62392769-18d6-40a3-957f-94d4508fbf7e", "node_type": "1", "metadata": {}, "hash": "e149fd7da6c9a87758e71292e1d1e7ef33beb1898632b9832983816c186604c7", "class_name": "RelatedNodeInfo"}}, "text": "Method In-Domain PG22 QMSum GovReport SummScreen\nRWKV 30.92 51.41 28.17 19.80 25.78\nH3 29.97 49.17 24.29 19.19 25.11\nHyena 32.08 52.75 28.18 20.55 26.51\nLinear Transformer 40.24 63.86 28.45 25.33 32.02\nRetNet 26.05 45.27 21.33 16.52 22.48\nTable 5: Perplexity results on language modeling. RetNet outperforms other architectures on both the\nin-domain evaluation set and various out-of-domain corpora.\nMethod In-Domain PG22 QMSum GovReport SummScreen\nRetNet 26.05 45.27 21.33 16.52 22.48\n\u2212swish gate 27.84 49.44 22.52 17.45 23.72\n\u2212GroupNorm 27.54 46.95 22.61 17.59 23.73\n\u2212\u03b3decay 27.86 47.85 21.99 17.49 23.70\n\u2212multi-scale decay 27.02 47.18 22.08 17.17 23.38\nReduce head dimension 27.68 47.72 23.09 17.46 23.41\nTable 6: Ablation results on in-domain and out-of-domain corpora.\nport [ HCP+21], SummScreen [ CCWG21 ,SSI+22]. Overall, RetNet outperforms previous methods\nacross different datasets. RetNet not only achieves better evaluation results on the in-domain corpus\nbut also obtains lower perplexity on several out-of-domain datasets. The favorable performance\nmakes RetNet a strong successor to Transformer, besides the benefits of significant cost reduction\n(Sections 3.3 and 3.4).\nIn addition, we discuss the training and inference efficiency of the compared methods. Let ddenote\nthe hidden dimension, and nthe sequence length. For training, RWKV\u2019s token-mixing complexity\nisO(dn)while Hyena\u2019s is O(dnlogn)with Fast Fourier Transform acceleration. The above two\nmethods reduce training FLOPS via employing element-wise operators to trade-off modeling capacity.\nIn comparison with retention, the chunk-wise recurrent representation is O(dn(b+h)), where bis\nthe chunk size, his the head dimension, and we usually set b= 512 , h= 256 . For either large model\nsize (i.e., larger d) or sequence length, the additional b+hhas negligible effects. So the RetNet\ntraining is quite efficient without sacrificing the modeling performance. For inference, among the\ncompared efficient architectures, Hyena has the same complexity (i.e., O(n)per step) as Transformer\nwhile the others can perform O(1)decoding.\n3.6 Ablation Studies\nWe ablate various design choices of RetNet and report the language modeling results in Table 6. The\nevaluation settings and metrics are the same as in Section 3.5.\nArchitecture We ablate the swish gate and GroupNorm as described in Equation (8). Table 6\nshows that the above two components improve the final performance. Firstly, the gating module is\nessential for enhancing non-linearity and improving model capability. Notice that we use the same\nparameter allocation as Transformers after removing the gate. Secondly, group normalization in\nretention balances the variances of multi-head outputs, which improves training stability and language\nmodeling results.\nMulti-Scale Decay Equation (8) shows that we use different \u03b3as the decay rates for the retention\nheads. In the ablation studies, we examine removing \u03b3decay (i.e., \u201c \u2212\u03b3decay\u201d) and applying the\nsame decay rate across heads (i.e., \u201c \u2212multi-scale decay\u201d). Specifically, ablating \u03b3decay is equivalent\nto\u03b3= 1. In the second setting, we set \u03b3= 127 /128for all heads. Table 6 indicates that both the\ndecay mechanism and using multiple decay rates can improve the language modeling performance.\nHead Dimension From the recurrent perspective of Equation (1), the head dimension implies the\nmemory capacity of hidden states. In the ablation study, we reduce the default head dimension from\n10", "start_char_idx": 0, "end_char_idx": 3467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62392769-18d6-40a3-957f-94d4508fbf7e": {"__data__": {"id_": "62392769-18d6-40a3-957f-94d4508fbf7e", "embedding": null, "metadata": {"page_label": "11", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72ebd1e0-7104-49f3-ac43-b44ec7632f13", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "fac5874b42abf03c62b40f44860d7df44a3cfbe626a64090e95a616663c83932", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef14231c-4800-469c-8491-a86160014866", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "dddca4b725ef169814c19d5ac149642118d193bf11c782d3f9761f4f289e5099", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "559e300e-957d-4821-b428-f17e6313bd51", "node_type": "1", "metadata": {}, "hash": "e4d0e2eba8cff173bd6d6c46c8e8807e897a33bcf3737e13b96829bbde935436", "class_name": "RelatedNodeInfo"}}, "text": "256to64, i.e., 64for queries and keys, and 128for values. We keep the hidden dimension dmodel the\nsame so the number of heads increases. Experimental results in Table 6 show that the larger head\ndimension achieves better performance.\n4 Conclusion\nIn this work, we propose retentive networks (RetNet) for sequence modeling, which enables various\nrepresentations, i.e., parallel, recurrent, and chunkwise recurrent. RetNet achieves significantly better\ninference efficiency (in terms of memory, speed, and latency), favorable training parallelization,\nand competitive performance compared with Transformers. The above advantages make RetNet an\nideal successor to Transformers for large language models, especially considering the deployment\nbenefits brought by the O(1)inference complexity. In the future, we would like to scale up RetNet\nin terms of model size [ CDH+22] and training steps. Moreover, retention can efficiently work with\nstructured prompting [ HSD+22b] by compressing long-term memory. We will also use RetNet as the\nbackbone architecture to train multimodal large language models [ HSD+22a,HDW+23,PWD+23].\nIn addition, we are interested in deploying RetNet models on various edge devices, such as mobile\nphones.\nAcknowledgement\nWe would like to acknowledge Jiayu Ding, Songlin Yang, and colleagues from MSRA System Group\nfor the helpful discussions.\nReferences\n[BKH16] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv\npreprint arXiv:1607.06450 , 2016.\n[BMR+20]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners. In Advances in Neural Information Processing Systems ,\nvolume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.\n[BZB+20]Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa:\nReasoning about physical commonsense in natural language. In Thirty-Fourth AAAI\nConference on Artificial Intelligence , 2020.\n[CCWG21] Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. Summscreen: A dataset\nfor abstractive screenplay summarization. arXiv preprint arXiv:2104.07091 , 2021.\n[CDH+22]Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham\nSinghal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the\nrepresentation collapse of sparse mixture of experts. In Advances in Neural Information\nProcessing Systems , 2022.\n[CLC+19]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins,\nand Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no\nquestions. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics , pages 2924\u20132936, 2019.\n[DFE+22]Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention:\nFast and memory-efficient exact attention with io-awareness. Advances in Neural\nInformation Processing Systems , 35:16344\u201316359, 2022.\n[DFS+22]Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher\nR\u00e9. Hungry hungry hippos: Towards language modeling with state space models. arXiv\npreprint arXiv:2212.14052 , 2022.\n11", "start_char_idx": 0, "end_char_idx": 3561, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "559e300e-957d-4821-b428-f17e6313bd51": {"__data__": {"id_": "559e300e-957d-4821-b428-f17e6313bd51", "embedding": null, "metadata": {"page_label": "12", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5875b819-8e5a-423f-8899-130c92be13fe", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7fd5c98df6891481ba9f2999739113bda7f1eadae538c51ee16777be0e1681e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62392769-18d6-40a3-957f-94d4508fbf7e", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "fac5874b42abf03c62b40f44860d7df44a3cfbe626a64090e95a616663c83932", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d19f5675-40df-4feb-ad44-1d51451b9fcb", "node_type": "1", "metadata": {}, "hash": "7ad71cac2c72ad6bc1e26a050d02b4bbdc8a178a51707bde7ff7ad98cdc03107", "class_name": "RelatedNodeInfo"}}, "text": "[DMI+21]Jesse Dodge, Ana Marasovi \u00b4c, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and\nMatt Gardner. Documenting large webtext corpora: A case study on the colossal clean\ncrawled corpus. In Conference on Empirical Methods in Natural Language Processing ,\n2021.\n[GBB+20]Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\nJason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB\ndataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.\n[GGR21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with\nstructured state spaces. arXiv preprint arXiv:2111.00396 , 2021.\n[HCP+21]Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient\nattentions for long document summarization. arXiv preprint arXiv:2104.02112 , 2021.\n[HDW+23]Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,\nTengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi,\nJohan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language\nis not all you need: Aligning perception with language models. ArXiv , abs/2302.14045,\n2023.\n[HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv:\nLearning , 2016.\n[HS97] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computa-\ntion, 9:1735\u20131780, November 1997.\n[HSD+22a] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shum-\ning Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv ,\nabs/2206.06336, 2022.\n[HSD+22b] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured\nprompting: Scaling in-context learning to 1,000 examples. ArXiv , abs/2212.06713,\n2022.\n[KLBA+22]Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos\nMu\u00f1oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf,\nDzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The Stack: 3TB of\npermissively licensed source code. Preprint , 2022.\n[KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Trans-\nformers are rnns: Fast autoregressive transformers with linear attention. In International\nConference on Machine Learning , pages 5156\u20135165. PMLR, 2020.\n[LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema chal-\nlenge. In Thirteenth International Conference on the Principles of Knowledge Repre-\nsentation and Reasoning , 2012.\n[LH19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Interna-\ntional Conference on Learning Representations , 2019.\n[MRL+17]Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James\nAllen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd\nWorkshop on Linking Models of Lexical, Sentential and Discourse-level Semantics ,\npages 46\u201351, 2017.", "start_char_idx": 0, "end_char_idx": 2925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d19f5675-40df-4feb-ad44-1d51451b9fcb": {"__data__": {"id_": "d19f5675-40df-4feb-ad44-1d51451b9fcb", "embedding": null, "metadata": {"page_label": "12", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5875b819-8e5a-423f-8899-130c92be13fe", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "7fd5c98df6891481ba9f2999739113bda7f1eadae538c51ee16777be0e1681e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "559e300e-957d-4821-b428-f17e6313bd51", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "6bdee5f99060d1bd0ab0fb19dac66ead3253a685b3c420d17e3cda3f74ad76ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ec347d1-6a1a-409c-8927-010db948f034", "node_type": "1", "metadata": {}, "hash": "6db85a3230fdfd7a910235e6fd712032de217ae5824913cadd4f435e86b126e1", "class_name": "RelatedNodeInfo"}}, "text": "PMLR, 2020.\n[LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema chal-\nlenge. In Thirteenth International Conference on the Principles of Knowledge Repre-\nsentation and Reasoning , 2012.\n[LH19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Interna-\ntional Conference on Learning Representations , 2019.\n[MRL+17]Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James\nAllen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd\nWorkshop on Linking Models of Lexical, Sentential and Discourse-level Semantics ,\npages 46\u201351, 2017.\n[MWH+22]Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong,\nAlon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei. TorchScale:\nTransformers at scale. CoRR , abs/2211.13184, 2022.\n[OSG+23]Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre,\nRazvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long\nsequences. ArXiv , abs/2303.06349, 2023.\n12", "start_char_idx": 2297, "end_char_idx": 3350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ec347d1-6a1a-409c-8927-010db948f034": {"__data__": {"id_": "1ec347d1-6a1a-409c-8927-010db948f034", "embedding": null, "metadata": {"page_label": "13", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68e1646a-3244-46e4-8a48-3f40d635d268", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "a0debe6ef0695919d6e4edf03ab2b79905b4362650a4a8d4ba0f04d896dd3c12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d19f5675-40df-4feb-ad44-1d51451b9fcb", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "0e11baa2ce0ef02ae8f002fd425994b6e4bf6c31404bb67c717502889ed803dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f96506f-1ada-43d2-bbe1-479bce31204d", "node_type": "1", "metadata": {}, "hash": "9e60a0845d3db6686829006321b0c52240f5492e5308c5f92c5d4a584791cdc6", "class_name": "RelatedNodeInfo"}}, "text": "[PAA+23]Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi\nCao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV , Xuzheng He,\nHaowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra,\nHayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang,\nBolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang,\nQihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the\ntransformer era, 2023.\n[PMN+23]Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus,\nYoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger\nconvolutional language models. arXiv preprint arXiv:2302.10866 , 2023.\n[PWD+23]Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and\nFuru Wei. Kosmos-2: Grounding multimodal large language models to the world.\nArXiv , abs/2306.14824, 2023.\n[RZL17] Prajit Ramachandran, Barret Zoph, and Quoc V . Le. Swish: a self-gated activation\nfunction. arXiv: Neural and Evolutionary Computing , 2017.\n[SDP+22]Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,\nVishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer.\narXiv preprint arXiv:2212.10554 , 2022.\n[Sha19] Noam M. Shazeer. Fast transformer decoding: One write-head is all you need. ArXiv ,\nabs/1911.02150, 2019.\n[SLP+21]Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv preprint arXiv:2104.09864 , 2021.\n[SPP+19]Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro. Megatron-LM: Training multi-billion parameter language models\nusing model parallelism. arXiv preprint arXiv:1909.08053 , 2019.\n[SSI+22]Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta,\nWenhan Xiong, Mor Geva, Jonathan Berant, et al. Scrolls: Standardized comparison\nover long language sequences. arXiv preprint arXiv:2201.03533 , 2022.\n[VSP+17]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in\nNeural Information Processing Systems 30: Annual Conference on Neural Information\nProcessing Systems 2017, 4-9 December 2017, Long Beach, CA, USA , pages 6000\u2013\n6010, 2017.\n[WH18] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European\nconference on computer vision (ECCV) , pages 3\u201319, 2018.\n[WMD+22]Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu\nWei. DeepNet: Scaling Transformers to 1,000 layers. ArXiv , abs/2203.00555, 2022.", "start_char_idx": 0, "end_char_idx": 2708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f96506f-1ada-43d2-bbe1-479bce31204d": {"__data__": {"id_": "6f96506f-1ada-43d2-bbe1-479bce31204d", "embedding": null, "metadata": {"page_label": "13", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68e1646a-3244-46e4-8a48-3f40d635d268", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "a0debe6ef0695919d6e4edf03ab2b79905b4362650a4a8d4ba0f04d896dd3c12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ec347d1-6a1a-409c-8927-010db948f034", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "67134074e40252ab4c8e9bbf0be6b2ed925e1043b1a786280591898ef92f6981", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc1eadf7-c33e-436a-b99f-4ed1019f1f7e", "node_type": "1", "metadata": {}, "hash": "9df8ca23d5bb56fc7e0b300abbad452c91c266669f225db37d3cbcb25e1fc002", "class_name": "RelatedNodeInfo"}}, "text": "Attention is all you need. In Advances in\nNeural Information Processing Systems 30: Annual Conference on Neural Information\nProcessing Systems 2017, 4-9 December 2017, Long Beach, CA, USA , pages 6000\u2013\n6010, 2017.\n[WH18] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European\nconference on computer vision (ECCV) , pages 3\u201319, 2018.\n[WMD+22]Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu\nWei. DeepNet: Scaling Transformers to 1,000 layers. ArXiv , abs/2203.00555, 2022.\n[WMH+22]Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng,\nYu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, et al. Foundation transformers.\narXiv preprint arXiv:2210.06423 , 2022.\n[WPN+19]Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel R Bowman. SuperGLUE: A stickier benchmark for\ngeneral-purpose language understanding systems. arXiv preprint arXiv:1905.00537 ,\n2019.\n[ZHB+19]Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag:\nCan a machine really finish your sentence? In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics , 2019.\n[ZYY+21]Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Has-\nsan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: A new\nbenchmark for query-based multi-domain meeting summarization. arXiv preprint\narXiv:2104.05938 , 2021.\n13", "start_char_idx": 2190, "end_char_idx": 3672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc1eadf7-c33e-436a-b99f-4ed1019f1f7e": {"__data__": {"id_": "fc1eadf7-c33e-436a-b99f-4ed1019f1f7e", "embedding": null, "metadata": {"page_label": "14", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1faeee5b-c3b6-4869-a10f-be0ef3f2eb9f", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "abfbdd4fe66d6bc45586be7da391b28b27adf27148e429776b5b81bb7de34071", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f96506f-1ada-43d2-bbe1-479bce31204d", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}, "hash": "cd3cee244b387bf4b7e2f52baefc28101ace594a8aa6decf8087b905ff4ffb48", "class_name": "RelatedNodeInfo"}}, "text": "A Hyperparameters\nHyperparameters 1.3B 2.7B 6.7B\nLayers 24 32 32\nHidden size 2048 2560 4096\nFFN size 4096 5120 8192\nHeads 8 10 16\nLearning rate 6\u00d710\u221243\u00d710\u221243\u00d710\u22124\nLR scheduler Polynomial decay\nWarm-up steps 375\nTokens per batch 4M\nAdam \u03b2 (0.9, 0.98)\nTraining steps 25,000\nGradient clipping 2.0\nDropout 0.1\nWeight decay 0.01\nTable 7: Hyperparamters used for the models in Section 3.\nB Grouped Results of Different Context Lengths\nAs shown in Table 8, we report language modeling results with different context lengths. In order\nto make the numbers comparable, we use 2048 text chunks as evaluation data and only compute\nperplexity for the last 128 tokens. Experimental results show that RetNet outperforms Transformer\nacross different context lengths. Besides, RetNet can utilize longer context for better results.\nModel 512 1024 2048\nTransformer 13.55 12.56 12.35\nRetNet 13.09 12.14 11.98\nTable 8: Language modeling perplexity of RetNet and Transformer with different context length. The\nresults show that RetNet has a consistent advantage across sequence length.\n14", "start_char_idx": 0, "end_char_idx": 1066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"549e1b3b-a091-4f8d-abe1-2244b6980d26": {"node_ids": ["e5635af9-1fdb-4d2f-b615-2e81e2d4e513", "8143711d-516d-42f8-a52e-97dd35b07675"], "metadata": {"page_label": "1", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "937eebbd-b7e2-42b6-b629-f4516bba1bce": {"node_ids": ["016a780b-82bf-48dc-b7a0-6d2f5005a13d", "5f93ba42-f0fd-421f-92dc-ee6f7778d77e"], "metadata": {"page_label": "2", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "13e5a54c-cb0c-48f3-a507-72968247b25f": {"node_ids": ["831d9a2b-5ccf-47d0-8351-161ef96e0023"], "metadata": {"page_label": "3", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "4b99402d-2a0c-4bcb-9f25-4bafe50e25e0": {"node_ids": ["bf77674c-7216-4516-8e33-606cad88487a", "23d7c70f-f784-43f9-a515-708542c786c2"], "metadata": {"page_label": "4", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "ff21b96b-3d44-46c1-8d56-afffd419ca7b": {"node_ids": ["51693bc7-c580-40e6-b3f6-cfb9b5552572", "37d86c02-1a23-48d8-befe-9514f8aaacd2"], "metadata": {"page_label": "5", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "b73c1dad-7c1d-4a8c-a02b-f98bfa7ed10d": {"node_ids": ["9e47eba8-3f53-4e69-a077-c58782bde03c", "50e1968e-96a5-419c-be50-f4525e3c95fb"], "metadata": {"page_label": "6", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "9a96f8ac-22d2-48de-980c-6a61390c23fa": {"node_ids": ["1243974e-f8e4-4a2e-8add-f662318bbe43", "8b714e85-9b55-436d-8850-c7248d366e94"], "metadata": {"page_label": "7", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "033e0bb8-3dcf-433a-b4f2-2e67b4a0c6d8": {"node_ids": ["70b0c482-be1e-4f37-b892-419da9e187de"], "metadata": {"page_label": "8", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "e4a1af29-7df3-4558-a3ed-125023d42c51": {"node_ids": ["3aaa02f6-d8dd-48b0-8032-96c96c68da8a", "a9fe1235-c857-4172-95a7-3e7cdda6cbb6"], "metadata": {"page_label": "9", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "6a6e3dcf-5eef-4889-adfb-d7e044f459f6": {"node_ids": ["b25c9ca3-d116-4a86-8589-80242f3e2529", "64ab38de-2ab2-4004-9c54-30307d2e8a01"], "metadata": {"page_label": "10", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "7fb5d34d-4a07-4911-abec-dbbf425108f4": {"node_ids": ["4ce9d3df-ba9e-4e68-81a5-2dca7d25e1a8", "aa939a26-1d07-416f-aca8-65fbdaa48348"], "metadata": {"page_label": "11", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "8ba6dbe8-8bcd-4626-9e9d-4b9c38bcdcae": {"node_ids": ["e7207de6-6d84-4172-b7fa-6ce41e8cdf96", "49d68f11-3811-46e6-ac67-6931e8ec84a3"], "metadata": {"page_label": "12", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "2737fd4b-f5af-4230-9635-e34ebb678626": {"node_ids": ["3ff1d2fc-0ee8-4776-9987-303059c8a6d6", "bd2e9333-ec11-40a4-9386-bae415501b28"], "metadata": {"page_label": "13", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "da2a7276-8d87-4b9a-b968-68c38bde67b3": {"node_ids": ["186a110d-27b0-448e-a529-3b3a031ad2ff", "7cd548c7-4dab-4201-aa76-f12bace264be"], "metadata": {"page_label": "14", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "6ade6dee-14b1-4b27-8ba3-66a6b1217f5a": {"node_ids": ["cb9d4f26-4723-4d4d-9371-44f35e54c106", "9e64d7ba-c5de-419c-85c7-f716a25e7fdb"], "metadata": {"page_label": "15", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "58c7a09b-8a15-4818-849d-387dc4e836d6": {"node_ids": ["6844e2e9-74ad-47c8-825a-5c807fd50fe7"], "metadata": {"page_label": "16", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "9cecb09d-cb03-458f-b2ce-7f27ab2bc876": {"node_ids": ["b8a82c3d-8149-4653-bb1c-7e9ed8624403"], "metadata": {"page_label": "I", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "d7ae6d01-304b-4c09-a244-572a4058e14c": {"node_ids": ["dc25b042-69a3-46b4-9ce8-9414d7b98230", "b15cd3a5-eb40-4e36-980e-3a7088a9a1d4", "9671fd20-6caa-4599-bcfa-ef4feeb27726"], "metadata": {"page_label": "II", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "e574a7b8-56aa-40b1-b72b-7f1553fc4059": {"node_ids": ["8ea26e5c-6bb4-4c34-92bf-6c1630ae1741"], "metadata": {"page_label": "III", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "78798ce2-5305-4892-9024-7cf57157fe08": {"node_ids": ["b9f262d6-21bd-43ce-9a57-5628e1fd884f"], "metadata": {"page_label": "IV", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "0913e9f7-fdd1-4eed-b481-6b47a8c8e23c": {"node_ids": ["20f0cac9-6d2b-42cb-9c6f-68c1d587c0a4"], "metadata": {"page_label": "V", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "15e29758-4a9f-46a5-8d36-c73b1463c98e": {"node_ids": ["ee69358b-a8ed-41fc-becb-d3ceaa668f2a"], "metadata": {"page_label": "VI", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "a38c2721-cca2-494b-887d-ac5bb388323d": {"node_ids": ["533c7a64-acba-4267-aaa3-66c093a0d474"], "metadata": {"page_label": "VII", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "c3ba744a-617b-4b10-acba-e1a62e7bbed0": {"node_ids": ["6cd8f8ae-3ca5-47b3-a75e-f7819f9a3d66"], "metadata": {"page_label": "VIII", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "fdb17ba8-4844-4df3-bcbb-392f471fb9bf": {"node_ids": ["d0892b19-09a9-4e8e-97fb-8bf1d22a182e"], "metadata": {"page_label": "IX", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "1a4fbe49-e77e-444c-aa62-6c31f1e2fc46": {"node_ids": ["fd53522e-075c-4667-bfab-278780d89409"], "metadata": {"page_label": "X", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "ec05b3b1-aad5-44d2-8c44-040ee82fb753": {"node_ids": ["67b0e9e0-57c3-4673-8de9-112b6a8a2811"], "metadata": {"page_label": "XI", "file_name": "2302.13971.pdf", "file_path": "docs\\2302.13971.pdf", "file_type": "application/pdf", "file_size": 726566, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "1367d900-2b26-4949-ad8b-e5b7c6d79721": {"node_ids": ["2ef333ac-4042-461b-8927-32b9b9f45f85", "e9021931-d6e6-45c9-a307-c58de1e20a21"], "metadata": {"page_label": "1", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "1c528bc4-d3b0-4ea5-9e00-ea1c812a5e63": {"node_ids": ["b86c3c4f-f7e9-4929-926e-60dfb5f1c41d"], "metadata": {"page_label": "2", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "7217d96d-aad0-4cc9-aa1b-0818e7b73bdb": {"node_ids": ["a6225579-6446-4701-91fa-e3ffded97535"], "metadata": {"page_label": "3", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "fe3c2a45-9111-42e1-8575-54ace684c88f": {"node_ids": ["7c57aae8-eea2-4ec4-84b5-22a3ea586fea", "e5aa6003-6819-4298-9edc-9203de83f727"], "metadata": {"page_label": "4", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "997f9442-a39f-4369-95e1-f13ab24249c0": {"node_ids": ["869f8667-4260-41d3-9154-a8428ee7b3b4", "ef6655fd-e555-4977-8f6d-5fff1d3eeed5"], "metadata": {"page_label": "5", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "1a4be734-6624-420b-8a46-bfaa870706ae": {"node_ids": ["e2a1dcbe-c6a2-4a78-a389-5e9441efc45c", "06fa9fd3-6bb7-491b-b0e7-4d10cc4ece24"], "metadata": {"page_label": "6", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "c42cc958-8536-4c4f-a43b-4562d494ee68": {"node_ids": ["1c21241b-f464-413d-9942-dfec22189f6c", "d14c55cc-8bc3-4f55-8b33-71578bfcde0d"], "metadata": {"page_label": "7", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "562f55cb-358a-4448-8f15-73d39df4745d": {"node_ids": ["d4189545-9f9a-4ef1-98cc-0a81aec6ea93", "b84764ee-9a70-4440-a2b0-d61a221a5e13"], "metadata": {"page_label": "8", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "bd786b2d-8393-4dd5-b5c7-7e07ab90747b": {"node_ids": ["2a0f0850-85f3-439f-84ce-f190377921d1", "3dbeae1b-299d-4f5f-a1b3-f0d4f879a423"], "metadata": {"page_label": "9", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "ba68a9fb-d9f3-4cab-9b39-2107974a3240": {"node_ids": ["1b251110-29d9-4bfb-9962-1398169cae0b", "e24fd0b9-8aec-4fa5-b5cb-2706b417a90f"], "metadata": {"page_label": "10", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "ae59d4a4-b6ff-4652-9261-0aba6de6a794": {"node_ids": ["357e8147-4dbe-4fd0-a197-20df732bec23", "fbd78736-83ee-4a0f-8123-4b3066399ee0"], "metadata": {"page_label": "11", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "dd852b53-a1b3-49b8-8721-705d90fc3e96": {"node_ids": ["9bec28a5-2384-4702-b391-0c209de2c43b", "71c2f4d4-9f8b-404a-9473-aae3dcf0894a"], "metadata": {"page_label": "12", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "749ba495-d5ca-4d10-a92f-283833d68dd2": {"node_ids": ["69a130fa-e299-4c24-8003-83a85da8ed33", "68aa091d-c8f4-44fd-ad48-98eec269670e"], "metadata": {"page_label": "13", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "8b124b9d-5509-4177-9e6c-d4e7c2cea021": {"node_ids": ["445afb20-7c2e-4c7d-93ee-72a2e75b38cb", "14f4c3aa-a3d8-4f30-a22e-9d18f9bbc32b"], "metadata": {"page_label": "14", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "30d5b6e7-7500-4405-b0bf-10a1615bff47": {"node_ids": ["a792c8ef-7a81-48a7-bc0d-6797331df8b8", "19f1515b-1bd1-4641-99a5-93e14df1457d"], "metadata": {"page_label": "15", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "128f4dbe-1291-4aa9-84f8-2b07369f8f9d": {"node_ids": ["ec383b36-8ddd-45f5-9703-287d48500c3e", "61c49036-b174-42f1-94d5-e42ac913de81"], "metadata": {"page_label": "16", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "a3c1b640-f9e6-4019-b9e3-bc78d5bdb498": {"node_ids": ["f4e28ff8-a124-47ca-857f-e84a5517d915", "6a9c6235-b1f6-41fd-8c2f-f0cea3d0c1d7"], "metadata": {"page_label": "17", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "5e020124-a827-4495-82dc-522c8f050497": {"node_ids": ["e281bd8a-7299-4f17-87fd-31d2eb25073e", "dd9a1243-7e48-4401-a299-71334c199a08"], "metadata": {"page_label": "18", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "2fcfd476-bafc-40c5-8ba0-af0499e67607": {"node_ids": ["b0321cb7-d60d-4168-a93d-ef0b89ee413a", "227521f7-0347-4971-a446-96cdfc8fa441", "410a1ea1-ee6a-4bad-91d8-faebdf087155"], "metadata": {"page_label": "19", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "da36e7b2-1ce3-4399-90a8-4f3aa340ed78": {"node_ids": ["2c4d8ebd-2d0b-484d-80ae-3623139f0a64", "b22e1b1c-268c-40b9-aa57-08a90f20bb23"], "metadata": {"page_label": "20", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "721837dc-eeac-49df-ad56-d17a2ad79445": {"node_ids": ["1d26bd8c-28b6-48e0-b848-15bbbcf909be"], "metadata": {"page_label": "21", "file_name": "2303.15647.pdf", "file_path": "docs\\2303.15647.pdf", "file_type": "application/pdf", "file_size": 860410, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "805869df-8afb-4fe8-aa9c-f78b9495293d": {"node_ids": ["45c4175e-d0b4-46d1-ab3e-e248d3a3d02e"], "metadata": {"page_label": "1", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "861a8f99-6af7-480e-92bc-93dc949fe274": {"node_ids": ["09815e6c-69c5-430a-a7a3-1ad3d7e60cd5", "38deda85-8093-417b-85de-eb894a6bc0c3", "cec8bdf5-d5c6-403c-8476-332ca241c1c4"], "metadata": {"page_label": "2", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "9cf3eec0-f1d0-4c09-8697-d5e9209aa401": {"node_ids": ["f3376e3b-0ace-4fed-a31a-542fa5161a23"], "metadata": {"page_label": "3", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "f881f6b9-c2ef-4b8b-8e9c-f288d43d1bea": {"node_ids": ["02a3d26f-6994-4f95-96af-10da10a657e6"], "metadata": {"page_label": "4", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "a2a03fce-b57d-422c-8109-c9debb279d07": {"node_ids": ["ac3055d4-bb2f-4a87-ab39-ffc64121cede"], "metadata": {"page_label": "5", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "68d67c24-2ade-4700-9f1b-0ae06f0c7b48": {"node_ids": ["35c8874c-aa7d-4995-a294-1f53a09420d9"], "metadata": {"page_label": "6", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "b429f3d3-930f-4efe-8130-a0b91c905db2": {"node_ids": ["8100895b-27cf-48c5-83e2-92b03cad7595"], "metadata": {"page_label": "7", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "5bf04aab-e0bb-4ea3-a49b-bfab1f38af92": {"node_ids": ["014e6ef1-9fdd-4594-8824-dbbacda770b9"], "metadata": {"page_label": "8", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "c252b906-0c44-4de1-a2e7-4fb5fa849b07": {"node_ids": ["14981466-f049-4a81-868c-9eaf9a0cb562"], "metadata": {"page_label": "9", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "1352167c-6c9a-4aa0-a0b5-9e86ded06817": {"node_ids": ["4344cb65-d9e8-42ab-9d34-1b7227c48170"], "metadata": {"page_label": "10", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "14dd126d-88c1-4a36-ae5c-481eb211bde1": {"node_ids": ["7ad3be36-b736-499a-a6f0-2d1312201665"], "metadata": {"page_label": "11", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "8132d26d-422e-4746-921e-709ef0ebef6b": {"node_ids": ["ad880d7e-4eea-4819-a6fb-174b059e8d8b"], "metadata": {"page_label": "12", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "c376a09f-b21c-48dc-bbd6-16ce3eda517d": {"node_ids": ["ceb1b3b9-7688-4146-a10b-f77092b54bcc"], "metadata": {"page_label": "13", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "d3233c7e-bafc-46f7-a025-194635e51725": {"node_ids": ["ee04aa4e-58e9-4da7-99c2-8eabd3fe3b70"], "metadata": {"page_label": "14", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "3493162d-89d8-494d-819b-28148f912471": {"node_ids": ["23154560-63b4-464a-bb4d-9ba8508019a2"], "metadata": {"page_label": "15", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "7894177e-62ee-4b0e-a2dd-612a30e939f8": {"node_ids": ["914f70f3-ee6c-4573-8342-236884201070"], "metadata": {"page_label": "16", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "8cadc1a7-4345-4ce4-8dae-4fe134bef877": {"node_ids": ["dd738e10-6b5f-4d17-872d-b41a1b9555f4"], "metadata": {"page_label": "17", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "6f51ddd1-f17e-44c6-b54b-de27c1552691": {"node_ids": ["47d9d1a7-d392-4ae7-8d30-0f396c41c545"], "metadata": {"page_label": "18", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "b58f2bcc-546a-4b45-b4de-7e264d2d5ce8": {"node_ids": ["1bd61933-8656-42a3-bf4e-182fd5fb7830"], "metadata": {"page_label": "19", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "01bd8987-9835-4bd2-840c-9ec42b2baa8a": {"node_ids": ["76afe11e-2b48-4eb5-bcf0-4324308aafe7"], "metadata": {"page_label": "20", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "3c4505c2-d38c-4c13-8c88-937414d7421f": {"node_ids": ["2622add1-04c3-465a-af28-ab8ed217f13f"], "metadata": {"page_label": "21", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "368d7731-a37c-4558-8253-ca751f363a3f": {"node_ids": ["59db3101-fe89-4f49-af6c-524894cd5e25"], "metadata": {"page_label": "22", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "daa9da1c-ea39-4154-bff7-ffc47d92b5c5": {"node_ids": ["0aad11f5-66e6-4f96-8e3f-a12fdc42feb2"], "metadata": {"page_label": "23", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "28fff96f-c15a-422f-b2b7-50bc4329b2b9": {"node_ids": ["9323b3ad-9c4c-4984-b858-720ed251718e"], "metadata": {"page_label": "24", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "7bd03e2c-d7cc-409a-a469-89af880ab637": {"node_ids": ["c009ee86-065f-4c23-a477-f4c34b40b966"], "metadata": {"page_label": "25", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "aac4e564-9b8e-41b8-a1b2-067cfc30e76e": {"node_ids": ["ac85c774-e6a8-46f9-8a07-7b9000793683"], "metadata": {"page_label": "26", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "83be183d-be91-4545-9073-281e10ca37fe": {"node_ids": ["c050c699-d858-4900-b333-bd529c18d160", "5996284b-6f5f-4520-8ac9-439f323ebc18"], "metadata": {"page_label": "27", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "3c921241-32ed-466a-bc70-20c987edd916": {"node_ids": ["61beed51-8a07-46be-9b78-2746cd56a7ae"], "metadata": {"page_label": "28", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "bf11b47f-2cee-4f29-ac7e-ddba1fb44e9f": {"node_ids": ["b8e8975e-d599-4977-8b24-8e8a6db4a85f"], "metadata": {"page_label": "29", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "548c70c4-a5c2-4af8-919f-e598b3a1894a": {"node_ids": ["d0e7e587-4d5c-4f68-9f37-24080beff268"], "metadata": {"page_label": "30", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "51f2fc71-6868-4970-b9e8-abe69179bd49": {"node_ids": ["49b83f48-8d9a-4115-9a3b-227b892deeb1"], "metadata": {"page_label": "31", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "1a25a127-5a7b-46f9-9b2b-6eb046d2a48d": {"node_ids": ["4fbff83f-69e2-4544-b216-51118f93f6ae"], "metadata": {"page_label": "32", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "c10ac9c0-2e0f-4be3-9b88-f8f332227633": {"node_ids": ["4057834c-f144-4091-b762-156af184d7d3"], "metadata": {"page_label": "33", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "ce7abbe7-6965-4e78-8da4-ec33851d2224": {"node_ids": ["e54fcc51-9ffd-4486-a814-59483440aa4f"], "metadata": {"page_label": "34", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "b56aac4d-70fb-4af1-9dc2-1159e6f70a90": {"node_ids": ["0a864d87-bb21-4df9-b4fd-8c2ccc4d6023"], "metadata": {"page_label": "35", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "f0cce281-4ff8-4f1f-ae48-fb2b43dcb158": {"node_ids": ["dd927af0-be9b-4059-9f49-ffc85c80591e"], "metadata": {"page_label": "36", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "b7a40f37-e82b-4365-b89b-4cc973ccc7d0": {"node_ids": ["3142acbf-d7fe-4ffe-b876-1ef3639689d1"], "metadata": {"page_label": "37", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "a969031d-199f-4d4f-a121-9e31ec421446": {"node_ids": ["b24cc7e5-9e3b-44c0-a98f-a4af24fc870c"], "metadata": {"page_label": "38", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "2ddf8e06-3dfa-4453-8241-b3ccdb1e4686": {"node_ids": ["7ad967cb-c751-486e-94bf-25c8263a6d54"], "metadata": {"page_label": "39", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "0d0f1663-f311-4760-bf06-5446e5a11ac8": {"node_ids": ["ff9e2105-0efc-4af2-aad8-655238572e31", "caa9f08c-26a6-4152-aa43-33bb82691896"], "metadata": {"page_label": "40", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "72441c3d-08f1-4370-b886-13ae6988801b": {"node_ids": ["4c6d3478-0708-466d-8879-612b1c553116"], "metadata": {"page_label": "41", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "c0bf30a6-9b8b-477b-8ce8-3e76e373ff84": {"node_ids": ["0ce758f1-3382-434c-b822-e3df25c82dd1"], "metadata": {"page_label": "42", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "76ad06c5-5675-4484-a6a0-d18b08d4ec51": {"node_ids": ["30c1a09a-883d-4149-aa13-7ad1fec264cd"], "metadata": {"page_label": "43", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "9421f734-ac27-415c-ba4b-3f33a1c94d7d": {"node_ids": ["5e9b55c4-d95f-47e9-ac5d-7185664aae28"], "metadata": {"page_label": "44", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "4aed0592-1b53-48c4-98d3-b5ee8f234150": {"node_ids": ["ea08b061-dc73-494b-b7b8-1d27ed7eca59"], "metadata": {"page_label": "45", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "1ec42e5c-347d-44e6-bdcf-ee4075bfcf16": {"node_ids": ["3672d4a5-84e8-49f8-88c9-147083c07bf4"], "metadata": {"page_label": "46", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "d209b906-a365-42d4-adc8-0c3063379b38": {"node_ids": ["12ec2990-47b7-4d98-983e-47ca5b137a6c", "9bdffaf9-f218-433d-b2b1-6008e437af15"], "metadata": {"page_label": "47", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "ffd86702-ba69-4f62-8c92-80c61026da92": {"node_ids": ["ecc63690-ae4d-43b9-89da-2382380a859e"], "metadata": {"page_label": "48", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "a70665c0-b121-40ed-9595-c3076e4bb1e5": {"node_ids": ["220a5241-6967-40bb-be6a-88f9f5bab38f"], "metadata": {"page_label": "49", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "0e6d7ce8-174e-4c01-9b11-175b862efb5c": {"node_ids": ["0862d41e-8859-4095-8b86-4fa56754799b"], "metadata": {"page_label": "50", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "bd745a49-a5f7-4006-9784-b1a86f585eee": {"node_ids": ["663ec988-571a-4d48-bcc8-77579b12a90d", "5865e9ff-6d2f-49a3-a974-cdf4abde8c13"], "metadata": {"page_label": "51", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "a8de0aef-41dd-4b0b-b165-eb9ad9fdaac7": {"node_ids": ["70dea20e-ff43-40ca-b4a7-b98a33cd17af", "abb5da81-1366-4cac-84c0-71aa17add4bc"], "metadata": {"page_label": "52", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "f4d5f94e-5740-424c-b113-569c7060a94c": {"node_ids": ["71cccb65-61d6-4c8b-ace1-45ada0e771fc", "b013476a-e3c6-40d8-bb2c-428c49e47399"], "metadata": {"page_label": "53", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "df4bc328-3a90-417d-9072-96ad5d79f56a": {"node_ids": ["b8d3c995-142b-4bec-ac7d-f34bd9f6e721", "7dbe9c08-6254-4374-89e8-7bd57b98dab2"], "metadata": {"page_label": "54", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "c88fa152-4bcc-489f-9ac5-741b47ad117c": {"node_ids": ["58c643a8-66b5-480d-b4e2-23d32784ec7e", "39a8bfa6-51e5-4271-9026-9893a14e8b98"], "metadata": {"page_label": "55", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "13b2d855-039c-4589-991f-bb81c9ff9347": {"node_ids": ["6aa86388-192e-4157-8f47-ceb12aaeaca8"], "metadata": {"page_label": "56", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "efd79ef3-5949-46b5-96b3-d0ad75fff690": {"node_ids": ["52a388a0-0eb6-45ea-8044-c1c9d4ec602d", "28769e7d-85f3-4e9f-a6b5-3e0a22053343"], "metadata": {"page_label": "57", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "81560a8f-4e99-4e34-a475-dfb6369c27d1": {"node_ids": ["456b8062-8b29-4194-8544-f00a12fab2db"], "metadata": {"page_label": "58", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "252b8374-5bea-45e9-a0d6-e4946391d66a": {"node_ids": ["caa1196b-e3bb-4666-ae04-9f27049045c3"], "metadata": {"page_label": "59", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "17fd7791-7164-43fa-b030-bf9103575a6b": {"node_ids": ["bf75c5af-383f-4bcf-a6ab-651996356030"], "metadata": {"page_label": "60", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "2117b9b4-41eb-4f4e-82f4-ce9c8d1dfaa3": {"node_ids": ["9d87d0f5-c036-4393-80e6-b9e50679fe79"], "metadata": {"page_label": "61", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "b4382b29-0500-44fd-8ed4-2deaabb2e05b": {"node_ids": ["9350bd68-ed09-403d-9c83-ebb2ba7b4a61"], "metadata": {"page_label": "62", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "f75c8b03-4488-4383-a657-d7f10c309d6e": {"node_ids": ["50980f3f-048e-441c-a5f8-4f7e8b8defd3"], "metadata": {"page_label": "63", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "ab4702da-d279-4896-8925-a65b7e313065": {"node_ids": ["891bbe3d-5f0a-4124-8182-209d3f664c7b"], "metadata": {"page_label": "64", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "01af99df-40e7-40b0-b4a0-794650092dc1": {"node_ids": ["d06bee6a-5e6f-49c1-88be-e7ed10778b1b"], "metadata": {"page_label": "65", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "4ce5b866-dad9-4d22-bdf8-14e0101cc40f": {"node_ids": ["40d9babf-5240-4874-99e5-c2295fef478e"], "metadata": {"page_label": "66", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "110bc2e0-e46b-4815-916c-8a83238ce208": {"node_ids": ["0fb1d4ef-3343-4248-aec6-f7bf88df2779"], "metadata": {"page_label": "67", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "d9be1181-982d-4805-806c-2bbac3fd3325": {"node_ids": ["67ad00fd-6021-413e-90ab-08f08567b531"], "metadata": {"page_label": "68", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "5297f41d-0971-4b2b-81df-1e27a1f387d7": {"node_ids": ["7d9d0885-581d-487a-8c22-b79f04d94586"], "metadata": {"page_label": "69", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "8ca061ec-19f8-4d1f-ae48-67aadc971609": {"node_ids": ["fff73933-4ce0-460c-bd35-2227653fc61e"], "metadata": {"page_label": "70", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "c80a239f-38a6-408d-ab3d-6f220a4389b1": {"node_ids": ["b73d2e4b-5fb6-4498-b288-bb2a2dea8204"], "metadata": {"page_label": "71", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "51552b13-7c1a-42a7-813f-a88c16b17b46": {"node_ids": ["29f2f10f-cafa-424b-988c-1aa1c30759aa"], "metadata": {"page_label": "72", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "c2e545e5-9aae-4bd6-a37c-8a63a016b0a2": {"node_ids": ["27fe09e2-f534-44f6-846b-05f52e89df48"], "metadata": {"page_label": "73", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "a7c27c93-dde6-4a93-bc20-faba0e635acf": {"node_ids": ["1d70ef45-14d9-4366-a5c1-44e9e4e0ac13"], "metadata": {"page_label": "74", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "d6dee29e-c418-4f94-a1d0-ecaa688bd3f7": {"node_ids": ["9c18f26b-3730-4938-9677-67b5ae468a51"], "metadata": {"page_label": "75", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "2c5a052c-f004-47f5-bedd-510130dbbf90": {"node_ids": ["4417de3c-9d8d-4511-b605-78ee08a5dabe"], "metadata": {"page_label": "76", "file_name": "2303.17564.pdf", "file_path": "docs\\2303.17564.pdf", "file_type": "application/pdf", "file_size": 2262658, "creation_date": "2024-02-07", "last_modified_date": "2023-08-24", "last_accessed_date": "2024-02-08"}}, "54a16377-b341-4c4a-ab6d-eeafe8cb7fd1": {"node_ids": ["495aaa09-00d7-4262-bc21-cc771273b11a"], "metadata": {"page_label": "1", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "c9c4d8c1-408f-4496-b4d3-abedd1762e8b": {"node_ids": ["99312a99-9417-4418-a639-10903110c875", "a2d42920-b34d-498b-8e4b-8ef47e1eb227"], "metadata": {"page_label": "2", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "4cd007e5-bd11-4e2f-aa9d-50c4b2feed70": {"node_ids": ["a1527307-042e-40ef-a150-aa6ba301bfb3"], "metadata": {"page_label": "3", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "577c11e7-bb98-41df-8de9-cdaaa1d639c9": {"node_ids": ["eb72a68c-84ba-49dc-8d47-eee11abe4e8d"], "metadata": {"page_label": "4", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "7fd8de60-897f-4090-983e-1d52831ea649": {"node_ids": ["6dc50b2d-9137-4d9f-be4f-a8463137625a", "c1101ad3-d165-4648-924c-11c3a70013c3"], "metadata": {"page_label": "5", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "8ed38d5d-a5f1-4d79-8ac5-f0acbeb0b124": {"node_ids": ["d6bd935c-ade2-4adf-8068-91093194f2a1", "c729b94e-6b22-415b-b0c6-bd1c37852472"], "metadata": {"page_label": "6", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "f8605f3a-7656-43e8-a4e6-81677bcc909a": {"node_ids": ["60931171-b5d5-48bf-b05b-c201cb1ad237", "d226190e-198f-484b-a37d-1c43de9c1875"], "metadata": {"page_label": "7", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "560552c8-8b86-4624-9648-827df3ef3543": {"node_ids": ["933dfac2-afde-4083-b742-41d075f5926b", "285cf2fb-92b2-4fe6-b97b-43b21867eb63"], "metadata": {"page_label": "8", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "dbf8dd20-f397-4917-a962-fc1dafdc121b": {"node_ids": ["1b3806c7-4b7f-4acd-91ab-046dc8023b1f", "e9394d0c-ddf4-4420-b307-1c790d35eede"], "metadata": {"page_label": "9", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "b9bd7ebe-c240-4518-8985-2d099c8fffdb": {"node_ids": ["56ac9a9b-f180-4603-b1f4-c1d6ceb541a2", "e6836dd8-052d-452e-addb-6bb263d29b99"], "metadata": {"page_label": "10", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "8ae3847f-a505-4e7f-9a0c-05cd6cdc9112": {"node_ids": ["d1eaf772-c31a-4cc9-b01a-7bc05c86030f"], "metadata": {"page_label": "11", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "7054c97e-a62b-4ff0-9233-358d37455907": {"node_ids": ["8b0eee68-8e4e-477f-bdbd-f33a8446a1eb"], "metadata": {"page_label": "12", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "a659c6a2-3b84-4066-acaa-8798044ee557": {"node_ids": ["e54a70f9-2e5f-460d-b90a-5514a5f76885"], "metadata": {"page_label": "13", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "5f68befb-8cb8-4dfe-b5cd-8ba2de4042e0": {"node_ids": ["cc41af11-f759-4221-ac56-a05855fc1ee1"], "metadata": {"page_label": "14", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "a6f08484-e26b-4953-9a42-807bca5f22f1": {"node_ids": ["a0bf00a8-bab2-48a8-8cda-bbf4e22454b3", "d46358f1-3d67-4719-bb4e-db223a03c4da"], "metadata": {"page_label": "15", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "9943056c-56a8-41f4-b21c-d92534f5aece": {"node_ids": ["59f22d61-12a1-41b6-9d16-a0913744d92d"], "metadata": {"page_label": "16", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "cc6d33ba-4593-43fc-bd2f-2465e7bb1077": {"node_ids": ["46754bd8-104c-4b62-9eb6-11aad7c55686", "4b4e0ffd-ffd4-4151-aece-d516c61d9865"], "metadata": {"page_label": "17", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "35c57f5e-f676-4e3b-b4e7-7cd353a50dc4": {"node_ids": ["8b7c76b4-e976-4279-931b-454636871612", "8ec02813-5214-40e8-a169-792c64d6543a"], "metadata": {"page_label": "18", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "47b8b997-1352-42a6-9c13-40907baa2ffb": {"node_ids": ["bbf0839c-8634-43c8-875d-5da2fbd2e309", "eda7da7c-da25-4650-a21a-bf4eecf0c572"], "metadata": {"page_label": "19", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "5db188b7-2661-49a3-a1f2-c226e5132704": {"node_ids": ["b7c47ce3-fb62-4a91-9dee-de6d199cdbf3", "be0ae17f-7f3c-451c-aa37-b9f3917863c1"], "metadata": {"page_label": "20", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "15fbef4c-46b7-4d87-829d-200106fa753a": {"node_ids": ["0fdd14b2-5370-425d-9063-009d3ac37a85"], "metadata": {"page_label": "21", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "96944c55-fd43-4667-9b61-492540397ba0": {"node_ids": ["2a8060fc-1044-4667-bc98-86fd9dfbfef8"], "metadata": {"page_label": "22", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "6c3243a6-66b9-44b1-931e-b6c563088ffb": {"node_ids": ["157723df-b07b-4cdb-b71f-53fc28a83b69"], "metadata": {"page_label": "23", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "53a91575-5885-4b08-ad3d-0cf3bfe21722": {"node_ids": ["7704b3c9-d7d7-4860-87e2-fead97ad9560"], "metadata": {"page_label": "24", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "346b92a8-56bc-498e-94c1-e4a5fadb1e02": {"node_ids": ["a60269f5-5e8c-486f-99d9-954a352676b0"], "metadata": {"page_label": "25", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "e5449ccf-2bf2-4bc0-aaf9-081bd938dfc5": {"node_ids": ["cdb455b8-2a93-45a2-90d8-4f6a1cce69ae"], "metadata": {"page_label": "26", "file_name": "2305.14314.pdf", "file_path": "docs\\2305.14314.pdf", "file_type": "application/pdf", "file_size": 1065470, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "de77cfa4-550c-4151-b98f-3ff1ddb3e672": {"node_ids": ["735ee020-1d0e-4528-84d8-162e3743ac09"], "metadata": {"page_label": "1", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "111e0d8d-9ea7-46d1-97be-86c885590e7b": {"node_ids": ["d234f88b-c0d9-44f0-bd3e-dcd26ee976ed"], "metadata": {"page_label": "2", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "2c0724c7-6f79-4b64-8e0a-adeda852ae02": {"node_ids": ["eb78dad9-0a0d-439b-a7e9-6314181b706c"], "metadata": {"page_label": "3", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "6c141fc2-7deb-4e13-a2e5-f9f7282e0823": {"node_ids": ["49796e13-fa2e-4cb9-9f64-4dd8831143d8"], "metadata": {"page_label": "4", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "e1a594ef-52f6-43fe-a7d5-8cc90d2fdc5a": {"node_ids": ["bb7ef439-7f96-4fd4-90ce-ea866a9c1053"], "metadata": {"page_label": "5", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "d2750f85-eeba-40dc-a6f9-f80310dc43c6": {"node_ids": ["caec7162-7ba7-4a2a-be70-c38ea68e2601"], "metadata": {"page_label": "6", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "1427bb47-aac0-482f-b2d4-09041d58bd78": {"node_ids": ["ba7d32a4-4f2c-475a-865e-ea5e89ac65d3"], "metadata": {"page_label": "7", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "353a2d97-af49-4f77-af61-d0b1b433f604": {"node_ids": ["d18414f6-ea25-4b2b-a279-0c01d47647f8"], "metadata": {"page_label": "8", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "c09ae629-23c8-4815-b115-5070dbda2856": {"node_ids": ["de0cdc44-f78a-4c5f-9efe-3562d03ae5a6"], "metadata": {"page_label": "9", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "4318d69b-6d75-423c-97c4-b0988a2f4c89": {"node_ids": ["ef14231c-4800-469c-8491-a86160014866"], "metadata": {"page_label": "10", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "72ebd1e0-7104-49f3-ac43-b44ec7632f13": {"node_ids": ["62392769-18d6-40a3-957f-94d4508fbf7e"], "metadata": {"page_label": "11", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "5875b819-8e5a-423f-8899-130c92be13fe": {"node_ids": ["559e300e-957d-4821-b428-f17e6313bd51", "d19f5675-40df-4feb-ad44-1d51451b9fcb"], "metadata": {"page_label": "12", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "68e1646a-3244-46e4-8a48-3f40d635d268": {"node_ids": ["1ec347d1-6a1a-409c-8927-010db948f034", "6f96506f-1ada-43d2-bbe1-479bce31204d"], "metadata": {"page_label": "13", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}, "1faeee5b-c3b6-4869-a10f-be0ef3f2eb9f": {"node_ids": ["fc1eadf7-c33e-436a-b99f-4ed1019f1f7e"], "metadata": {"page_label": "14", "file_name": "2307.08621.pdf", "file_path": "docs\\2307.08621.pdf", "file_type": "application/pdf", "file_size": 778073, "creation_date": "2024-02-07", "last_modified_date": "2023-09-07", "last_accessed_date": "2024-02-08"}}}}